2025-03-09 18:09:29 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 3000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 3000, 'batch_size_valid': 64, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.001], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 12, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=3000, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=3000, batch_size_valid='64', max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='lstm', max_epoch=0, max_update=0, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[0.001], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='/home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=12, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, share_decoder_input_output_embed=False, share_all_embeddings=True, data='/home/users/s/solfrini/git/normalisation_training/data/data_norm_bin_1000', source_lang=None, target_lang=None, load_alignments=False, left_pad_source=True, left_pad_target=False, max_source_positions=1024, max_target_positions=1024, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_print_samples=False, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, warmup_updates=4000, warmup_init_lr=-1, pad=1, eos=2, unk=3, encoder_layers=3, decoder_layers=3, encoder_embed_dim=384, decoder_embed_dim=384, decoder_out_embed_dim=384, encoder_hidden_size=768, encoder_bidirectional=True, decoder_hidden_size=768, dropout=0.3, no_seed_provided=False, encoder_embed_path=None, encoder_freeze_embed=False, encoder_dropout_in=0.3, encoder_dropout_out=0.3, decoder_embed_path=None, decoder_freeze_embed=False, decoder_attention='1', decoder_dropout_in=0.3, decoder_dropout_out=0.3, adaptive_softmax_cutoff='10000,50000,200000', _name='lstm'), 'task': {'_name': 'translation', 'data': '/home/users/s/solfrini/git/normalisation_training/data/data_norm_bin_1000', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.001]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.001]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2025-03-09 18:09:29 | INFO | fairseq.tasks.translation | [trg] dictionary: 1032 types
2025-03-09 18:09:29 | INFO | fairseq.tasks.translation | [src] dictionary: 1032 types
2025-03-09 18:09:30 | INFO | fairseq_cli.train | LSTMModel(
  (encoder): LSTMEncoder(
    (dropout_in_module): FairseqDropout()
    (dropout_out_module): FairseqDropout()
    (embed_tokens): Embedding(1032, 384, padding_idx=1)
    (lstm): LSTM(384, 768, num_layers=3, dropout=0.3, bidirectional=True)
  )
  (decoder): LSTMDecoder(
    (dropout_in_module): FairseqDropout()
    (dropout_out_module): FairseqDropout()
    (embed_tokens): Embedding(1032, 384, padding_idx=1)
    (encoder_hidden_proj): Linear(in_features=1536, out_features=768, bias=True)
    (encoder_cell_proj): Linear(in_features=1536, out_features=768, bias=True)
    (layers): ModuleList(
      (0): LSTMCell(1152, 768)
      (1-2): 2 x LSTMCell(768, 768)
    )
    (attention): AttentionLayer(
      (input_proj): Linear(in_features=768, out_features=1536, bias=False)
      (output_proj): Linear(in_features=2304, out_features=768, bias=False)
    )
    (additional_fc): Linear(in_features=768, out_features=384, bias=True)
  )
)
2025-03-09 18:09:30 | INFO | fairseq_cli.train | task: TranslationTask
2025-03-09 18:09:30 | INFO | fairseq_cli.train | model: LSTMModel
2025-03-09 18:09:30 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion
2025-03-09 18:09:30 | INFO | fairseq_cli.train | num. shared model params: 56,781,696 (num. trained: 56,781,696)
2025-03-09 18:09:30 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2025-03-09 18:09:30 | INFO | fairseq.data.data_utils | loaded 7,087 examples from: /home/users/s/solfrini/git/normalisation_training/data/data_norm_bin_1000/valid.trg-src.trg
2025-03-09 18:09:30 | INFO | fairseq.data.data_utils | loaded 7,087 examples from: /home/users/s/solfrini/git/normalisation_training/data/data_norm_bin_1000/valid.trg-src.src
2025-03-09 18:09:30 | INFO | fairseq.tasks.translation | /home/users/s/solfrini/git/normalisation_training/data/data_norm_bin_1000 valid trg-src 7087 examples
2025-03-09 18:09:30 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2025-03-09 18:09:30 | INFO | fairseq.trainer | detected shared parameter: decoder.attention.input_proj.bias <- decoder.attention.output_proj.bias
2025-03-09 18:09:30 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2025-03-09 18:09:30 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.638 GB ; name = NVIDIA TITAN RTX                        
2025-03-09 18:09:30 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2025-03-09 18:09:30 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2025-03-09 18:09:30 | INFO | fairseq_cli.train | max tokens per device = 3000 and max sentences per device = None
2025-03-09 18:09:30 | INFO | fairseq.trainer | Preparing to load checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint_last.pt
2025-03-09 18:09:30 | INFO | fairseq.trainer | No existing checkpoint found /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint_last.pt
2025-03-09 18:09:30 | INFO | fairseq.trainer | loading train data for epoch 1
2025-03-09 18:09:30 | INFO | fairseq.data.data_utils | loaded 28,377 examples from: /home/users/s/solfrini/git/normalisation_training/data/data_norm_bin_1000/train.trg-src.trg
2025-03-09 18:09:30 | INFO | fairseq.data.data_utils | loaded 28,377 examples from: /home/users/s/solfrini/git/normalisation_training/data/data_norm_bin_1000/train.trg-src.src
2025-03-09 18:09:30 | INFO | fairseq.tasks.translation | /home/users/s/solfrini/git/normalisation_training/data/data_norm_bin_1000 train trg-src 28377 examples
2025-03-09 18:09:30 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp
2025-03-09 18:09:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:09:32 | INFO | fairseq.trainer | begin training epoch 1
2025-03-09 18:09:32 | INFO | fairseq_cli.train | Start iterating over samples
/home/users/s/solfrini/.local/lib/python3.9/site-packages/fairseq/tasks/fairseq_task.py:514: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
/home/users/s/solfrini/.local/lib/python3.9/site-packages/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
2025-03-09 18:09:45 | INFO | train_inner | epoch 001:    100 / 159 loss=9.633, ppl=793.9, wps=21902.3, ups=8.21, wpb=2670.4, bsz=181.8, num_updates=100, lr=2.5e-05, gnorm=2.112, train_wall=12, gb_free=22.1, wall=15
2025-03-09 18:09:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:09:54 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 8.172 | ppl 288.38 | wps 48709 | wpb 945.6 | bsz 63.3 | num_updates 159
2025-03-09 18:09:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 159 updates
2025-03-09 18:09:54 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint1.pt
2025-03-09 18:09:55 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint1.pt
2025-03-09 18:09:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint1.pt (epoch 1 @ 159 updates, score 8.172) (writing took 2.7707286402583122 seconds)
2025-03-09 18:09:57 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2025-03-09 18:09:57 | INFO | train | epoch 001 | loss 9.328 | ppl 642.81 | wps 17272.8 | ups 6.48 | wpb 2665.8 | bsz 178.5 | num_updates 159 | lr 3.975e-05 | gnorm 2.211 | train_wall 19 | gb_free 22.1 | wall 27
2025-03-09 18:09:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:09:57 | INFO | fairseq.trainer | begin training epoch 2
2025-03-09 18:09:57 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:10:02 | INFO | train_inner | epoch 002:     41 / 159 loss=8.633, ppl=397, wps=15484.6, ups=5.78, wpb=2680.5, bsz=180.7, num_updates=200, lr=5e-05, gnorm=2.164, train_wall=12, gb_free=22.1, wall=32
2025-03-09 18:10:15 | INFO | train_inner | epoch 002:    141 / 159 loss=8.002, ppl=256.35, wps=21393.5, ups=8.07, wpb=2651, bsz=173.8, num_updates=300, lr=7.5e-05, gnorm=2.188, train_wall=12, gb_free=22.1, wall=44
2025-03-09 18:10:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:10:19 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.157 | ppl 142.67 | wps 48379.3 | wpb 945.6 | bsz 63.3 | num_updates 318 | best_loss 7.157
2025-03-09 18:10:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 318 updates
2025-03-09 18:10:19 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint2.pt
2025-03-09 18:10:20 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint2.pt
2025-03-09 18:10:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint2.pt (epoch 2 @ 318 updates, score 7.157) (writing took 2.780669911764562 seconds)
2025-03-09 18:10:22 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2025-03-09 18:10:22 | INFO | train | epoch 002 | loss 8.062 | ppl 267.24 | wps 17189.6 | ups 6.45 | wpb 2665.8 | bsz 178.5 | num_updates 318 | lr 7.95e-05 | gnorm 2.134 | train_wall 19 | gb_free 22.1 | wall 52
2025-03-09 18:10:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:10:22 | INFO | fairseq.trainer | begin training epoch 3
2025-03-09 18:10:22 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:10:32 | INFO | train_inner | epoch 003:     82 / 159 loss=7.375, ppl=165.95, wps=15323.7, ups=5.76, wpb=2662.2, bsz=185.3, num_updates=400, lr=0.0001, gnorm=2.528, train_wall=12, gb_free=22.1, wall=62
2025-03-09 18:10:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:10:44 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 6.222 | ppl 74.65 | wps 48122.5 | wpb 945.6 | bsz 63.3 | num_updates 477 | best_loss 6.222
2025-03-09 18:10:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 477 updates
2025-03-09 18:10:44 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint3.pt
2025-03-09 18:10:45 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint3.pt
2025-03-09 18:10:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint3.pt (epoch 3 @ 477 updates, score 6.222) (writing took 2.754943954758346 seconds)
2025-03-09 18:10:47 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2025-03-09 18:10:47 | INFO | train | epoch 003 | loss 7.136 | ppl 140.69 | wps 17095.3 | ups 6.41 | wpb 2665.8 | bsz 178.5 | num_updates 477 | lr 0.00011925 | gnorm 2.559 | train_wall 19 | gb_free 22.1 | wall 76
2025-03-09 18:10:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:10:47 | INFO | fairseq.trainer | begin training epoch 4
2025-03-09 18:10:47 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:10:49 | INFO | train_inner | epoch 004:     23 / 159 loss=6.88, ppl=117.75, wps=15185.8, ups=5.68, wpb=2671.7, bsz=172.1, num_updates=500, lr=0.000125, gnorm=2.592, train_wall=12, gb_free=22.1, wall=79
2025-03-09 18:11:02 | INFO | train_inner | epoch 004:    123 / 159 loss=6.212, ppl=74.11, wps=21012.1, ups=7.91, wpb=2655.7, bsz=175.4, num_updates=600, lr=0.00015, gnorm=2.538, train_wall=12, gb_free=22.1, wall=92
2025-03-09 18:11:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:11:09 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 5.03 | ppl 32.67 | wps 47874.6 | wpb 945.6 | bsz 63.3 | num_updates 636 | best_loss 5.03
2025-03-09 18:11:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 636 updates
2025-03-09 18:11:09 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint4.pt
2025-03-09 18:11:09 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint4.pt
2025-03-09 18:11:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint4.pt (epoch 4 @ 636 updates, score 5.03) (writing took 2.7214740538038313 seconds)
2025-03-09 18:11:12 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2025-03-09 18:11:12 | INFO | train | epoch 004 | loss 6.187 | ppl 72.88 | wps 17002.6 | ups 6.38 | wpb 2665.8 | bsz 178.5 | num_updates 636 | lr 0.000159 | gnorm 2.707 | train_wall 19 | gb_free 22.1 | wall 101
2025-03-09 18:11:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:11:12 | INFO | fairseq.trainer | begin training epoch 5
2025-03-09 18:11:12 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:11:20 | INFO | train_inner | epoch 005:     64 / 159 loss=5.635, ppl=49.7, wps=15177.8, ups=5.73, wpb=2647.8, bsz=184.2, num_updates=700, lr=0.000175, gnorm=3.134, train_wall=12, gb_free=22.1, wall=109
2025-03-09 18:11:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:11:34 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 4.092 | ppl 17.05 | wps 47858.5 | wpb 945.6 | bsz 63.3 | num_updates 795 | best_loss 4.092
2025-03-09 18:11:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 795 updates
2025-03-09 18:11:34 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint5.pt
2025-03-09 18:11:34 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint5.pt
2025-03-09 18:11:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint5.pt (epoch 5 @ 795 updates, score 4.092) (writing took 2.7532607009634376 seconds)
2025-03-09 18:11:37 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2025-03-09 18:11:37 | INFO | train | epoch 005 | loss 5.234 | ppl 37.63 | wps 16934.4 | ups 6.35 | wpb 2665.8 | bsz 178.5 | num_updates 795 | lr 0.00019875 | gnorm 3.198 | train_wall 19 | gb_free 22.2 | wall 126
2025-03-09 18:11:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:11:37 | INFO | fairseq.trainer | begin training epoch 6
2025-03-09 18:11:37 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:11:37 | INFO | train_inner | epoch 006:      5 / 159 loss=5.013, ppl=32.28, wps=15225.8, ups=5.66, wpb=2689.2, bsz=174.2, num_updates=800, lr=0.0002, gnorm=3.282, train_wall=12, gb_free=22.1, wall=127
2025-03-09 18:11:50 | INFO | train_inner | epoch 006:    105 / 159 loss=4.48, ppl=22.31, wps=20998.8, ups=7.96, wpb=2638.6, bsz=173.4, num_updates=900, lr=0.000225, gnorm=3.18, train_wall=12, gb_free=22.1, wall=140
2025-03-09 18:11:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:11:59 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 2.852 | ppl 7.22 | wps 47747.6 | wpb 945.6 | bsz 63.3 | num_updates 954 | best_loss 2.852
2025-03-09 18:11:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 954 updates
2025-03-09 18:11:59 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint6.pt
2025-03-09 18:12:00 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint6.pt
2025-03-09 18:12:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint6.pt (epoch 6 @ 954 updates, score 2.852) (writing took 2.8051334558986127 seconds)
2025-03-09 18:12:02 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2025-03-09 18:12:02 | INFO | train | epoch 006 | loss 4.29 | ppl 19.57 | wps 16863.1 | ups 6.33 | wpb 2665.8 | bsz 178.5 | num_updates 954 | lr 0.0002385 | gnorm 3.188 | train_wall 19 | gb_free 22.1 | wall 151
2025-03-09 18:12:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:12:02 | INFO | fairseq.trainer | begin training epoch 7
2025-03-09 18:12:02 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:12:07 | INFO | train_inner | epoch 007:     46 / 159 loss=3.803, ppl=13.95, wps=15194.3, ups=5.66, wpb=2685.6, bsz=187.8, num_updates=1000, lr=0.00025, gnorm=3.254, train_wall=12, gb_free=22.1, wall=157
2025-03-09 18:12:20 | INFO | train_inner | epoch 007:    146 / 159 loss=3.551, ppl=11.72, wps=20884.5, ups=7.87, wpb=2653.3, bsz=175.3, num_updates=1100, lr=0.000275, gnorm=3.599, train_wall=12, gb_free=22.1, wall=170
2025-03-09 18:12:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:12:24 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 2.347 | ppl 5.09 | wps 47591.4 | wpb 945.6 | bsz 63.3 | num_updates 1113 | best_loss 2.347
2025-03-09 18:12:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 1113 updates
2025-03-09 18:12:24 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint7.pt
2025-03-09 18:12:25 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint7.pt
2025-03-09 18:12:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint7.pt (epoch 7 @ 1113 updates, score 2.347) (writing took 2.7097304849885404 seconds)
2025-03-09 18:12:27 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2025-03-09 18:12:27 | INFO | train | epoch 007 | loss 3.528 | ppl 11.54 | wps 16874.5 | ups 6.33 | wpb 2665.8 | bsz 178.5 | num_updates 1113 | lr 0.00027825 | gnorm 3.472 | train_wall 20 | gb_free 22.1 | wall 177
2025-03-09 18:12:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:12:27 | INFO | fairseq.trainer | begin training epoch 8
2025-03-09 18:12:27 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:12:38 | INFO | train_inner | epoch 008:     87 / 159 loss=2.852, ppl=7.22, wps=15231.6, ups=5.67, wpb=2684, bsz=178.2, num_updates=1200, lr=0.0003, gnorm=3.007, train_wall=12, gb_free=22.2, wall=188
2025-03-09 18:12:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:12:49 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 1.83 | ppl 3.56 | wps 47681.4 | wpb 945.6 | bsz 63.3 | num_updates 1272 | best_loss 1.83
2025-03-09 18:12:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 1272 updates
2025-03-09 18:12:49 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint8.pt
2025-03-09 18:12:50 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint8.pt
2025-03-09 18:12:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint8.pt (epoch 8 @ 1272 updates, score 1.83) (writing took 2.6221827077679336 seconds)
2025-03-09 18:12:52 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2025-03-09 18:12:52 | INFO | train | epoch 008 | loss 2.748 | ppl 6.72 | wps 16912.3 | ups 6.34 | wpb 2665.8 | bsz 178.5 | num_updates 1272 | lr 0.000318 | gnorm 2.991 | train_wall 20 | gb_free 22.1 | wall 202
2025-03-09 18:12:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:12:52 | INFO | fairseq.trainer | begin training epoch 9
2025-03-09 18:12:52 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:12:55 | INFO | train_inner | epoch 009:     28 / 159 loss=2.562, ppl=5.91, wps=15134.2, ups=5.67, wpb=2669.2, bsz=181.4, num_updates=1300, lr=0.000325, gnorm=3, train_wall=12, gb_free=22.1, wall=205
2025-03-09 18:13:08 | INFO | train_inner | epoch 009:    128 / 159 loss=1.998, ppl=4, wps=20878.8, ups=7.82, wpb=2668.7, bsz=173.6, num_updates=1400, lr=0.00035, gnorm=2.251, train_wall=12, gb_free=22.1, wall=218
2025-03-09 18:13:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:13:14 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 1.206 | ppl 2.31 | wps 47620.9 | wpb 945.6 | bsz 63.3 | num_updates 1431 | best_loss 1.206
2025-03-09 18:13:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 1431 updates
2025-03-09 18:13:14 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint9.pt
2025-03-09 18:13:15 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint9.pt
2025-03-09 18:13:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint9.pt (epoch 9 @ 1431 updates, score 1.206) (writing took 2.7163245151750743 seconds)
2025-03-09 18:13:17 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2025-03-09 18:13:17 | INFO | train | epoch 009 | loss 2.011 | ppl 4.03 | wps 16822.4 | ups 6.31 | wpb 2665.8 | bsz 178.5 | num_updates 1431 | lr 0.00035775 | gnorm 2.322 | train_wall 20 | gb_free 22.2 | wall 227
2025-03-09 18:13:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:13:17 | INFO | fairseq.trainer | begin training epoch 10
2025-03-09 18:13:17 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:13:26 | INFO | train_inner | epoch 010:     69 / 159 loss=1.592, ppl=3.01, wps=14915.7, ups=5.66, wpb=2637.4, bsz=179.8, num_updates=1500, lr=0.000375, gnorm=2.094, train_wall=12, gb_free=22.1, wall=236
2025-03-09 18:13:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:13:40 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 0.849 | ppl 1.8 | wps 47616.9 | wpb 945.6 | bsz 63.3 | num_updates 1590 | best_loss 0.849
2025-03-09 18:13:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 1590 updates
2025-03-09 18:13:40 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint10.pt
2025-03-09 18:13:40 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint10.pt
2025-03-09 18:13:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint10.pt (epoch 10 @ 1590 updates, score 0.849) (writing took 2.733099151868373 seconds)
2025-03-09 18:13:42 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2025-03-09 18:13:42 | INFO | train | epoch 010 | loss 1.39 | ppl 2.62 | wps 16814.4 | ups 6.31 | wpb 2665.8 | bsz 178.5 | num_updates 1590 | lr 0.0003975 | gnorm 1.833 | train_wall 20 | gb_free 22.1 | wall 252
2025-03-09 18:13:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:13:42 | INFO | fairseq.trainer | begin training epoch 11
2025-03-09 18:13:42 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:13:44 | INFO | train_inner | epoch 011:     10 / 159 loss=1.314, ppl=2.49, wps=15112.7, ups=5.63, wpb=2683, bsz=175.5, num_updates=1600, lr=0.0004, gnorm=1.596, train_wall=12, gb_free=22.1, wall=253
2025-03-09 18:13:56 | INFO | train_inner | epoch 011:    110 / 159 loss=1.04, ppl=2.06, wps=21454.9, ups=8, wpb=2683.1, bsz=188.2, num_updates=1700, lr=0.000425, gnorm=1.328, train_wall=12, gb_free=22.1, wall=266
2025-03-09 18:14:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:14:05 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 0.625 | ppl 1.54 | wps 47571.9 | wpb 945.6 | bsz 63.3 | num_updates 1749 | best_loss 0.625
2025-03-09 18:14:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 1749 updates
2025-03-09 18:14:05 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint11.pt
2025-03-09 18:14:05 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint11.pt
2025-03-09 18:14:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint11.pt (epoch 11 @ 1749 updates, score 0.625) (writing took 2.6752925938926637 seconds)
2025-03-09 18:14:07 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2025-03-09 18:14:07 | INFO | train | epoch 011 | loss 1.06 | ppl 2.09 | wps 16838.2 | ups 6.32 | wpb 2665.8 | bsz 178.5 | num_updates 1749 | lr 0.00043725 | gnorm 1.383 | train_wall 20 | gb_free 22.1 | wall 277
2025-03-09 18:14:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:14:07 | INFO | fairseq.trainer | begin training epoch 12
2025-03-09 18:14:07 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:14:14 | INFO | train_inner | epoch 012:     51 / 159 loss=0.993, ppl=1.99, wps=14839.2, ups=5.6, wpb=2652.2, bsz=167.5, num_updates=1800, lr=0.00045, gnorm=1.742, train_wall=13, gb_free=22.1, wall=284
2025-03-09 18:14:27 | INFO | train_inner | epoch 012:    151 / 159 loss=0.772, ppl=1.71, wps=21365.1, ups=7.96, wpb=2683.7, bsz=185.6, num_updates=1900, lr=0.000475, gnorm=0.957, train_wall=12, gb_free=22.1, wall=296
2025-03-09 18:14:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:14:30 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 0.622 | ppl 1.54 | wps 47590 | wpb 945.6 | bsz 63.3 | num_updates 1908 | best_loss 0.622
2025-03-09 18:14:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 1908 updates
2025-03-09 18:14:30 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint12.pt
2025-03-09 18:14:31 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint12.pt
2025-03-09 18:14:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint12.pt (epoch 12 @ 1908 updates, score 0.622) (writing took 2.6815745099447668 seconds)
2025-03-09 18:14:33 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2025-03-09 18:14:33 | INFO | train | epoch 012 | loss 0.85 | ppl 1.8 | wps 16830.1 | ups 6.31 | wpb 2665.8 | bsz 178.5 | num_updates 1908 | lr 0.000477 | gnorm 1.401 | train_wall 20 | gb_free 22.1 | wall 302
2025-03-09 18:14:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:14:33 | INFO | fairseq.trainer | begin training epoch 13
2025-03-09 18:14:33 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:14:44 | INFO | train_inner | epoch 013:     92 / 159 loss=1.005, ppl=2.01, wps=14767.4, ups=5.59, wpb=2640.8, bsz=170.8, num_updates=2000, lr=0.0005, gnorm=2.064, train_wall=13, gb_free=22.3, wall=314
2025-03-09 18:14:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:14:55 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 0.503 | ppl 1.42 | wps 47543.5 | wpb 945.6 | bsz 63.3 | num_updates 2067 | best_loss 0.503
2025-03-09 18:14:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 2067 updates
2025-03-09 18:14:55 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint13.pt
2025-03-09 18:14:56 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint13.pt
2025-03-09 18:14:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint13.pt (epoch 13 @ 2067 updates, score 0.503) (writing took 2.7503933156840503 seconds)
2025-03-09 18:14:58 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2025-03-09 18:14:58 | INFO | train | epoch 013 | loss 0.855 | ppl 1.81 | wps 16771 | ups 6.29 | wpb 2665.8 | bsz 178.5 | num_updates 2067 | lr 0.00051675 | gnorm 1.519 | train_wall 20 | gb_free 22.1 | wall 328
2025-03-09 18:14:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:14:58 | INFO | fairseq.trainer | begin training epoch 14
2025-03-09 18:14:58 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:15:02 | INFO | train_inner | epoch 014:     33 / 159 loss=0.681, ppl=1.6, wps=15020.8, ups=5.64, wpb=2660.9, bsz=179.8, num_updates=2100, lr=0.000525, gnorm=0.983, train_wall=12, gb_free=22.1, wall=332
2025-03-09 18:15:15 | INFO | train_inner | epoch 014:    133 / 159 loss=0.749, ppl=1.68, wps=20985.7, ups=7.9, wpb=2657.8, bsz=179.1, num_updates=2200, lr=0.00055, gnorm=1.199, train_wall=12, gb_free=22.1, wall=345
2025-03-09 18:15:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:15:20 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 0.409 | ppl 1.33 | wps 47571.4 | wpb 945.6 | bsz 63.3 | num_updates 2226 | best_loss 0.409
2025-03-09 18:15:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 2226 updates
2025-03-09 18:15:20 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint14.pt
2025-03-09 18:15:21 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint14.pt
2025-03-09 18:15:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint14.pt (epoch 14 @ 2226 updates, score 0.409) (writing took 2.7428805520758033 seconds)
2025-03-09 18:15:23 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2025-03-09 18:15:23 | INFO | train | epoch 014 | loss 0.713 | ppl 1.64 | wps 16781.6 | ups 6.3 | wpb 2665.8 | bsz 178.5 | num_updates 2226 | lr 0.0005565 | gnorm 1.127 | train_wall 20 | gb_free 22.1 | wall 353
2025-03-09 18:15:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:15:23 | INFO | fairseq.trainer | begin training epoch 15
2025-03-09 18:15:23 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:15:33 | INFO | train_inner | epoch 015:     74 / 159 loss=0.562, ppl=1.48, wps=14822, ups=5.61, wpb=2642, bsz=177.2, num_updates=2300, lr=0.000575, gnorm=0.866, train_wall=12, gb_free=22.1, wall=362
2025-03-09 18:15:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:15:46 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 0.381 | ppl 1.3 | wps 47625.7 | wpb 945.6 | bsz 63.3 | num_updates 2385 | best_loss 0.381
2025-03-09 18:15:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 2385 updates
2025-03-09 18:15:46 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint15.pt
2025-03-09 18:15:46 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint15.pt
2025-03-09 18:15:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint15.pt (epoch 15 @ 2385 updates, score 0.381) (writing took 2.6682552536949515 seconds)
2025-03-09 18:15:48 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2025-03-09 18:15:48 | INFO | train | epoch 015 | loss 0.584 | ppl 1.5 | wps 16828.7 | ups 6.31 | wpb 2665.8 | bsz 178.5 | num_updates 2385 | lr 0.00059625 | gnorm 0.873 | train_wall 20 | gb_free 22.1 | wall 378
2025-03-09 18:15:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:15:48 | INFO | fairseq.trainer | begin training epoch 16
2025-03-09 18:15:48 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:15:50 | INFO | train_inner | epoch 016:     15 / 159 loss=0.564, ppl=1.48, wps=15444, ups=5.69, wpb=2716.4, bsz=183.6, num_updates=2400, lr=0.0006, gnorm=0.752, train_wall=12, gb_free=22.1, wall=380
2025-03-09 18:16:03 | INFO | train_inner | epoch 016:    115 / 159 loss=0.591, ppl=1.51, wps=20945.7, ups=7.86, wpb=2665.2, bsz=178.2, num_updates=2500, lr=0.000625, gnorm=1.158, train_wall=12, gb_free=22.1, wall=393
2025-03-09 18:16:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:16:11 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 0.373 | ppl 1.3 | wps 47634.1 | wpb 945.6 | bsz 63.3 | num_updates 2544 | best_loss 0.373
2025-03-09 18:16:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 2544 updates
2025-03-09 18:16:11 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint16.pt
2025-03-09 18:16:11 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint16.pt
2025-03-09 18:16:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint16.pt (epoch 16 @ 2544 updates, score 0.373) (writing took 2.5837921402417123 seconds)
2025-03-09 18:16:13 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2025-03-09 18:16:13 | INFO | train | epoch 016 | loss 0.581 | ppl 1.5 | wps 16880.1 | ups 6.33 | wpb 2665.8 | bsz 178.5 | num_updates 2544 | lr 0.000636 | gnorm 1.034 | train_wall 20 | gb_free 22.2 | wall 403
2025-03-09 18:16:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:16:13 | INFO | fairseq.trainer | begin training epoch 17
2025-03-09 18:16:13 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:16:21 | INFO | train_inner | epoch 017:     56 / 159 loss=0.545, ppl=1.46, wps=15154.2, ups=5.68, wpb=2665.7, bsz=181.1, num_updates=2600, lr=0.00065, gnorm=0.803, train_wall=12, gb_free=22.1, wall=410
2025-03-09 18:16:33 | INFO | train_inner | epoch 017:    156 / 159 loss=0.543, ppl=1.46, wps=20795.4, ups=7.84, wpb=2654.1, bsz=174.3, num_updates=2700, lr=0.000675, gnorm=0.959, train_wall=12, gb_free=22.1, wall=423
2025-03-09 18:16:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:16:36 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 0.418 | ppl 1.34 | wps 47428.9 | wpb 945.6 | bsz 63.3 | num_updates 2703 | best_loss 0.373
2025-03-09 18:16:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 2703 updates
2025-03-09 18:16:36 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint17.pt
2025-03-09 18:16:37 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint17.pt
2025-03-09 18:16:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint17.pt (epoch 17 @ 2703 updates, score 0.418) (writing took 1.5774897742085159 seconds)
2025-03-09 18:16:38 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2025-03-09 18:16:38 | INFO | train | epoch 017 | loss 0.526 | ppl 1.44 | wps 17571 | ups 6.59 | wpb 2665.8 | bsz 178.5 | num_updates 2703 | lr 0.00067575 | gnorm 0.861 | train_wall 20 | gb_free 22.1 | wall 427
2025-03-09 18:16:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:16:38 | INFO | fairseq.trainer | begin training epoch 18
2025-03-09 18:16:38 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:16:50 | INFO | train_inner | epoch 018:     97 / 159 loss=0.502, ppl=1.42, wps=15981.7, ups=5.98, wpb=2672.9, bsz=174.6, num_updates=2800, lr=0.0007, gnorm=0.94, train_wall=13, gb_free=22.1, wall=440
2025-03-09 18:16:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:17:00 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 0.342 | ppl 1.27 | wps 47555.5 | wpb 945.6 | bsz 63.3 | num_updates 2862 | best_loss 0.342
2025-03-09 18:17:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 2862 updates
2025-03-09 18:17:00 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint18.pt
2025-03-09 18:17:01 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint18.pt
2025-03-09 18:17:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint18.pt (epoch 18 @ 2862 updates, score 0.342) (writing took 2.620400350075215 seconds)
2025-03-09 18:17:03 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2025-03-09 18:17:03 | INFO | train | epoch 018 | loss 0.533 | ppl 1.45 | wps 16864.7 | ups 6.33 | wpb 2665.8 | bsz 178.5 | num_updates 2862 | lr 0.0007155 | gnorm 1.012 | train_wall 20 | gb_free 22.1 | wall 452
2025-03-09 18:17:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:17:03 | INFO | fairseq.trainer | begin training epoch 19
2025-03-09 18:17:03 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:17:08 | INFO | train_inner | epoch 019:     38 / 159 loss=0.529, ppl=1.44, wps=15339.3, ups=5.72, wpb=2682.3, bsz=184.6, num_updates=2900, lr=0.000725, gnorm=0.903, train_wall=12, gb_free=22.2, wall=457
2025-03-09 18:17:20 | INFO | train_inner | epoch 019:    138 / 159 loss=0.462, ppl=1.38, wps=20870.9, ups=7.89, wpb=2645.1, bsz=176.5, num_updates=3000, lr=0.00075, gnorm=0.729, train_wall=12, gb_free=22.1, wall=470
2025-03-09 18:17:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:17:25 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 0.31 | ppl 1.24 | wps 47605.8 | wpb 945.6 | bsz 63.3 | num_updates 3021 | best_loss 0.31
2025-03-09 18:17:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 3021 updates
2025-03-09 18:17:25 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint19.pt
2025-03-09 18:17:26 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint19.pt
2025-03-09 18:17:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint19.pt (epoch 19 @ 3021 updates, score 0.31) (writing took 2.6071952409110963 seconds)
2025-03-09 18:17:28 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2025-03-09 18:17:28 | INFO | train | epoch 019 | loss 0.448 | ppl 1.36 | wps 16863.7 | ups 6.33 | wpb 2665.8 | bsz 178.5 | num_updates 3021 | lr 0.00075525 | gnorm 0.674 | train_wall 20 | gb_free 22.1 | wall 478
2025-03-09 18:17:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:17:28 | INFO | fairseq.trainer | begin training epoch 20
2025-03-09 18:17:28 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:17:38 | INFO | train_inner | epoch 020:     79 / 159 loss=0.49, ppl=1.4, wps=15160.9, ups=5.7, wpb=2661.8, bsz=183.2, num_updates=3100, lr=0.000775, gnorm=0.874, train_wall=12, gb_free=22.1, wall=488
2025-03-09 18:17:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:17:50 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 0.348 | ppl 1.27 | wps 47521.1 | wpb 945.6 | bsz 63.3 | num_updates 3180 | best_loss 0.31
2025-03-09 18:17:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 3180 updates
2025-03-09 18:17:50 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint20.pt
2025-03-09 18:17:51 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint20.pt
2025-03-09 18:17:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint20.pt (epoch 20 @ 3180 updates, score 0.348) (writing took 1.5629783691838384 seconds)
2025-03-09 18:17:52 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2025-03-09 18:17:52 | INFO | train | epoch 020 | loss 0.487 | ppl 1.4 | wps 17597.6 | ups 6.6 | wpb 2665.8 | bsz 178.5 | num_updates 3180 | lr 0.000795 | gnorm 0.814 | train_wall 20 | gb_free 22.1 | wall 502
2025-03-09 18:17:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:17:52 | INFO | fairseq.trainer | begin training epoch 21
2025-03-09 18:17:52 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:17:54 | INFO | train_inner | epoch 021:     20 / 159 loss=0.456, ppl=1.37, wps=16155.2, ups=6.02, wpb=2683.9, bsz=177.3, num_updates=3200, lr=0.0008, gnorm=0.649, train_wall=12, gb_free=22.1, wall=504
2025-03-09 18:18:07 | INFO | train_inner | epoch 021:    120 / 159 loss=0.377, ppl=1.3, wps=20654.3, ups=7.78, wpb=2654.2, bsz=177.8, num_updates=3300, lr=0.000825, gnorm=0.616, train_wall=13, gb_free=22.1, wall=517
2025-03-09 18:18:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:18:14 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 0.358 | ppl 1.28 | wps 47448.1 | wpb 945.6 | bsz 63.3 | num_updates 3339 | best_loss 0.31
2025-03-09 18:18:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 3339 updates
2025-03-09 18:18:14 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint21.pt
2025-03-09 18:18:15 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint21.pt
2025-03-09 18:18:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint21.pt (epoch 21 @ 3339 updates, score 0.358) (writing took 1.554227587301284 seconds)
2025-03-09 18:18:16 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2025-03-09 18:18:16 | INFO | train | epoch 021 | loss 0.458 | ppl 1.37 | wps 17607.6 | ups 6.61 | wpb 2665.8 | bsz 178.5 | num_updates 3339 | lr 0.00083475 | gnorm 0.722 | train_wall 20 | gb_free 22.1 | wall 526
2025-03-09 18:18:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:18:16 | INFO | fairseq.trainer | begin training epoch 22
2025-03-09 18:18:16 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:18:24 | INFO | train_inner | epoch 022:     61 / 159 loss=0.539, ppl=1.45, wps=16134.2, ups=6.08, wpb=2652.4, bsz=175.6, num_updates=3400, lr=0.00085, gnorm=0.834, train_wall=12, gb_free=22.1, wall=533
2025-03-09 18:18:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:18:39 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 0.31 | ppl 1.24 | wps 47537.5 | wpb 945.6 | bsz 63.3 | num_updates 3498 | best_loss 0.31
2025-03-09 18:18:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 3498 updates
2025-03-09 18:18:39 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint22.pt
2025-03-09 18:18:39 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint22.pt
2025-03-09 18:18:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint22.pt (epoch 22 @ 3498 updates, score 0.31) (writing took 2.621882528066635 seconds)
2025-03-09 18:18:41 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2025-03-09 18:18:41 | INFO | train | epoch 022 | loss 0.43 | ppl 1.35 | wps 16860.2 | ups 6.32 | wpb 2665.8 | bsz 178.5 | num_updates 3498 | lr 0.0008745 | gnorm 0.698 | train_wall 20 | gb_free 22.1 | wall 551
2025-03-09 18:18:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:18:41 | INFO | fairseq.trainer | begin training epoch 23
2025-03-09 18:18:41 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:18:41 | INFO | train_inner | epoch 023:      2 / 159 loss=0.414, ppl=1.33, wps=15120.4, ups=5.63, wpb=2685.3, bsz=180.3, num_updates=3500, lr=0.000875, gnorm=0.714, train_wall=12, gb_free=22.1, wall=551
2025-03-09 18:18:54 | INFO | train_inner | epoch 023:    102 / 159 loss=0.36, ppl=1.28, wps=21222.9, ups=7.9, wpb=2687.6, bsz=179.2, num_updates=3600, lr=0.0009, gnorm=0.433, train_wall=12, gb_free=22.1, wall=564
2025-03-09 18:19:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:19:04 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 0.304 | ppl 1.23 | wps 47575 | wpb 945.6 | bsz 63.3 | num_updates 3657 | best_loss 0.304
2025-03-09 18:19:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 3657 updates
2025-03-09 18:19:04 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint23.pt
2025-03-09 18:19:04 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint23.pt
2025-03-09 18:19:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint23.pt (epoch 23 @ 3657 updates, score 0.304) (writing took 2.622106473892927 seconds)
2025-03-09 18:19:06 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2025-03-09 18:19:06 | INFO | train | epoch 023 | loss 0.372 | ppl 1.29 | wps 16811.3 | ups 6.31 | wpb 2665.8 | bsz 178.5 | num_updates 3657 | lr 0.00091425 | gnorm 0.537 | train_wall 20 | gb_free 22.1 | wall 576
2025-03-09 18:19:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:19:06 | INFO | fairseq.trainer | begin training epoch 24
2025-03-09 18:19:06 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:19:12 | INFO | train_inner | epoch 024:     43 / 159 loss=0.393, ppl=1.31, wps=15294.1, ups=5.66, wpb=2701.8, bsz=180.3, num_updates=3700, lr=0.000925, gnorm=0.632, train_wall=12, gb_free=22.1, wall=582
2025-03-09 18:19:24 | INFO | train_inner | epoch 024:    143 / 159 loss=0.346, ppl=1.27, wps=20819.9, ups=7.89, wpb=2637.2, bsz=180.2, num_updates=3800, lr=0.00095, gnorm=0.418, train_wall=12, gb_free=22.2, wall=594
2025-03-09 18:19:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:19:29 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 0.326 | ppl 1.25 | wps 47613.4 | wpb 945.6 | bsz 63.3 | num_updates 3816 | best_loss 0.304
2025-03-09 18:19:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 3816 updates
2025-03-09 18:19:29 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint24.pt
2025-03-09 18:19:29 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint24.pt
2025-03-09 18:19:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint24.pt (epoch 24 @ 3816 updates, score 0.326) (writing took 1.5749512230977416 seconds)
2025-03-09 18:19:30 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2025-03-09 18:19:30 | INFO | train | epoch 024 | loss 0.358 | ppl 1.28 | wps 17594.5 | ups 6.6 | wpb 2665.8 | bsz 178.5 | num_updates 3816 | lr 0.000954 | gnorm 0.481 | train_wall 20 | gb_free 22.1 | wall 600
2025-03-09 18:19:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:19:30 | INFO | fairseq.trainer | begin training epoch 25
2025-03-09 18:19:30 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:19:41 | INFO | train_inner | epoch 025:     84 / 159 loss=0.356, ppl=1.28, wps=15743.6, ups=6.01, wpb=2619.5, bsz=173.9, num_updates=3900, lr=0.000975, gnorm=0.545, train_wall=12, gb_free=22.1, wall=611
2025-03-09 18:19:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:19:53 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 0.31 | ppl 1.24 | wps 48228.4 | wpb 945.6 | bsz 63.3 | num_updates 3975 | best_loss 0.304
2025-03-09 18:19:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 3975 updates
2025-03-09 18:19:53 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint25.pt
2025-03-09 18:19:53 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint25.pt
2025-03-09 18:19:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint25.pt (epoch 25 @ 3975 updates, score 0.31) (writing took 1.5723334578797221 seconds)
2025-03-09 18:19:54 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2025-03-09 18:19:54 | INFO | train | epoch 025 | loss 0.364 | ppl 1.29 | wps 17631.6 | ups 6.61 | wpb 2665.8 | bsz 178.5 | num_updates 3975 | lr 0.00099375 | gnorm 0.491 | train_wall 20 | gb_free 22.1 | wall 624
2025-03-09 18:19:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:19:55 | INFO | fairseq.trainer | begin training epoch 26
2025-03-09 18:19:55 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:19:58 | INFO | train_inner | epoch 026:     25 / 159 loss=0.355, ppl=1.28, wps=16088.6, ups=5.98, wpb=2690.2, bsz=175.2, num_updates=4000, lr=0.001, gnorm=0.477, train_wall=13, gb_free=22.1, wall=628
2025-03-09 18:20:11 | INFO | train_inner | epoch 026:    125 / 159 loss=0.417, ppl=1.33, wps=20696.9, ups=7.87, wpb=2630, bsz=175.2, num_updates=4100, lr=0.00098773, gnorm=0.795, train_wall=12, gb_free=22.2, wall=640
2025-03-09 18:20:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:20:17 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 0.275 | ppl 1.21 | wps 47530.6 | wpb 945.6 | bsz 63.3 | num_updates 4134 | best_loss 0.275
2025-03-09 18:20:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 4134 updates
2025-03-09 18:20:17 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint26.pt
2025-03-09 18:20:18 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint26.pt
2025-03-09 18:20:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint26.pt (epoch 26 @ 4134 updates, score 0.275) (writing took 2.605250551365316 seconds)
2025-03-09 18:20:20 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2025-03-09 18:20:20 | INFO | train | epoch 026 | loss 0.392 | ppl 1.31 | wps 16870.9 | ups 6.33 | wpb 2665.8 | bsz 178.5 | num_updates 4134 | lr 0.000983659 | gnorm 0.668 | train_wall 20 | gb_free 22.1 | wall 649
2025-03-09 18:20:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:20:20 | INFO | fairseq.trainer | begin training epoch 27
2025-03-09 18:20:20 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:20:28 | INFO | train_inner | epoch 027:     66 / 159 loss=0.338, ppl=1.26, wps=15470, ups=5.68, wpb=2723.2, bsz=184.5, num_updates=4200, lr=0.0009759, gnorm=0.437, train_wall=12, gb_free=22.1, wall=658
2025-03-09 18:20:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:20:42 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 0.3 | ppl 1.23 | wps 48158.5 | wpb 945.6 | bsz 63.3 | num_updates 4293 | best_loss 0.275
2025-03-09 18:20:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 4293 updates
2025-03-09 18:20:42 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint27.pt
2025-03-09 18:20:43 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint27.pt
2025-03-09 18:20:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint27.pt (epoch 27 @ 4293 updates, score 0.3) (writing took 1.5701927747577429 seconds)
2025-03-09 18:20:44 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2025-03-09 18:20:44 | INFO | train | epoch 027 | loss 0.326 | ppl 1.25 | wps 17597.3 | ups 6.6 | wpb 2665.8 | bsz 178.5 | num_updates 4293 | lr 0.000965272 | gnorm 0.454 | train_wall 20 | gb_free 22.2 | wall 673
2025-03-09 18:20:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:20:44 | INFO | fairseq.trainer | begin training epoch 28
2025-03-09 18:20:44 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:20:45 | INFO | train_inner | epoch 028:      7 / 159 loss=0.336, ppl=1.26, wps=15741.3, ups=6.05, wpb=2603.6, bsz=176.7, num_updates=4300, lr=0.000964486, gnorm=0.547, train_wall=12, gb_free=22.2, wall=674
2025-03-09 18:20:57 | INFO | train_inner | epoch 028:    107 / 159 loss=0.348, ppl=1.27, wps=20801.8, ups=7.83, wpb=2656.6, bsz=173.3, num_updates=4400, lr=0.000953463, gnorm=0.464, train_wall=12, gb_free=22.1, wall=687
2025-03-09 18:21:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:21:06 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 0.27 | ppl 1.21 | wps 47563.9 | wpb 945.6 | bsz 63.3 | num_updates 4452 | best_loss 0.27
2025-03-09 18:21:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 4452 updates
2025-03-09 18:21:06 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint28.pt
2025-03-09 18:21:07 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint28.pt
2025-03-09 18:21:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint28.pt (epoch 28 @ 4452 updates, score 0.27) (writing took 2.599452082067728 seconds)
2025-03-09 18:21:09 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2025-03-09 18:21:09 | INFO | train | epoch 028 | loss 0.324 | ppl 1.25 | wps 16859.1 | ups 6.32 | wpb 2665.8 | bsz 178.5 | num_updates 4452 | lr 0.000947878 | gnorm 0.464 | train_wall 20 | gb_free 22.1 | wall 699
2025-03-09 18:21:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:21:09 | INFO | fairseq.trainer | begin training epoch 29
2025-03-09 18:21:09 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:21:15 | INFO | train_inner | epoch 029:     48 / 159 loss=0.278, ppl=1.21, wps=15327.3, ups=5.71, wpb=2686.6, bsz=182.8, num_updates=4500, lr=0.000942809, gnorm=0.312, train_wall=12, gb_free=22.1, wall=705
2025-03-09 18:21:28 | INFO | train_inner | epoch 029:    148 / 159 loss=0.306, ppl=1.24, wps=21163.6, ups=7.89, wpb=2682.1, bsz=182.2, num_updates=4600, lr=0.000932505, gnorm=0.488, train_wall=12, gb_free=22.1, wall=717
2025-03-09 18:21:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:21:31 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 0.265 | ppl 1.2 | wps 47564.4 | wpb 945.6 | bsz 63.3 | num_updates 4611 | best_loss 0.265
2025-03-09 18:21:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 4611 updates
2025-03-09 18:21:31 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint29.pt
2025-03-09 18:21:32 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint29.pt
2025-03-09 18:21:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint29.pt (epoch 29 @ 4611 updates, score 0.265) (writing took 2.64665818400681 seconds)
2025-03-09 18:21:34 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2025-03-09 18:21:34 | INFO | train | epoch 029 | loss 0.297 | ppl 1.23 | wps 16844 | ups 6.32 | wpb 2665.8 | bsz 178.5 | num_updates 4611 | lr 0.000931392 | gnorm 0.426 | train_wall 20 | gb_free 22.1 | wall 724
2025-03-09 18:21:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:21:34 | INFO | fairseq.trainer | begin training epoch 30
2025-03-09 18:21:34 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:21:45 | INFO | train_inner | epoch 030:     89 / 159 loss=0.307, ppl=1.24, wps=14958.1, ups=5.66, wpb=2643.1, bsz=176.8, num_updates=4700, lr=0.000922531, gnorm=0.427, train_wall=12, gb_free=22.1, wall=735
2025-03-09 18:21:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:21:57 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 0.259 | ppl 1.2 | wps 47593.4 | wpb 945.6 | bsz 63.3 | num_updates 4770 | best_loss 0.259
2025-03-09 18:21:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 4770 updates
2025-03-09 18:21:57 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint30.pt
2025-03-09 18:21:57 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint30.pt
2025-03-09 18:21:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint30.pt (epoch 30 @ 4770 updates, score 0.259) (writing took 2.6283276919275522 seconds)
2025-03-09 18:21:59 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2025-03-09 18:21:59 | INFO | train | epoch 030 | loss 0.292 | ppl 1.22 | wps 16846 | ups 6.32 | wpb 2665.8 | bsz 178.5 | num_updates 4770 | lr 0.000915737 | gnorm 0.424 | train_wall 20 | gb_free 22.2 | wall 749
2025-03-09 18:21:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:21:59 | INFO | fairseq.trainer | begin training epoch 31
2025-03-09 18:21:59 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:22:03 | INFO | train_inner | epoch 031:     30 / 159 loss=0.276, ppl=1.21, wps=15052.1, ups=5.65, wpb=2664.2, bsz=175.8, num_updates=4800, lr=0.000912871, gnorm=0.395, train_wall=12, gb_free=22.1, wall=753
2025-03-09 18:22:16 | INFO | train_inner | epoch 031:    130 / 159 loss=0.296, ppl=1.23, wps=21026.2, ups=7.84, wpb=2680.5, bsz=180.5, num_updates=4900, lr=0.000903508, gnorm=0.561, train_wall=12, gb_free=22.1, wall=766
2025-03-09 18:22:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:22:22 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 0.272 | ppl 1.21 | wps 47511.6 | wpb 945.6 | bsz 63.3 | num_updates 4929 | best_loss 0.259
2025-03-09 18:22:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 4929 updates
2025-03-09 18:22:22 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint31.pt
2025-03-09 18:22:22 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint31.pt
2025-03-09 18:22:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint31.pt (epoch 31 @ 4929 updates, score 0.272) (writing took 1.5660720141604543 seconds)
2025-03-09 18:22:23 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2025-03-09 18:22:23 | INFO | train | epoch 031 | loss 0.298 | ppl 1.23 | wps 17585.7 | ups 6.6 | wpb 2665.8 | bsz 178.5 | num_updates 4929 | lr 0.000900846 | gnorm 0.528 | train_wall 20 | gb_free 22.1 | wall 773
2025-03-09 18:22:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:22:23 | INFO | fairseq.trainer | begin training epoch 32
2025-03-09 18:22:23 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:22:32 | INFO | train_inner | epoch 032:     71 / 159 loss=0.317, ppl=1.25, wps=15878, ups=6, wpb=2648.4, bsz=172.7, num_updates=5000, lr=0.000894427, gnorm=0.609, train_wall=12, gb_free=22.1, wall=782
2025-03-09 18:22:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:22:46 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 0.236 | ppl 1.18 | wps 47509.9 | wpb 945.6 | bsz 63.3 | num_updates 5088 | best_loss 0.236
2025-03-09 18:22:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 5088 updates
2025-03-09 18:22:46 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint32.pt
2025-03-09 18:22:46 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint32.pt
2025-03-09 18:22:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint32.pt (epoch 32 @ 5088 updates, score 0.236) (writing took 2.6029417417012155 seconds)
2025-03-09 18:22:48 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2025-03-09 18:22:48 | INFO | train | epoch 032 | loss 0.284 | ppl 1.22 | wps 16883.9 | ups 6.33 | wpb 2665.8 | bsz 178.5 | num_updates 5088 | lr 0.000886659 | gnorm 0.45 | train_wall 20 | gb_free 22.1 | wall 798
2025-03-09 18:22:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:22:48 | INFO | fairseq.trainer | begin training epoch 33
2025-03-09 18:22:48 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:22:50 | INFO | train_inner | epoch 033:     12 / 159 loss=0.258, ppl=1.2, wps=15419, ups=5.73, wpb=2692, bsz=185.8, num_updates=5100, lr=0.000885615, gnorm=0.312, train_wall=12, gb_free=22.1, wall=800
2025-03-09 18:23:03 | INFO | train_inner | epoch 033:    112 / 159 loss=0.251, ppl=1.19, wps=21153, ups=7.92, wpb=2670.1, bsz=183.8, num_updates=5200, lr=0.000877058, gnorm=0.378, train_wall=12, gb_free=22.1, wall=812
2025-03-09 18:23:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:23:11 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 0.269 | ppl 1.21 | wps 47631.2 | wpb 945.6 | bsz 63.3 | num_updates 5247 | best_loss 0.236
2025-03-09 18:23:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 5247 updates
2025-03-09 18:23:11 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint33.pt
2025-03-09 18:23:11 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint33.pt
2025-03-09 18:23:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint33.pt (epoch 33 @ 5247 updates, score 0.269) (writing took 1.592468005605042 seconds)
2025-03-09 18:23:12 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2025-03-09 18:23:12 | INFO | train | epoch 033 | loss 0.251 | ppl 1.19 | wps 17580.6 | ups 6.59 | wpb 2665.8 | bsz 178.5 | num_updates 5247 | lr 0.000873121 | gnorm 0.349 | train_wall 20 | gb_free 22.1 | wall 822
2025-03-09 18:23:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:23:12 | INFO | fairseq.trainer | begin training epoch 34
2025-03-09 18:23:12 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:23:19 | INFO | train_inner | epoch 034:     53 / 159 loss=0.246, ppl=1.19, wps=16201.4, ups=5.98, wpb=2707.5, bsz=174.3, num_updates=5300, lr=0.000868744, gnorm=0.294, train_wall=13, gb_free=22.1, wall=829
2025-03-09 18:23:32 | INFO | train_inner | epoch 034:    153 / 159 loss=0.28, ppl=1.21, wps=20716.2, ups=7.9, wpb=2623.9, bsz=177.1, num_updates=5400, lr=0.000860663, gnorm=0.546, train_wall=12, gb_free=22.1, wall=842
2025-03-09 18:23:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:23:35 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 0.257 | ppl 1.2 | wps 47632.2 | wpb 945.6 | bsz 63.3 | num_updates 5406 | best_loss 0.236
2025-03-09 18:23:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 5406 updates
2025-03-09 18:23:35 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint34.pt
2025-03-09 18:23:36 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint34.pt
2025-03-09 18:23:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint34.pt (epoch 34 @ 5406 updates, score 0.257) (writing took 1.562795878853649 seconds)
2025-03-09 18:23:37 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2025-03-09 18:23:37 | INFO | train | epoch 034 | loss 0.268 | ppl 1.2 | wps 17624.3 | ups 6.61 | wpb 2665.8 | bsz 178.5 | num_updates 5406 | lr 0.000860185 | gnorm 0.462 | train_wall 20 | gb_free 22.1 | wall 846
2025-03-09 18:23:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:23:37 | INFO | fairseq.trainer | begin training epoch 35
2025-03-09 18:23:37 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:23:49 | INFO | train_inner | epoch 035:     94 / 159 loss=0.304, ppl=1.23, wps=15883.3, ups=6.02, wpb=2638.6, bsz=178.5, num_updates=5500, lr=0.000852803, gnorm=0.632, train_wall=12, gb_free=22.1, wall=858
2025-03-09 18:23:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:23:59 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 0.234 | ppl 1.18 | wps 47522.2 | wpb 945.6 | bsz 63.3 | num_updates 5565 | best_loss 0.234
2025-03-09 18:23:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 5565 updates
2025-03-09 18:23:59 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint35.pt
2025-03-09 18:24:00 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint35.pt
2025-03-09 18:24:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint35.pt (epoch 35 @ 5565 updates, score 0.234) (writing took 2.761964430101216 seconds)
2025-03-09 18:24:02 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2025-03-09 18:24:02 | INFO | train | epoch 035 | loss 0.282 | ppl 1.22 | wps 16779.1 | ups 6.29 | wpb 2665.8 | bsz 178.5 | num_updates 5565 | lr 0.000847808 | gnorm 0.499 | train_wall 20 | gb_free 22.1 | wall 872
2025-03-09 18:24:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:24:02 | INFO | fairseq.trainer | begin training epoch 36
2025-03-09 18:24:02 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:24:06 | INFO | train_inner | epoch 036:     35 / 159 loss=0.246, ppl=1.19, wps=15186.9, ups=5.64, wpb=2693.2, bsz=178.7, num_updates=5600, lr=0.000845154, gnorm=0.316, train_wall=12, gb_free=22.1, wall=876
2025-03-09 18:24:19 | INFO | train_inner | epoch 036:    135 / 159 loss=0.237, ppl=1.18, wps=21037.4, ups=7.88, wpb=2668.5, bsz=179.5, num_updates=5700, lr=0.000837708, gnorm=0.337, train_wall=12, gb_free=22.1, wall=889
2025-03-09 18:24:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:24:24 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 0.251 | ppl 1.19 | wps 47495.1 | wpb 945.6 | bsz 63.3 | num_updates 5724 | best_loss 0.234
2025-03-09 18:24:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 5724 updates
2025-03-09 18:24:24 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint36.pt
2025-03-09 18:24:25 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint36.pt
2025-03-09 18:24:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint36.pt (epoch 36 @ 5724 updates, score 0.251) (writing took 1.703005172777921 seconds)
2025-03-09 18:24:26 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2025-03-09 18:24:26 | INFO | train | epoch 036 | loss 0.242 | ppl 1.18 | wps 17509.6 | ups 6.57 | wpb 2665.8 | bsz 178.5 | num_updates 5724 | lr 0.00083595 | gnorm 0.342 | train_wall 20 | gb_free 22.1 | wall 896
2025-03-09 18:24:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:24:26 | INFO | fairseq.trainer | begin training epoch 37
2025-03-09 18:24:26 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:24:36 | INFO | train_inner | epoch 037:     76 / 159 loss=0.252, ppl=1.19, wps=15849.8, ups=5.99, wpb=2644.7, bsz=171, num_updates=5800, lr=0.000830455, gnorm=0.319, train_wall=12, gb_free=22.1, wall=905
2025-03-09 18:24:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:24:48 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 0.249 | ppl 1.19 | wps 47618.2 | wpb 945.6 | bsz 63.3 | num_updates 5883 | best_loss 0.234
2025-03-09 18:24:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 5883 updates
2025-03-09 18:24:48 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint37.pt
2025-03-09 18:24:49 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint37.pt
2025-03-09 18:24:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint37.pt (epoch 37 @ 5883 updates, score 0.249) (writing took 1.5785561436787248 seconds)
2025-03-09 18:24:50 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2025-03-09 18:24:50 | INFO | train | epoch 037 | loss 0.238 | ppl 1.18 | wps 17620.4 | ups 6.61 | wpb 2665.8 | bsz 178.5 | num_updates 5883 | lr 0.000824576 | gnorm 0.371 | train_wall 20 | gb_free 22.1 | wall 920
2025-03-09 18:24:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:24:50 | INFO | fairseq.trainer | begin training epoch 38
2025-03-09 18:24:50 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:24:52 | INFO | train_inner | epoch 038:     17 / 159 loss=0.23, ppl=1.17, wps=16129.1, ups=6.03, wpb=2674.6, bsz=184.6, num_updates=5900, lr=0.000823387, gnorm=0.411, train_wall=12, gb_free=22.1, wall=922
2025-03-09 18:25:05 | INFO | train_inner | epoch 038:    117 / 159 loss=0.232, ppl=1.17, wps=21098.7, ups=7.88, wpb=2678.7, bsz=183.6, num_updates=6000, lr=0.000816497, gnorm=0.394, train_wall=12, gb_free=22.1, wall=935
2025-03-09 18:25:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:25:13 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 0.263 | ppl 1.2 | wps 48264.4 | wpb 945.6 | bsz 63.3 | num_updates 6042 | best_loss 0.234
2025-03-09 18:25:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 6042 updates
2025-03-09 18:25:13 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint38.pt
2025-03-09 18:25:13 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint38.pt
2025-03-09 18:25:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint38.pt (epoch 38 @ 6042 updates, score 0.263) (writing took 1.5907967169769108 seconds)
2025-03-09 18:25:14 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2025-03-09 18:25:14 | INFO | train | epoch 038 | loss 0.242 | ppl 1.18 | wps 17622.5 | ups 6.61 | wpb 2665.8 | bsz 178.5 | num_updates 6042 | lr 0.000813654 | gnorm 0.383 | train_wall 20 | gb_free 22.1 | wall 944
2025-03-09 18:25:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:25:14 | INFO | fairseq.trainer | begin training epoch 39
2025-03-09 18:25:14 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:25:21 | INFO | train_inner | epoch 039:     58 / 159 loss=0.232, ppl=1.17, wps=16031.6, ups=6.05, wpb=2651.1, bsz=175.3, num_updates=6100, lr=0.000809776, gnorm=0.32, train_wall=12, gb_free=22.1, wall=951
2025-03-09 18:25:34 | INFO | train_inner | epoch 039:    158 / 159 loss=0.241, ppl=1.18, wps=20893.2, ups=7.82, wpb=2670.2, bsz=175.6, num_updates=6200, lr=0.000803219, gnorm=0.378, train_wall=12, gb_free=22.1, wall=964
2025-03-09 18:25:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:25:37 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 0.254 | ppl 1.19 | wps 47612.8 | wpb 945.6 | bsz 63.3 | num_updates 6201 | best_loss 0.234
2025-03-09 18:25:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 6201 updates
2025-03-09 18:25:37 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint39.pt
2025-03-09 18:25:37 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint39.pt
2025-03-09 18:25:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint39.pt (epoch 39 @ 6201 updates, score 0.254) (writing took 1.5935134193859994 seconds)
2025-03-09 18:25:38 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2025-03-09 18:25:38 | INFO | train | epoch 039 | loss 0.226 | ppl 1.17 | wps 17596.7 | ups 6.6 | wpb 2665.8 | bsz 178.5 | num_updates 6201 | lr 0.000803155 | gnorm 0.334 | train_wall 20 | gb_free 22.1 | wall 968
2025-03-09 18:25:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:25:38 | INFO | fairseq.trainer | begin training epoch 40
2025-03-09 18:25:38 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:25:51 | INFO | train_inner | epoch 040:     99 / 159 loss=0.237, ppl=1.18, wps=15865.5, ups=6.01, wpb=2639.5, bsz=177.1, num_updates=6300, lr=0.000796819, gnorm=0.411, train_wall=12, gb_free=22.1, wall=981
2025-03-09 18:25:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:26:01 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 0.248 | ppl 1.19 | wps 47574.9 | wpb 945.6 | bsz 63.3 | num_updates 6360 | best_loss 0.234
2025-03-09 18:26:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 6360 updates
2025-03-09 18:26:01 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint40.pt
2025-03-09 18:26:01 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint40.pt
2025-03-09 18:26:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint40.pt (epoch 40 @ 6360 updates, score 0.248) (writing took 1.629562294576317 seconds)
2025-03-09 18:26:02 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2025-03-09 18:26:02 | INFO | train | epoch 040 | loss 0.226 | ppl 1.17 | wps 17539.2 | ups 6.58 | wpb 2665.8 | bsz 178.5 | num_updates 6360 | lr 0.000793052 | gnorm 0.358 | train_wall 20 | gb_free 22.1 | wall 992
2025-03-09 18:26:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:26:02 | INFO | fairseq.trainer | begin training epoch 41
2025-03-09 18:26:02 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:26:08 | INFO | train_inner | epoch 041:     40 / 159 loss=0.201, ppl=1.15, wps=16280.1, ups=6, wpb=2713.8, bsz=180.8, num_updates=6400, lr=0.000790569, gnorm=0.256, train_wall=12, gb_free=22.1, wall=997
2025-03-09 18:26:20 | INFO | train_inner | epoch 041:    140 / 159 loss=0.225, ppl=1.17, wps=21038.5, ups=7.85, wpb=2680.2, bsz=180.3, num_updates=6500, lr=0.000784465, gnorm=0.406, train_wall=12, gb_free=22.1, wall=1010
2025-03-09 18:26:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:26:25 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 0.271 | ppl 1.21 | wps 47665.3 | wpb 945.6 | bsz 63.3 | num_updates 6519 | best_loss 0.234
2025-03-09 18:26:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 6519 updates
2025-03-09 18:26:25 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint41.pt
2025-03-09 18:26:25 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint41.pt
2025-03-09 18:26:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint41.pt (epoch 41 @ 6519 updates, score 0.271) (writing took 1.604078663047403 seconds)
2025-03-09 18:26:27 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2025-03-09 18:26:27 | INFO | train | epoch 041 | loss 0.222 | ppl 1.17 | wps 17524.5 | ups 6.57 | wpb 2665.8 | bsz 178.5 | num_updates 6519 | lr 0.000783321 | gnorm 0.413 | train_wall 20 | gb_free 22.1 | wall 1016
2025-03-09 18:26:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:26:27 | INFO | fairseq.trainer | begin training epoch 42
2025-03-09 18:26:27 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:26:37 | INFO | train_inner | epoch 042:     81 / 159 loss=0.244, ppl=1.18, wps=15878, ups=6.07, wpb=2617.6, bsz=180, num_updates=6600, lr=0.000778499, gnorm=0.489, train_wall=12, gb_free=22.3, wall=1027
2025-03-09 18:26:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:26:49 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 0.246 | ppl 1.19 | wps 47553.1 | wpb 945.6 | bsz 63.3 | num_updates 6678 | best_loss 0.234
2025-03-09 18:26:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 6678 updates
2025-03-09 18:26:49 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint42.pt
2025-03-09 18:26:50 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint42.pt
2025-03-09 18:26:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint42.pt (epoch 42 @ 6678 updates, score 0.246) (writing took 1.6029464527964592 seconds)
2025-03-09 18:26:51 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2025-03-09 18:26:51 | INFO | train | epoch 042 | loss 0.222 | ppl 1.17 | wps 17576.7 | ups 6.59 | wpb 2665.8 | bsz 178.5 | num_updates 6678 | lr 0.000773939 | gnorm 0.357 | train_wall 20 | gb_free 22.1 | wall 1040
2025-03-09 18:26:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:26:51 | INFO | fairseq.trainer | begin training epoch 43
2025-03-09 18:26:51 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:26:54 | INFO | train_inner | epoch 043:     22 / 159 loss=0.209, ppl=1.16, wps=15755.2, ups=5.96, wpb=2643.5, bsz=170.2, num_updates=6700, lr=0.000772667, gnorm=0.347, train_wall=13, gb_free=22.1, wall=1043
2025-03-09 18:27:06 | INFO | train_inner | epoch 043:    122 / 159 loss=0.2, ppl=1.15, wps=21494.2, ups=7.95, wpb=2702.6, bsz=185.3, num_updates=6800, lr=0.000766965, gnorm=0.274, train_wall=12, gb_free=22.1, wall=1056
2025-03-09 18:27:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:27:13 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 0.245 | ppl 1.19 | wps 47492.2 | wpb 945.6 | bsz 63.3 | num_updates 6837 | best_loss 0.234
2025-03-09 18:27:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 6837 updates
2025-03-09 18:27:13 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint43.pt
2025-03-09 18:27:14 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint43.pt
2025-03-09 18:27:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint43.pt (epoch 43 @ 6837 updates, score 0.245) (writing took 1.6492255423218012 seconds)
2025-03-09 18:27:15 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2025-03-09 18:27:15 | INFO | train | epoch 043 | loss 0.203 | ppl 1.15 | wps 17550.6 | ups 6.58 | wpb 2665.8 | bsz 178.5 | num_updates 6837 | lr 0.000764887 | gnorm 0.299 | train_wall 20 | gb_free 22.1 | wall 1065
2025-03-09 18:27:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:27:15 | INFO | fairseq.trainer | begin training epoch 44
2025-03-09 18:27:15 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:27:23 | INFO | train_inner | epoch 044:     63 / 159 loss=0.194, ppl=1.14, wps=16139.1, ups=6, wpb=2689.5, bsz=179.2, num_updates=6900, lr=0.000761387, gnorm=0.234, train_wall=12, gb_free=22.1, wall=1073
2025-03-09 18:27:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:27:37 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 0.246 | ppl 1.19 | wps 47521.1 | wpb 945.6 | bsz 63.3 | num_updates 6996 | best_loss 0.234
2025-03-09 18:27:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 6996 updates
2025-03-09 18:27:37 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint44.pt
2025-03-09 18:27:38 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint44.pt
2025-03-09 18:27:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint44.pt (epoch 44 @ 6996 updates, score 0.246) (writing took 1.5681023243814707 seconds)
2025-03-09 18:27:39 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2025-03-09 18:27:39 | INFO | train | epoch 044 | loss 0.194 | ppl 1.14 | wps 17600.5 | ups 6.6 | wpb 2665.8 | bsz 178.5 | num_updates 6996 | lr 0.000756145 | gnorm 0.256 | train_wall 20 | gb_free 22.6 | wall 1089
2025-03-09 18:27:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:27:39 | INFO | fairseq.trainer | begin training epoch 45
2025-03-09 18:27:39 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:27:39 | INFO | train_inner | epoch 045:      4 / 159 loss=0.198, ppl=1.15, wps=15897.8, ups=6, wpb=2648.2, bsz=175.6, num_updates=7000, lr=0.000755929, gnorm=0.272, train_wall=12, gb_free=22.1, wall=1089
2025-03-09 18:27:52 | INFO | train_inner | epoch 045:    104 / 159 loss=0.192, ppl=1.14, wps=21296.3, ups=7.92, wpb=2688, bsz=181.6, num_updates=7100, lr=0.000750587, gnorm=0.299, train_wall=12, gb_free=22.1, wall=1102
2025-03-09 18:27:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:28:01 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 0.257 | ppl 1.19 | wps 47473.3 | wpb 945.6 | bsz 63.3 | num_updates 7155 | best_loss 0.234
2025-03-09 18:28:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 7155 updates
2025-03-09 18:28:01 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint45.pt
2025-03-09 18:28:02 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint45.pt
2025-03-09 18:28:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint45.pt (epoch 45 @ 7155 updates, score 0.257) (writing took 1.5866645239293575 seconds)
2025-03-09 18:28:03 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2025-03-09 18:28:03 | INFO | train | epoch 045 | loss 0.196 | ppl 1.15 | wps 17593.9 | ups 6.6 | wpb 2665.8 | bsz 178.5 | num_updates 7155 | lr 0.000747696 | gnorm 0.325 | train_wall 20 | gb_free 22.1 | wall 1113
2025-03-09 18:28:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:28:03 | INFO | fairseq.trainer | begin training epoch 46
2025-03-09 18:28:03 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:28:09 | INFO | train_inner | epoch 046:     45 / 159 loss=0.202, ppl=1.15, wps=15539.6, ups=6.02, wpb=2582.1, bsz=177.5, num_updates=7200, lr=0.000745356, gnorm=0.542, train_wall=12, gb_free=22.1, wall=1118
2025-03-09 18:28:21 | INFO | train_inner | epoch 046:    145 / 159 loss=0.188, ppl=1.14, wps=21301.7, ups=7.8, wpb=2730.6, bsz=176.2, num_updates=7300, lr=0.000740233, gnorm=0.254, train_wall=12, gb_free=22.1, wall=1131
2025-03-09 18:28:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:28:26 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 0.249 | ppl 1.19 | wps 47594.1 | wpb 945.6 | bsz 63.3 | num_updates 7314 | best_loss 0.234
2025-03-09 18:28:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 7314 updates
2025-03-09 18:28:26 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint46.pt
2025-03-09 18:28:26 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint46.pt
2025-03-09 18:28:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint46.pt (epoch 46 @ 7314 updates, score 0.249) (writing took 1.5485280682332814 seconds)
2025-03-09 18:28:27 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2025-03-09 18:28:27 | INFO | train | epoch 046 | loss 0.193 | ppl 1.14 | wps 17601.3 | ups 6.6 | wpb 2665.8 | bsz 178.5 | num_updates 7314 | lr 0.000739524 | gnorm 0.4 | train_wall 20 | gb_free 22.1 | wall 1137
2025-03-09 18:28:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:28:27 | INFO | fairseq.trainer | begin training epoch 47
2025-03-09 18:28:27 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:28:38 | INFO | train_inner | epoch 047:     86 / 159 loss=0.191, ppl=1.14, wps=15705.2, ups=5.96, wpb=2633.9, bsz=166.9, num_updates=7400, lr=0.000735215, gnorm=0.342, train_wall=13, gb_free=22.1, wall=1148
2025-03-09 18:28:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:28:50 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 0.26 | ppl 1.2 | wps 47465.5 | wpb 945.6 | bsz 63.3 | num_updates 7473 | best_loss 0.234
2025-03-09 18:28:50 | INFO | fairseq_cli.train | early stop since valid performance hasn't improved for last 12 runs
2025-03-09 18:28:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 7473 updates
2025-03-09 18:28:50 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint47.pt
2025-03-09 18:28:50 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint47.pt
2025-03-09 18:28:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict1000_3l_embed384/checkpoint47.pt (epoch 47 @ 7473 updates, score 0.26) (writing took 1.6103862370364368 seconds)
2025-03-09 18:28:51 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2025-03-09 18:28:51 | INFO | train | epoch 047 | loss 0.184 | ppl 1.14 | wps 17576.9 | ups 6.59 | wpb 2665.8 | bsz 178.5 | num_updates 7473 | lr 0.000731615 | gnorm 0.297 | train_wall 20 | gb_free 22.2 | wall 1161
2025-03-09 18:28:51 | INFO | fairseq_cli.train | done training in 1158.9 seconds
2025-03-09 18:29:15 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 3000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 3000, 'batch_size_valid': 64, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.001], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 12, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=3000, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=3000, batch_size_valid='64', max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='lstm', max_epoch=0, max_update=0, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[0.001], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='/home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=12, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, share_decoder_input_output_embed=False, share_all_embeddings=True, data='/home/users/s/solfrini/git/normalisation_training/data/data_norm_bin_2000', source_lang=None, target_lang=None, load_alignments=False, left_pad_source=True, left_pad_target=False, max_source_positions=1024, max_target_positions=1024, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_print_samples=False, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, warmup_updates=4000, warmup_init_lr=-1, pad=1, eos=2, unk=3, encoder_layers=3, decoder_layers=3, encoder_embed_dim=384, decoder_embed_dim=384, decoder_out_embed_dim=384, encoder_hidden_size=768, encoder_bidirectional=True, decoder_hidden_size=768, dropout=0.3, no_seed_provided=False, encoder_embed_path=None, encoder_freeze_embed=False, encoder_dropout_in=0.3, encoder_dropout_out=0.3, decoder_embed_path=None, decoder_freeze_embed=False, decoder_attention='1', decoder_dropout_in=0.3, decoder_dropout_out=0.3, adaptive_softmax_cutoff='10000,50000,200000', _name='lstm'), 'task': {'_name': 'translation', 'data': '/home/users/s/solfrini/git/normalisation_training/data/data_norm_bin_2000', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.001]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.001]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2025-03-09 18:29:15 | INFO | fairseq.tasks.translation | [trg] dictionary: 1032 types
2025-03-09 18:29:15 | INFO | fairseq.tasks.translation | [src] dictionary: 1032 types
2025-03-09 18:29:15 | INFO | fairseq_cli.train | LSTMModel(
  (encoder): LSTMEncoder(
    (dropout_in_module): FairseqDropout()
    (dropout_out_module): FairseqDropout()
    (embed_tokens): Embedding(1032, 384, padding_idx=1)
    (lstm): LSTM(384, 768, num_layers=3, dropout=0.3, bidirectional=True)
  )
  (decoder): LSTMDecoder(
    (dropout_in_module): FairseqDropout()
    (dropout_out_module): FairseqDropout()
    (embed_tokens): Embedding(1032, 384, padding_idx=1)
    (encoder_hidden_proj): Linear(in_features=1536, out_features=768, bias=True)
    (encoder_cell_proj): Linear(in_features=1536, out_features=768, bias=True)
    (layers): ModuleList(
      (0): LSTMCell(1152, 768)
      (1-2): 2 x LSTMCell(768, 768)
    )
    (attention): AttentionLayer(
      (input_proj): Linear(in_features=768, out_features=1536, bias=False)
      (output_proj): Linear(in_features=2304, out_features=768, bias=False)
    )
    (additional_fc): Linear(in_features=768, out_features=384, bias=True)
  )
)
2025-03-09 18:29:15 | INFO | fairseq_cli.train | task: TranslationTask
2025-03-09 18:29:15 | INFO | fairseq_cli.train | model: LSTMModel
2025-03-09 18:29:15 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion
2025-03-09 18:29:15 | INFO | fairseq_cli.train | num. shared model params: 56,781,696 (num. trained: 56,781,696)
2025-03-09 18:29:15 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2025-03-09 18:29:15 | INFO | fairseq.data.data_utils | loaded 7,087 examples from: /home/users/s/solfrini/git/normalisation_training/data/data_norm_bin_2000/valid.trg-src.trg
2025-03-09 18:29:15 | INFO | fairseq.data.data_utils | loaded 7,087 examples from: /home/users/s/solfrini/git/normalisation_training/data/data_norm_bin_2000/valid.trg-src.src
2025-03-09 18:29:15 | INFO | fairseq.tasks.translation | /home/users/s/solfrini/git/normalisation_training/data/data_norm_bin_2000 valid trg-src 7087 examples
2025-03-09 18:29:16 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2025-03-09 18:29:16 | INFO | fairseq.trainer | detected shared parameter: decoder.attention.input_proj.bias <- decoder.attention.output_proj.bias
2025-03-09 18:29:16 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2025-03-09 18:29:16 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.638 GB ; name = NVIDIA TITAN RTX                        
2025-03-09 18:29:16 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2025-03-09 18:29:16 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2025-03-09 18:29:16 | INFO | fairseq_cli.train | max tokens per device = 3000 and max sentences per device = None
2025-03-09 18:29:16 | INFO | fairseq.trainer | Preparing to load checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint_last.pt
2025-03-09 18:29:16 | INFO | fairseq.trainer | No existing checkpoint found /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint_last.pt
2025-03-09 18:29:16 | INFO | fairseq.trainer | loading train data for epoch 1
2025-03-09 18:29:16 | INFO | fairseq.data.data_utils | loaded 28,377 examples from: /home/users/s/solfrini/git/normalisation_training/data/data_norm_bin_2000/train.trg-src.trg
2025-03-09 18:29:16 | INFO | fairseq.data.data_utils | loaded 28,377 examples from: /home/users/s/solfrini/git/normalisation_training/data/data_norm_bin_2000/train.trg-src.src
2025-03-09 18:29:16 | INFO | fairseq.tasks.translation | /home/users/s/solfrini/git/normalisation_training/data/data_norm_bin_2000 train trg-src 28377 examples
2025-03-09 18:29:16 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp
2025-03-09 18:29:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:29:20 | INFO | fairseq.trainer | begin training epoch 1
2025-03-09 18:29:20 | INFO | fairseq_cli.train | Start iterating over samples
/home/users/s/solfrini/.local/lib/python3.9/site-packages/fairseq/tasks/fairseq_task.py:514: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
/home/users/s/solfrini/.local/lib/python3.9/site-packages/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
2025-03-09 18:29:34 | INFO | train_inner | epoch 001:    100 / 159 loss=9.633, ppl=793.9, wps=21036.1, ups=7.89, wpb=2670.4, bsz=181.8, num_updates=100, lr=2.5e-05, gnorm=2.112, train_wall=14, gb_free=22.1, wall=18
2025-03-09 18:29:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:29:43 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 8.172 | ppl 288.38 | wps 48496 | wpb 945.6 | bsz 63.3 | num_updates 159
2025-03-09 18:29:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 159 updates
2025-03-09 18:29:43 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint1.pt
2025-03-09 18:29:44 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint1.pt
2025-03-09 18:29:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint1.pt (epoch 1 @ 159 updates, score 8.172) (writing took 2.779818048235029 seconds)
2025-03-09 18:29:46 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2025-03-09 18:29:46 | INFO | train | epoch 001 | loss 9.328 | ppl 642.81 | wps 16775.1 | ups 6.3 | wpb 2665.8 | bsz 178.5 | num_updates 159 | lr 3.975e-05 | gnorm 2.211 | train_wall 21 | gb_free 22.1 | wall 30
2025-03-09 18:29:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:29:46 | INFO | fairseq.trainer | begin training epoch 2
2025-03-09 18:29:46 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:29:51 | INFO | train_inner | epoch 002:     41 / 159 loss=8.633, ppl=397, wps=15167.5, ups=5.66, wpb=2680.5, bsz=180.7, num_updates=200, lr=5e-05, gnorm=2.164, train_wall=12, gb_free=22.1, wall=35
2025-03-09 18:30:04 | INFO | train_inner | epoch 002:    141 / 159 loss=8.002, ppl=256.35, wps=20892.1, ups=7.88, wpb=2651, bsz=173.8, num_updates=300, lr=7.5e-05, gnorm=2.188, train_wall=12, gb_free=22.1, wall=48
2025-03-09 18:30:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:30:09 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.157 | ppl 142.67 | wps 47706.1 | wpb 945.6 | bsz 63.3 | num_updates 318 | best_loss 7.157
2025-03-09 18:30:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 318 updates
2025-03-09 18:30:09 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint2.pt
2025-03-09 18:30:09 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint2.pt
2025-03-09 18:30:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint2.pt (epoch 2 @ 318 updates, score 7.157) (writing took 2.813684501219541 seconds)
2025-03-09 18:30:11 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2025-03-09 18:30:11 | INFO | train | epoch 002 | loss 8.062 | ppl 267.24 | wps 16817.8 | ups 6.31 | wpb 2665.8 | bsz 178.5 | num_updates 318 | lr 7.95e-05 | gnorm 2.134 | train_wall 20 | gb_free 22.1 | wall 56
2025-03-09 18:30:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:30:11 | INFO | fairseq.trainer | begin training epoch 3
2025-03-09 18:30:11 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:30:22 | INFO | train_inner | epoch 003:     82 / 159 loss=7.375, ppl=165.95, wps=15043.7, ups=5.65, wpb=2662.2, bsz=185.3, num_updates=400, lr=0.0001, gnorm=2.528, train_wall=12, gb_free=22.1, wall=66
2025-03-09 18:30:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:30:34 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 6.222 | ppl 74.65 | wps 47640 | wpb 945.6 | bsz 63.3 | num_updates 477 | best_loss 6.222
2025-03-09 18:30:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 477 updates
2025-03-09 18:30:34 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint3.pt
2025-03-09 18:30:34 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint3.pt
2025-03-09 18:30:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint3.pt (epoch 3 @ 477 updates, score 6.222) (writing took 2.8478697119280696 seconds)
2025-03-09 18:30:37 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2025-03-09 18:30:37 | INFO | train | epoch 003 | loss 7.136 | ppl 140.69 | wps 16766.3 | ups 6.29 | wpb 2665.8 | bsz 178.5 | num_updates 477 | lr 0.00011925 | gnorm 2.559 | train_wall 20 | gb_free 22.1 | wall 81
2025-03-09 18:30:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:30:37 | INFO | fairseq.trainer | begin training epoch 4
2025-03-09 18:30:37 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:30:40 | INFO | train_inner | epoch 004:     23 / 159 loss=6.88, ppl=117.75, wps=14936.3, ups=5.59, wpb=2671.7, bsz=172.1, num_updates=500, lr=0.000125, gnorm=2.592, train_wall=12, gb_free=22.1, wall=84
2025-03-09 18:30:52 | INFO | train_inner | epoch 004:    123 / 159 loss=6.212, ppl=74.11, wps=20784.4, ups=7.83, wpb=2655.7, bsz=175.4, num_updates=600, lr=0.00015, gnorm=2.538, train_wall=12, gb_free=22.1, wall=97
2025-03-09 18:30:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:30:59 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 5.03 | ppl 32.67 | wps 47568.2 | wpb 945.6 | bsz 63.3 | num_updates 636 | best_loss 5.03
2025-03-09 18:30:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 636 updates
2025-03-09 18:30:59 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint4.pt
2025-03-09 18:31:00 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint4.pt
2025-03-09 18:31:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint4.pt (epoch 4 @ 636 updates, score 5.03) (writing took 2.782279621809721 seconds)
2025-03-09 18:31:02 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2025-03-09 18:31:02 | INFO | train | epoch 004 | loss 6.187 | ppl 72.88 | wps 16825.3 | ups 6.31 | wpb 2665.8 | bsz 178.5 | num_updates 636 | lr 0.000159 | gnorm 2.707 | train_wall 20 | gb_free 22.1 | wall 106
2025-03-09 18:31:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:31:02 | INFO | fairseq.trainer | begin training epoch 5
2025-03-09 18:31:02 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:31:10 | INFO | train_inner | epoch 005:     64 / 159 loss=5.635, ppl=49.7, wps=15025.6, ups=5.67, wpb=2647.8, bsz=184.2, num_updates=700, lr=0.000175, gnorm=3.134, train_wall=12, gb_free=22.1, wall=114
2025-03-09 18:31:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:31:24 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 4.092 | ppl 17.05 | wps 47752.4 | wpb 945.6 | bsz 63.3 | num_updates 795 | best_loss 4.092
2025-03-09 18:31:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 795 updates
2025-03-09 18:31:24 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint5.pt
2025-03-09 18:31:25 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint5.pt
2025-03-09 18:31:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint5.pt (epoch 5 @ 795 updates, score 4.092) (writing took 2.828758303076029 seconds)
2025-03-09 18:31:27 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2025-03-09 18:31:27 | INFO | train | epoch 005 | loss 5.234 | ppl 37.63 | wps 16754.3 | ups 6.28 | wpb 2665.8 | bsz 178.5 | num_updates 795 | lr 0.00019875 | gnorm 3.198 | train_wall 20 | gb_free 22.2 | wall 131
2025-03-09 18:31:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:31:27 | INFO | fairseq.trainer | begin training epoch 6
2025-03-09 18:31:27 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:31:28 | INFO | train_inner | epoch 006:      5 / 159 loss=5.013, ppl=32.28, wps=15047.2, ups=5.6, wpb=2689.2, bsz=174.2, num_updates=800, lr=0.0002, gnorm=3.282, train_wall=12, gb_free=22.1, wall=132
2025-03-09 18:31:40 | INFO | train_inner | epoch 006:    105 / 159 loss=4.48, ppl=22.31, wps=20845.3, ups=7.9, wpb=2638.6, bsz=173.4, num_updates=900, lr=0.000225, gnorm=3.18, train_wall=12, gb_free=22.1, wall=145
2025-03-09 18:31:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:31:50 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 2.852 | ppl 7.22 | wps 48265.3 | wpb 945.6 | bsz 63.3 | num_updates 954 | best_loss 2.852
2025-03-09 18:31:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 954 updates
2025-03-09 18:31:50 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint6.pt
2025-03-09 18:31:50 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint6.pt
2025-03-09 18:31:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint6.pt (epoch 6 @ 954 updates, score 2.852) (writing took 2.615515331737697 seconds)
2025-03-09 18:31:52 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2025-03-09 18:31:52 | INFO | train | epoch 006 | loss 4.29 | ppl 19.57 | wps 16906.9 | ups 6.34 | wpb 2665.8 | bsz 178.5 | num_updates 954 | lr 0.0002385 | gnorm 3.188 | train_wall 20 | gb_free 22.1 | wall 156
2025-03-09 18:31:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:31:52 | INFO | fairseq.trainer | begin training epoch 7
2025-03-09 18:31:52 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:31:58 | INFO | train_inner | epoch 007:     46 / 159 loss=3.803, ppl=13.95, wps=15320.4, ups=5.7, wpb=2685.6, bsz=187.8, num_updates=1000, lr=0.00025, gnorm=3.254, train_wall=12, gb_free=22.1, wall=162
2025-03-09 18:32:11 | INFO | train_inner | epoch 007:    146 / 159 loss=3.551, ppl=11.72, wps=20816.6, ups=7.85, wpb=2653.3, bsz=175.3, num_updates=1100, lr=0.000275, gnorm=3.599, train_wall=12, gb_free=22.1, wall=175
2025-03-09 18:32:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:32:15 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 2.347 | ppl 5.09 | wps 48271.1 | wpb 945.6 | bsz 63.3 | num_updates 1113 | best_loss 2.347
2025-03-09 18:32:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 1113 updates
2025-03-09 18:32:15 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint7.pt
2025-03-09 18:32:15 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint7.pt
2025-03-09 18:32:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint7.pt (epoch 7 @ 1113 updates, score 2.347) (writing took 2.8763309218920767 seconds)
2025-03-09 18:32:17 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2025-03-09 18:32:17 | INFO | train | epoch 007 | loss 3.528 | ppl 11.54 | wps 16741 | ups 6.28 | wpb 2665.8 | bsz 178.5 | num_updates 1113 | lr 0.00027825 | gnorm 3.472 | train_wall 20 | gb_free 22.1 | wall 182
2025-03-09 18:32:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:32:18 | INFO | fairseq.trainer | begin training epoch 8
2025-03-09 18:32:18 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:32:29 | INFO | train_inner | epoch 008:     87 / 159 loss=2.852, ppl=7.22, wps=15086.5, ups=5.62, wpb=2684, bsz=178.2, num_updates=1200, lr=0.0003, gnorm=3.007, train_wall=12, gb_free=22.2, wall=193
2025-03-09 18:32:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:32:40 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 1.83 | ppl 3.56 | wps 48281.8 | wpb 945.6 | bsz 63.3 | num_updates 1272 | best_loss 1.83
2025-03-09 18:32:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 1272 updates
2025-03-09 18:32:40 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint8.pt
2025-03-09 18:32:41 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint8.pt
2025-03-09 18:32:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint8.pt (epoch 8 @ 1272 updates, score 1.83) (writing took 2.709238847717643 seconds)
2025-03-09 18:32:43 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2025-03-09 18:32:43 | INFO | train | epoch 008 | loss 2.748 | ppl 6.72 | wps 16840.7 | ups 6.32 | wpb 2665.8 | bsz 178.5 | num_updates 1272 | lr 0.000318 | gnorm 2.991 | train_wall 20 | gb_free 22.1 | wall 207
2025-03-09 18:32:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:32:43 | INFO | fairseq.trainer | begin training epoch 9
2025-03-09 18:32:43 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:32:46 | INFO | train_inner | epoch 009:     28 / 159 loss=2.562, ppl=5.91, wps=15071.2, ups=5.65, wpb=2669.2, bsz=181.4, num_updates=1300, lr=0.000325, gnorm=3, train_wall=12, gb_free=22.1, wall=210
2025-03-09 18:32:59 | INFO | train_inner | epoch 009:    128 / 159 loss=1.998, ppl=4, wps=20881, ups=7.82, wpb=2668.7, bsz=173.6, num_updates=1400, lr=0.00035, gnorm=2.251, train_wall=12, gb_free=22.1, wall=223
2025-03-09 18:33:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:33:05 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 1.206 | ppl 2.31 | wps 48292.6 | wpb 945.6 | bsz 63.3 | num_updates 1431 | best_loss 1.206
2025-03-09 18:33:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 1431 updates
2025-03-09 18:33:05 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint9.pt
2025-03-09 18:33:06 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint9.pt
2025-03-09 18:33:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint9.pt (epoch 9 @ 1431 updates, score 1.206) (writing took 2.8263080161996186 seconds)
2025-03-09 18:33:08 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2025-03-09 18:33:08 | INFO | train | epoch 009 | loss 2.011 | ppl 4.03 | wps 16768.6 | ups 6.29 | wpb 2665.8 | bsz 178.5 | num_updates 1431 | lr 0.00035775 | gnorm 2.322 | train_wall 20 | gb_free 22.2 | wall 232
2025-03-09 18:33:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:33:08 | INFO | fairseq.trainer | begin training epoch 10
2025-03-09 18:33:08 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:33:17 | INFO | train_inner | epoch 010:     69 / 159 loss=1.592, ppl=3.01, wps=14841, ups=5.63, wpb=2637.4, bsz=179.8, num_updates=1500, lr=0.000375, gnorm=2.094, train_wall=12, gb_free=22.1, wall=241
2025-03-09 18:33:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:33:30 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 0.849 | ppl 1.8 | wps 48294.6 | wpb 945.6 | bsz 63.3 | num_updates 1590 | best_loss 0.849
2025-03-09 18:33:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 1590 updates
2025-03-09 18:33:30 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint10.pt
2025-03-09 18:33:31 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint10.pt
2025-03-09 18:33:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint10.pt (epoch 10 @ 1590 updates, score 0.849) (writing took 2.7498142290860415 seconds)
2025-03-09 18:33:33 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2025-03-09 18:33:33 | INFO | train | epoch 010 | loss 1.39 | ppl 2.62 | wps 16832 | ups 6.31 | wpb 2665.8 | bsz 178.5 | num_updates 1590 | lr 0.0003975 | gnorm 1.833 | train_wall 20 | gb_free 22.1 | wall 257
2025-03-09 18:33:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:33:33 | INFO | fairseq.trainer | begin training epoch 11
2025-03-09 18:33:33 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:33:35 | INFO | train_inner | epoch 011:     10 / 159 loss=1.314, ppl=2.49, wps=15145.8, ups=5.65, wpb=2683, bsz=175.5, num_updates=1600, lr=0.0004, gnorm=1.596, train_wall=12, gb_free=22.1, wall=259
2025-03-09 18:33:47 | INFO | train_inner | epoch 011:    110 / 159 loss=1.04, ppl=2.06, wps=21470.6, ups=8, wpb=2683.1, bsz=188.2, num_updates=1700, lr=0.000425, gnorm=1.328, train_wall=12, gb_free=22.1, wall=271
2025-03-09 18:33:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:33:56 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 0.625 | ppl 1.54 | wps 48297.9 | wpb 945.6 | bsz 63.3 | num_updates 1749 | best_loss 0.625
2025-03-09 18:33:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 1749 updates
2025-03-09 18:33:56 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint11.pt
2025-03-09 18:33:56 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint11.pt
2025-03-09 18:33:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint11.pt (epoch 11 @ 1749 updates, score 0.625) (writing took 2.7402399498969316 seconds)
2025-03-09 18:33:58 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2025-03-09 18:33:58 | INFO | train | epoch 011 | loss 1.06 | ppl 2.09 | wps 16826.8 | ups 6.31 | wpb 2665.8 | bsz 178.5 | num_updates 1749 | lr 0.00043725 | gnorm 1.383 | train_wall 20 | gb_free 22.1 | wall 283
2025-03-09 18:33:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:33:58 | INFO | fairseq.trainer | begin training epoch 12
2025-03-09 18:33:58 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:34:05 | INFO | train_inner | epoch 012:     51 / 159 loss=0.993, ppl=1.99, wps=14826.6, ups=5.59, wpb=2652.2, bsz=167.5, num_updates=1800, lr=0.00045, gnorm=1.742, train_wall=13, gb_free=22.1, wall=289
2025-03-09 18:34:17 | INFO | train_inner | epoch 012:    151 / 159 loss=0.772, ppl=1.71, wps=21408.3, ups=7.98, wpb=2683.7, bsz=185.6, num_updates=1900, lr=0.000475, gnorm=0.957, train_wall=12, gb_free=22.1, wall=302
2025-03-09 18:34:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:34:21 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 0.622 | ppl 1.54 | wps 48220.4 | wpb 945.6 | bsz 63.3 | num_updates 1908 | best_loss 0.622
2025-03-09 18:34:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 1908 updates
2025-03-09 18:34:21 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint12.pt
2025-03-09 18:34:21 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint12.pt
2025-03-09 18:34:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint12.pt (epoch 12 @ 1908 updates, score 0.622) (writing took 2.6061150450259447 seconds)
2025-03-09 18:34:23 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2025-03-09 18:34:23 | INFO | train | epoch 012 | loss 0.85 | ppl 1.8 | wps 16929.2 | ups 6.35 | wpb 2665.8 | bsz 178.5 | num_updates 1908 | lr 0.000477 | gnorm 1.401 | train_wall 20 | gb_free 22.1 | wall 308
2025-03-09 18:34:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:34:23 | INFO | fairseq.trainer | begin training epoch 13
2025-03-09 18:34:23 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:34:35 | INFO | train_inner | epoch 013:     92 / 159 loss=1.005, ppl=2.01, wps=14880, ups=5.63, wpb=2640.8, bsz=170.8, num_updates=2000, lr=0.0005, gnorm=2.064, train_wall=13, gb_free=22.3, wall=319
2025-03-09 18:34:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:34:46 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 0.503 | ppl 1.42 | wps 48278.9 | wpb 945.6 | bsz 63.3 | num_updates 2067 | best_loss 0.503
2025-03-09 18:34:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 2067 updates
2025-03-09 18:34:46 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint13.pt
2025-03-09 18:34:46 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint13.pt
2025-03-09 18:34:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint13.pt (epoch 13 @ 2067 updates, score 0.503) (writing took 2.649582325015217 seconds)
2025-03-09 18:34:48 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2025-03-09 18:34:48 | INFO | train | epoch 013 | loss 0.855 | ppl 1.81 | wps 16887.8 | ups 6.34 | wpb 2665.8 | bsz 178.5 | num_updates 2067 | lr 0.00051675 | gnorm 1.519 | train_wall 20 | gb_free 22.1 | wall 333
2025-03-09 18:34:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:34:48 | INFO | fairseq.trainer | begin training epoch 14
2025-03-09 18:34:48 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:34:53 | INFO | train_inner | epoch 014:     33 / 159 loss=0.681, ppl=1.6, wps=15151.7, ups=5.69, wpb=2660.9, bsz=179.8, num_updates=2100, lr=0.000525, gnorm=0.983, train_wall=12, gb_free=22.1, wall=337
2025-03-09 18:35:05 | INFO | train_inner | epoch 014:    133 / 159 loss=0.749, ppl=1.68, wps=21039, ups=7.92, wpb=2657.8, bsz=179.1, num_updates=2200, lr=0.00055, gnorm=1.199, train_wall=12, gb_free=22.1, wall=350
2025-03-09 18:35:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:35:11 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 0.409 | ppl 1.33 | wps 48283.1 | wpb 945.6 | bsz 63.3 | num_updates 2226 | best_loss 0.409
2025-03-09 18:35:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 2226 updates
2025-03-09 18:35:11 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint14.pt
2025-03-09 18:35:11 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint14.pt
2025-03-09 18:35:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint14.pt (epoch 14 @ 2226 updates, score 0.409) (writing took 2.6151641318574548 seconds)
2025-03-09 18:35:13 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2025-03-09 18:35:13 | INFO | train | epoch 014 | loss 0.713 | ppl 1.64 | wps 16922.2 | ups 6.35 | wpb 2665.8 | bsz 178.5 | num_updates 2226 | lr 0.0005565 | gnorm 1.127 | train_wall 20 | gb_free 22.1 | wall 358
2025-03-09 18:35:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:35:14 | INFO | fairseq.trainer | begin training epoch 15
2025-03-09 18:35:14 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:35:23 | INFO | train_inner | epoch 015:     74 / 159 loss=0.562, ppl=1.48, wps=14966.7, ups=5.66, wpb=2642, bsz=177.2, num_updates=2300, lr=0.000575, gnorm=0.866, train_wall=12, gb_free=22.1, wall=367
2025-03-09 18:35:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:35:36 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 0.381 | ppl 1.3 | wps 48327.3 | wpb 945.6 | bsz 63.3 | num_updates 2385 | best_loss 0.381
2025-03-09 18:35:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 2385 updates
2025-03-09 18:35:36 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint15.pt
2025-03-09 18:35:37 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint15.pt
2025-03-09 18:35:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint15.pt (epoch 15 @ 2385 updates, score 0.381) (writing took 2.632784969639033 seconds)
2025-03-09 18:35:39 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2025-03-09 18:35:39 | INFO | train | epoch 015 | loss 0.584 | ppl 1.5 | wps 16885 | ups 6.33 | wpb 2665.8 | bsz 178.5 | num_updates 2385 | lr 0.00059625 | gnorm 0.873 | train_wall 20 | gb_free 22.1 | wall 383
2025-03-09 18:35:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:35:39 | INFO | fairseq.trainer | begin training epoch 16
2025-03-09 18:35:39 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:35:41 | INFO | train_inner | epoch 016:     15 / 159 loss=0.564, ppl=1.48, wps=15529.7, ups=5.72, wpb=2716.4, bsz=183.6, num_updates=2400, lr=0.0006, gnorm=0.752, train_wall=12, gb_free=22.1, wall=385
2025-03-09 18:35:53 | INFO | train_inner | epoch 016:    115 / 159 loss=0.591, ppl=1.51, wps=20980.3, ups=7.87, wpb=2665.2, bsz=178.2, num_updates=2500, lr=0.000625, gnorm=1.158, train_wall=12, gb_free=22.1, wall=397
2025-03-09 18:35:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:36:01 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 0.373 | ppl 1.3 | wps 48258.3 | wpb 945.6 | bsz 63.3 | num_updates 2544 | best_loss 0.373
2025-03-09 18:36:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 2544 updates
2025-03-09 18:36:01 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint16.pt
2025-03-09 18:36:02 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint16.pt
2025-03-09 18:36:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint16.pt (epoch 16 @ 2544 updates, score 0.373) (writing took 2.538141488097608 seconds)
2025-03-09 18:36:04 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2025-03-09 18:36:04 | INFO | train | epoch 016 | loss 0.581 | ppl 1.5 | wps 16959.8 | ups 6.36 | wpb 2665.8 | bsz 178.5 | num_updates 2544 | lr 0.000636 | gnorm 1.034 | train_wall 20 | gb_free 22.2 | wall 408
2025-03-09 18:36:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:36:04 | INFO | fairseq.trainer | begin training epoch 17
2025-03-09 18:36:04 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:36:11 | INFO | train_inner | epoch 017:     56 / 159 loss=0.545, ppl=1.46, wps=15239.5, ups=5.72, wpb=2665.7, bsz=181.1, num_updates=2600, lr=0.00065, gnorm=0.803, train_wall=12, gb_free=22.1, wall=415
2025-03-09 18:36:23 | INFO | train_inner | epoch 017:    156 / 159 loss=0.543, ppl=1.46, wps=20837.7, ups=7.85, wpb=2654.1, bsz=174.3, num_updates=2700, lr=0.000675, gnorm=0.959, train_wall=12, gb_free=22.1, wall=428
2025-03-09 18:36:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:36:26 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 0.418 | ppl 1.34 | wps 47516.2 | wpb 945.6 | bsz 63.3 | num_updates 2703 | best_loss 0.373
2025-03-09 18:36:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 2703 updates
2025-03-09 18:36:26 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint17.pt
2025-03-09 18:36:27 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint17.pt
2025-03-09 18:36:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint17.pt (epoch 17 @ 2703 updates, score 0.418) (writing took 1.6165887243114412 seconds)
2025-03-09 18:36:28 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2025-03-09 18:36:28 | INFO | train | epoch 017 | loss 0.526 | ppl 1.44 | wps 17576.9 | ups 6.59 | wpb 2665.8 | bsz 178.5 | num_updates 2703 | lr 0.00067575 | gnorm 0.861 | train_wall 20 | gb_free 22.1 | wall 432
2025-03-09 18:36:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:36:28 | INFO | fairseq.trainer | begin training epoch 18
2025-03-09 18:36:28 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:36:40 | INFO | train_inner | epoch 018:     97 / 159 loss=0.502, ppl=1.42, wps=15961.8, ups=5.97, wpb=2672.9, bsz=174.6, num_updates=2800, lr=0.0007, gnorm=0.94, train_wall=12, gb_free=22.1, wall=444
2025-03-09 18:36:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:36:50 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 0.342 | ppl 1.27 | wps 47624.4 | wpb 945.6 | bsz 63.3 | num_updates 2862 | best_loss 0.342
2025-03-09 18:36:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 2862 updates
2025-03-09 18:36:50 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint18.pt
2025-03-09 18:36:51 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint18.pt
2025-03-09 18:36:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint18.pt (epoch 18 @ 2862 updates, score 0.342) (writing took 2.6242572772316635 seconds)
2025-03-09 18:36:53 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2025-03-09 18:36:53 | INFO | train | epoch 018 | loss 0.533 | ppl 1.45 | wps 16855.1 | ups 6.32 | wpb 2665.8 | bsz 178.5 | num_updates 2862 | lr 0.0007155 | gnorm 1.012 | train_wall 20 | gb_free 22.1 | wall 457
2025-03-09 18:36:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:36:53 | INFO | fairseq.trainer | begin training epoch 19
2025-03-09 18:36:53 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:36:58 | INFO | train_inner | epoch 019:     38 / 159 loss=0.529, ppl=1.44, wps=15325, ups=5.71, wpb=2682.3, bsz=184.6, num_updates=2900, lr=0.000725, gnorm=0.903, train_wall=12, gb_free=22.2, wall=462
2025-03-09 18:37:10 | INFO | train_inner | epoch 019:    138 / 159 loss=0.462, ppl=1.38, wps=20902.3, ups=7.9, wpb=2645.1, bsz=176.5, num_updates=3000, lr=0.00075, gnorm=0.729, train_wall=12, gb_free=22.1, wall=475
2025-03-09 18:37:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:37:15 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 0.31 | ppl 1.24 | wps 47505.7 | wpb 945.6 | bsz 63.3 | num_updates 3021 | best_loss 0.31
2025-03-09 18:37:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 3021 updates
2025-03-09 18:37:15 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint19.pt
2025-03-09 18:37:16 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint19.pt
2025-03-09 18:37:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint19.pt (epoch 19 @ 3021 updates, score 0.31) (writing took 2.666724630165845 seconds)
2025-03-09 18:37:18 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2025-03-09 18:37:18 | INFO | train | epoch 019 | loss 0.448 | ppl 1.36 | wps 16839.7 | ups 6.32 | wpb 2665.8 | bsz 178.5 | num_updates 3021 | lr 0.00075525 | gnorm 0.674 | train_wall 20 | gb_free 22.1 | wall 482
2025-03-09 18:37:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:37:18 | INFO | fairseq.trainer | begin training epoch 20
2025-03-09 18:37:18 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:37:28 | INFO | train_inner | epoch 020:     79 / 159 loss=0.49, ppl=1.4, wps=15114.2, ups=5.68, wpb=2661.8, bsz=183.2, num_updates=3100, lr=0.000775, gnorm=0.874, train_wall=12, gb_free=22.1, wall=492
2025-03-09 18:37:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:37:41 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 0.348 | ppl 1.27 | wps 47740.9 | wpb 945.6 | bsz 63.3 | num_updates 3180 | best_loss 0.31
2025-03-09 18:37:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 3180 updates
2025-03-09 18:37:41 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint20.pt
2025-03-09 18:37:41 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint20.pt
2025-03-09 18:37:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint20.pt (epoch 20 @ 3180 updates, score 0.348) (writing took 1.613594001159072 seconds)
2025-03-09 18:37:42 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2025-03-09 18:37:42 | INFO | train | epoch 020 | loss 0.487 | ppl 1.4 | wps 17567 | ups 6.59 | wpb 2665.8 | bsz 178.5 | num_updates 3180 | lr 0.000795 | gnorm 0.814 | train_wall 20 | gb_free 22.1 | wall 506
2025-03-09 18:37:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:37:42 | INFO | fairseq.trainer | begin training epoch 21
2025-03-09 18:37:42 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:37:45 | INFO | train_inner | epoch 021:     20 / 159 loss=0.456, ppl=1.37, wps=16105.8, ups=6, wpb=2683.9, bsz=177.3, num_updates=3200, lr=0.0008, gnorm=0.649, train_wall=12, gb_free=22.1, wall=509
2025-03-09 18:37:58 | INFO | train_inner | epoch 021:    120 / 159 loss=0.377, ppl=1.3, wps=20489.7, ups=7.72, wpb=2654.2, bsz=177.8, num_updates=3300, lr=0.000825, gnorm=0.616, train_wall=12, gb_free=22.1, wall=522
2025-03-09 18:38:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:38:05 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 0.358 | ppl 1.28 | wps 47503.7 | wpb 945.6 | bsz 63.3 | num_updates 3339 | best_loss 0.31
2025-03-09 18:38:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 3339 updates
2025-03-09 18:38:05 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint21.pt
2025-03-09 18:38:05 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint21.pt
2025-03-09 18:38:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint21.pt (epoch 21 @ 3339 updates, score 0.358) (writing took 1.5749685298651457 seconds)
2025-03-09 18:38:06 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2025-03-09 18:38:06 | INFO | train | epoch 021 | loss 0.458 | ppl 1.37 | wps 17504.6 | ups 6.57 | wpb 2665.8 | bsz 178.5 | num_updates 3339 | lr 0.00083475 | gnorm 0.722 | train_wall 20 | gb_free 22.1 | wall 531
2025-03-09 18:38:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:38:06 | INFO | fairseq.trainer | begin training epoch 22
2025-03-09 18:38:06 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:38:14 | INFO | train_inner | epoch 022:     61 / 159 loss=0.539, ppl=1.45, wps=16059.2, ups=6.05, wpb=2652.4, bsz=175.6, num_updates=3400, lr=0.00085, gnorm=0.834, train_wall=12, gb_free=22.1, wall=538
2025-03-09 18:38:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:38:29 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 0.31 | ppl 1.24 | wps 47523.5 | wpb 945.6 | bsz 63.3 | num_updates 3498 | best_loss 0.31
2025-03-09 18:38:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 3498 updates
2025-03-09 18:38:29 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint22.pt
2025-03-09 18:38:30 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint22.pt
2025-03-09 18:38:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint22.pt (epoch 22 @ 3498 updates, score 0.31) (writing took 2.7127607450820506 seconds)
2025-03-09 18:38:32 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2025-03-09 18:38:32 | INFO | train | epoch 022 | loss 0.43 | ppl 1.35 | wps 16747 | ups 6.28 | wpb 2665.8 | bsz 178.5 | num_updates 3498 | lr 0.0008745 | gnorm 0.698 | train_wall 20 | gb_free 22.1 | wall 556
2025-03-09 18:38:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:38:32 | INFO | fairseq.trainer | begin training epoch 23
2025-03-09 18:38:32 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:38:32 | INFO | train_inner | epoch 023:      2 / 159 loss=0.414, ppl=1.33, wps=15013, ups=5.59, wpb=2685.3, bsz=180.3, num_updates=3500, lr=0.000875, gnorm=0.714, train_wall=12, gb_free=22.1, wall=556
2025-03-09 18:38:45 | INFO | train_inner | epoch 023:    102 / 159 loss=0.36, ppl=1.28, wps=21270.6, ups=7.91, wpb=2687.6, bsz=179.2, num_updates=3600, lr=0.0009, gnorm=0.433, train_wall=12, gb_free=22.1, wall=569
2025-03-09 18:38:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:38:54 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 0.304 | ppl 1.23 | wps 47583.7 | wpb 945.6 | bsz 63.3 | num_updates 3657 | best_loss 0.304
2025-03-09 18:38:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 3657 updates
2025-03-09 18:38:54 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint23.pt
2025-03-09 18:38:55 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint23.pt
2025-03-09 18:38:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint23.pt (epoch 23 @ 3657 updates, score 0.304) (writing took 2.701947554014623 seconds)
2025-03-09 18:38:57 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2025-03-09 18:38:57 | INFO | train | epoch 023 | loss 0.372 | ppl 1.29 | wps 16793.2 | ups 6.3 | wpb 2665.8 | bsz 178.5 | num_updates 3657 | lr 0.00091425 | gnorm 0.537 | train_wall 20 | gb_free 22.1 | wall 581
2025-03-09 18:38:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:38:57 | INFO | fairseq.trainer | begin training epoch 24
2025-03-09 18:38:57 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:39:02 | INFO | train_inner | epoch 024:     43 / 159 loss=0.393, ppl=1.31, wps=15221.8, ups=5.63, wpb=2701.8, bsz=180.3, num_updates=3700, lr=0.000925, gnorm=0.632, train_wall=12, gb_free=22.1, wall=587
2025-03-09 18:39:15 | INFO | train_inner | epoch 024:    143 / 159 loss=0.346, ppl=1.27, wps=20821.5, ups=7.9, wpb=2637.2, bsz=180.2, num_updates=3800, lr=0.00095, gnorm=0.418, train_wall=12, gb_free=22.2, wall=599
2025-03-09 18:39:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:39:19 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 0.326 | ppl 1.25 | wps 47663.9 | wpb 945.6 | bsz 63.3 | num_updates 3816 | best_loss 0.304
2025-03-09 18:39:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 3816 updates
2025-03-09 18:39:19 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint24.pt
2025-03-09 18:39:20 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint24.pt
2025-03-09 18:39:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint24.pt (epoch 24 @ 3816 updates, score 0.326) (writing took 1.6463416712358594 seconds)
2025-03-09 18:39:21 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2025-03-09 18:39:21 | INFO | train | epoch 024 | loss 0.358 | ppl 1.28 | wps 17527.4 | ups 6.58 | wpb 2665.8 | bsz 178.5 | num_updates 3816 | lr 0.000954 | gnorm 0.481 | train_wall 20 | gb_free 22.1 | wall 605
2025-03-09 18:39:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:39:21 | INFO | fairseq.trainer | begin training epoch 25
2025-03-09 18:39:21 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:39:32 | INFO | train_inner | epoch 025:     84 / 159 loss=0.356, ppl=1.28, wps=15672.7, ups=5.98, wpb=2619.5, bsz=173.9, num_updates=3900, lr=0.000975, gnorm=0.545, train_wall=12, gb_free=22.1, wall=616
2025-03-09 18:39:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:39:44 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 0.31 | ppl 1.24 | wps 47589.8 | wpb 945.6 | bsz 63.3 | num_updates 3975 | best_loss 0.304
2025-03-09 18:39:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 3975 updates
2025-03-09 18:39:44 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint25.pt
2025-03-09 18:39:44 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint25.pt
2025-03-09 18:39:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint25.pt (epoch 25 @ 3975 updates, score 0.31) (writing took 1.599471228197217 seconds)
2025-03-09 18:39:45 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2025-03-09 18:39:45 | INFO | train | epoch 025 | loss 0.364 | ppl 1.29 | wps 17590 | ups 6.6 | wpb 2665.8 | bsz 178.5 | num_updates 3975 | lr 0.00099375 | gnorm 0.491 | train_wall 20 | gb_free 22.1 | wall 629
2025-03-09 18:39:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:39:45 | INFO | fairseq.trainer | begin training epoch 26
2025-03-09 18:39:45 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:39:49 | INFO | train_inner | epoch 026:     25 / 159 loss=0.355, ppl=1.28, wps=16036.8, ups=5.96, wpb=2690.2, bsz=175.2, num_updates=4000, lr=0.001, gnorm=0.477, train_wall=13, gb_free=22.1, wall=633
2025-03-09 18:40:01 | INFO | train_inner | epoch 026:    125 / 159 loss=0.417, ppl=1.33, wps=20689.1, ups=7.87, wpb=2630, bsz=175.2, num_updates=4100, lr=0.00098773, gnorm=0.795, train_wall=12, gb_free=22.2, wall=645
2025-03-09 18:40:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:40:08 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 0.275 | ppl 1.21 | wps 47551.1 | wpb 945.6 | bsz 63.3 | num_updates 4134 | best_loss 0.275
2025-03-09 18:40:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 4134 updates
2025-03-09 18:40:08 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint26.pt
2025-03-09 18:40:08 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint26.pt
2025-03-09 18:40:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint26.pt (epoch 26 @ 4134 updates, score 0.275) (writing took 2.6635972592048347 seconds)
2025-03-09 18:40:10 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2025-03-09 18:40:10 | INFO | train | epoch 026 | loss 0.392 | ppl 1.31 | wps 16830.9 | ups 6.31 | wpb 2665.8 | bsz 178.5 | num_updates 4134 | lr 0.000983659 | gnorm 0.668 | train_wall 20 | gb_free 22.1 | wall 655
2025-03-09 18:40:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:40:10 | INFO | fairseq.trainer | begin training epoch 27
2025-03-09 18:40:10 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:40:19 | INFO | train_inner | epoch 027:     66 / 159 loss=0.338, ppl=1.26, wps=15432.8, ups=5.67, wpb=2723.2, bsz=184.5, num_updates=4200, lr=0.0009759, gnorm=0.437, train_wall=12, gb_free=22.1, wall=663
2025-03-09 18:40:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:40:33 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 0.3 | ppl 1.23 | wps 48309.7 | wpb 945.6 | bsz 63.3 | num_updates 4293 | best_loss 0.275
2025-03-09 18:40:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 4293 updates
2025-03-09 18:40:33 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint27.pt
2025-03-09 18:40:33 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint27.pt
2025-03-09 18:40:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint27.pt (epoch 27 @ 4293 updates, score 0.3) (writing took 1.6507971999235451 seconds)
2025-03-09 18:40:35 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2025-03-09 18:40:35 | INFO | train | epoch 027 | loss 0.326 | ppl 1.25 | wps 17558.5 | ups 6.59 | wpb 2665.8 | bsz 178.5 | num_updates 4293 | lr 0.000965272 | gnorm 0.454 | train_wall 20 | gb_free 22.2 | wall 679
2025-03-09 18:40:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:40:35 | INFO | fairseq.trainer | begin training epoch 28
2025-03-09 18:40:35 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:40:35 | INFO | train_inner | epoch 028:      7 / 159 loss=0.336, ppl=1.26, wps=15679, ups=6.02, wpb=2603.6, bsz=176.7, num_updates=4300, lr=0.000964486, gnorm=0.547, train_wall=12, gb_free=22.2, wall=680
2025-03-09 18:40:48 | INFO | train_inner | epoch 028:    107 / 159 loss=0.348, ppl=1.27, wps=20840, ups=7.84, wpb=2656.6, bsz=173.3, num_updates=4400, lr=0.000953463, gnorm=0.464, train_wall=12, gb_free=22.1, wall=692
2025-03-09 18:40:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:40:57 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 0.27 | ppl 1.21 | wps 47654.4 | wpb 945.6 | bsz 63.3 | num_updates 4452 | best_loss 0.27
2025-03-09 18:40:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 4452 updates
2025-03-09 18:40:57 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint28.pt
2025-03-09 18:40:58 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint28.pt
2025-03-09 18:41:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint28.pt (epoch 28 @ 4452 updates, score 0.27) (writing took 2.702473714016378 seconds)
2025-03-09 18:41:00 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2025-03-09 18:41:00 | INFO | train | epoch 028 | loss 0.324 | ppl 1.25 | wps 16820.3 | ups 6.31 | wpb 2665.8 | bsz 178.5 | num_updates 4452 | lr 0.000947878 | gnorm 0.464 | train_wall 20 | gb_free 22.1 | wall 704
2025-03-09 18:41:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:41:00 | INFO | fairseq.trainer | begin training epoch 29
2025-03-09 18:41:00 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:41:06 | INFO | train_inner | epoch 029:     48 / 159 loss=0.278, ppl=1.21, wps=15251.7, ups=5.68, wpb=2686.6, bsz=182.8, num_updates=4500, lr=0.000942809, gnorm=0.312, train_wall=12, gb_free=22.1, wall=710
2025-03-09 18:41:19 | INFO | train_inner | epoch 029:    148 / 159 loss=0.306, ppl=1.24, wps=21196.7, ups=7.9, wpb=2682.1, bsz=182.2, num_updates=4600, lr=0.000932505, gnorm=0.488, train_wall=12, gb_free=22.1, wall=723
2025-03-09 18:41:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:41:22 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 0.265 | ppl 1.2 | wps 48093 | wpb 945.6 | bsz 63.3 | num_updates 4611 | best_loss 0.265
2025-03-09 18:41:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 4611 updates
2025-03-09 18:41:22 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint29.pt
2025-03-09 18:41:23 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint29.pt
2025-03-09 18:41:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint29.pt (epoch 29 @ 4611 updates, score 0.265) (writing took 2.6711380262859166 seconds)
2025-03-09 18:41:25 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2025-03-09 18:41:25 | INFO | train | epoch 029 | loss 0.297 | ppl 1.23 | wps 16852 | ups 6.32 | wpb 2665.8 | bsz 178.5 | num_updates 4611 | lr 0.000931392 | gnorm 0.426 | train_wall 20 | gb_free 22.1 | wall 729
2025-03-09 18:41:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:41:25 | INFO | fairseq.trainer | begin training epoch 30
2025-03-09 18:41:25 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:41:36 | INFO | train_inner | epoch 030:     89 / 159 loss=0.307, ppl=1.24, wps=14964.2, ups=5.66, wpb=2643.1, bsz=176.8, num_updates=4700, lr=0.000922531, gnorm=0.427, train_wall=12, gb_free=22.1, wall=740
2025-03-09 18:41:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:41:47 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 0.259 | ppl 1.2 | wps 47680.7 | wpb 945.6 | bsz 63.3 | num_updates 4770 | best_loss 0.259
2025-03-09 18:41:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 4770 updates
2025-03-09 18:41:47 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint30.pt
2025-03-09 18:41:48 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint30.pt
2025-03-09 18:41:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint30.pt (epoch 30 @ 4770 updates, score 0.259) (writing took 2.6694062049500644 seconds)
2025-03-09 18:41:50 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2025-03-09 18:41:50 | INFO | train | epoch 030 | loss 0.292 | ppl 1.22 | wps 16837.4 | ups 6.32 | wpb 2665.8 | bsz 178.5 | num_updates 4770 | lr 0.000915737 | gnorm 0.424 | train_wall 20 | gb_free 22.2 | wall 754
2025-03-09 18:41:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:41:50 | INFO | fairseq.trainer | begin training epoch 31
2025-03-09 18:41:50 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:41:54 | INFO | train_inner | epoch 031:     30 / 159 loss=0.276, ppl=1.21, wps=15066.8, ups=5.66, wpb=2664.2, bsz=175.8, num_updates=4800, lr=0.000912871, gnorm=0.395, train_wall=12, gb_free=22.1, wall=758
2025-03-09 18:42:07 | INFO | train_inner | epoch 031:    130 / 159 loss=0.296, ppl=1.23, wps=21033, ups=7.85, wpb=2680.5, bsz=180.5, num_updates=4900, lr=0.000903508, gnorm=0.561, train_wall=12, gb_free=22.1, wall=771
2025-03-09 18:42:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:42:13 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 0.272 | ppl 1.21 | wps 47623.2 | wpb 945.6 | bsz 63.3 | num_updates 4929 | best_loss 0.259
2025-03-09 18:42:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 4929 updates
2025-03-09 18:42:13 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint31.pt
2025-03-09 18:42:13 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint31.pt
2025-03-09 18:42:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint31.pt (epoch 31 @ 4929 updates, score 0.272) (writing took 1.599950669799 seconds)
2025-03-09 18:42:14 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2025-03-09 18:42:14 | INFO | train | epoch 031 | loss 0.298 | ppl 1.23 | wps 17602.3 | ups 6.6 | wpb 2665.8 | bsz 178.5 | num_updates 4929 | lr 0.000900846 | gnorm 0.528 | train_wall 20 | gb_free 22.1 | wall 778
2025-03-09 18:42:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:42:14 | INFO | fairseq.trainer | begin training epoch 32
2025-03-09 18:42:14 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:42:23 | INFO | train_inner | epoch 032:     71 / 159 loss=0.317, ppl=1.25, wps=15863.1, ups=5.99, wpb=2648.4, bsz=172.7, num_updates=5000, lr=0.000894427, gnorm=0.609, train_wall=12, gb_free=22.1, wall=788
2025-03-09 18:42:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:42:37 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 0.236 | ppl 1.18 | wps 47449.9 | wpb 945.6 | bsz 63.3 | num_updates 5088 | best_loss 0.236
2025-03-09 18:42:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 5088 updates
2025-03-09 18:42:37 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint32.pt
2025-03-09 18:42:37 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint32.pt
2025-03-09 18:42:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint32.pt (epoch 32 @ 5088 updates, score 0.236) (writing took 2.695512882899493 seconds)
2025-03-09 18:42:39 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2025-03-09 18:42:39 | INFO | train | epoch 032 | loss 0.284 | ppl 1.22 | wps 16828.2 | ups 6.31 | wpb 2665.8 | bsz 178.5 | num_updates 5088 | lr 0.000886659 | gnorm 0.45 | train_wall 20 | gb_free 22.1 | wall 804
2025-03-09 18:42:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:42:39 | INFO | fairseq.trainer | begin training epoch 33
2025-03-09 18:42:39 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:42:41 | INFO | train_inner | epoch 033:     12 / 159 loss=0.258, ppl=1.2, wps=15352.7, ups=5.7, wpb=2692, bsz=185.8, num_updates=5100, lr=0.000885615, gnorm=0.312, train_wall=12, gb_free=22.1, wall=805
2025-03-09 18:42:53 | INFO | train_inner | epoch 033:    112 / 159 loss=0.251, ppl=1.19, wps=21148.4, ups=7.92, wpb=2670.1, bsz=183.8, num_updates=5200, lr=0.000877058, gnorm=0.378, train_wall=12, gb_free=22.1, wall=818
2025-03-09 18:43:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:43:02 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 0.269 | ppl 1.21 | wps 47579.4 | wpb 945.6 | bsz 63.3 | num_updates 5247 | best_loss 0.236
2025-03-09 18:43:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 5247 updates
2025-03-09 18:43:02 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint33.pt
2025-03-09 18:43:02 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint33.pt
2025-03-09 18:43:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint33.pt (epoch 33 @ 5247 updates, score 0.269) (writing took 1.6013554609380662 seconds)
2025-03-09 18:43:03 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2025-03-09 18:43:03 | INFO | train | epoch 033 | loss 0.251 | ppl 1.19 | wps 17574.3 | ups 6.59 | wpb 2665.8 | bsz 178.5 | num_updates 5247 | lr 0.000873121 | gnorm 0.349 | train_wall 20 | gb_free 22.1 | wall 828
2025-03-09 18:43:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:43:03 | INFO | fairseq.trainer | begin training epoch 34
2025-03-09 18:43:03 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:43:10 | INFO | train_inner | epoch 034:     53 / 159 loss=0.246, ppl=1.19, wps=16181.7, ups=5.98, wpb=2707.5, bsz=174.3, num_updates=5300, lr=0.000868744, gnorm=0.294, train_wall=12, gb_free=22.1, wall=834
2025-03-09 18:43:23 | INFO | train_inner | epoch 034:    153 / 159 loss=0.28, ppl=1.21, wps=20717, ups=7.9, wpb=2623.9, bsz=177.1, num_updates=5400, lr=0.000860663, gnorm=0.546, train_wall=12, gb_free=22.1, wall=847
2025-03-09 18:43:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:43:26 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 0.257 | ppl 1.2 | wps 47587.9 | wpb 945.6 | bsz 63.3 | num_updates 5406 | best_loss 0.236
2025-03-09 18:43:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 5406 updates
2025-03-09 18:43:26 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint34.pt
2025-03-09 18:43:26 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint34.pt
2025-03-09 18:43:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint34.pt (epoch 34 @ 5406 updates, score 0.257) (writing took 1.6316834539175034 seconds)
2025-03-09 18:43:28 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2025-03-09 18:43:28 | INFO | train | epoch 034 | loss 0.268 | ppl 1.2 | wps 17571 | ups 6.59 | wpb 2665.8 | bsz 178.5 | num_updates 5406 | lr 0.000860185 | gnorm 0.462 | train_wall 20 | gb_free 22.1 | wall 852
2025-03-09 18:43:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:43:28 | INFO | fairseq.trainer | begin training epoch 35
2025-03-09 18:43:28 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:43:40 | INFO | train_inner | epoch 035:     94 / 159 loss=0.304, ppl=1.23, wps=15812.6, ups=5.99, wpb=2638.6, bsz=178.5, num_updates=5500, lr=0.000852803, gnorm=0.632, train_wall=12, gb_free=22.1, wall=864
2025-03-09 18:43:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:43:50 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 0.234 | ppl 1.18 | wps 47582.1 | wpb 945.6 | bsz 63.3 | num_updates 5565 | best_loss 0.234
2025-03-09 18:43:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 5565 updates
2025-03-09 18:43:50 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint35.pt
2025-03-09 18:43:51 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint35.pt
2025-03-09 18:43:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint35.pt (epoch 35 @ 5565 updates, score 0.234) (writing took 2.7865487481467426 seconds)
2025-03-09 18:43:53 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2025-03-09 18:43:53 | INFO | train | epoch 035 | loss 0.282 | ppl 1.22 | wps 16762.9 | ups 6.29 | wpb 2665.8 | bsz 178.5 | num_updates 5565 | lr 0.000847808 | gnorm 0.499 | train_wall 20 | gb_free 22.1 | wall 877
2025-03-09 18:43:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:43:53 | INFO | fairseq.trainer | begin training epoch 36
2025-03-09 18:43:53 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:43:57 | INFO | train_inner | epoch 036:     35 / 159 loss=0.246, ppl=1.19, wps=15166.5, ups=5.63, wpb=2693.2, bsz=178.7, num_updates=5600, lr=0.000845154, gnorm=0.316, train_wall=12, gb_free=22.1, wall=882
2025-03-09 18:44:10 | INFO | train_inner | epoch 036:    135 / 159 loss=0.237, ppl=1.18, wps=21035.4, ups=7.88, wpb=2668.5, bsz=179.5, num_updates=5700, lr=0.000837708, gnorm=0.337, train_wall=12, gb_free=22.1, wall=894
2025-03-09 18:44:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:44:15 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 0.251 | ppl 1.19 | wps 47559.9 | wpb 945.6 | bsz 63.3 | num_updates 5724 | best_loss 0.234
2025-03-09 18:44:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 5724 updates
2025-03-09 18:44:15 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint36.pt
2025-03-09 18:44:16 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint36.pt
2025-03-09 18:44:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint36.pt (epoch 36 @ 5724 updates, score 0.251) (writing took 1.8144618938677013 seconds)
2025-03-09 18:44:17 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2025-03-09 18:44:17 | INFO | train | epoch 036 | loss 0.242 | ppl 1.18 | wps 17428.5 | ups 6.54 | wpb 2665.8 | bsz 178.5 | num_updates 5724 | lr 0.00083595 | gnorm 0.342 | train_wall 20 | gb_free 22.1 | wall 901
2025-03-09 18:44:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:44:17 | INFO | fairseq.trainer | begin training epoch 37
2025-03-09 18:44:17 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:44:27 | INFO | train_inner | epoch 037:     76 / 159 loss=0.252, ppl=1.19, wps=15753.9, ups=5.96, wpb=2644.7, bsz=171, num_updates=5800, lr=0.000830455, gnorm=0.319, train_wall=12, gb_free=22.1, wall=911
2025-03-09 18:44:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:44:40 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 0.249 | ppl 1.19 | wps 47582 | wpb 945.6 | bsz 63.3 | num_updates 5883 | best_loss 0.234
2025-03-09 18:44:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 5883 updates
2025-03-09 18:44:40 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint37.pt
2025-03-09 18:44:40 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint37.pt
2025-03-09 18:44:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint37.pt (epoch 37 @ 5883 updates, score 0.249) (writing took 1.7436878131702542 seconds)
2025-03-09 18:44:41 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2025-03-09 18:44:41 | INFO | train | epoch 037 | loss 0.238 | ppl 1.18 | wps 17506 | ups 6.57 | wpb 2665.8 | bsz 178.5 | num_updates 5883 | lr 0.000824576 | gnorm 0.371 | train_wall 20 | gb_free 22.1 | wall 926
2025-03-09 18:44:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:44:41 | INFO | fairseq.trainer | begin training epoch 38
2025-03-09 18:44:41 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:44:44 | INFO | train_inner | epoch 038:     17 / 159 loss=0.23, ppl=1.17, wps=15939.4, ups=5.96, wpb=2674.6, bsz=184.6, num_updates=5900, lr=0.000823387, gnorm=0.411, train_wall=12, gb_free=22.1, wall=928
2025-03-09 18:44:56 | INFO | train_inner | epoch 038:    117 / 159 loss=0.232, ppl=1.17, wps=21118.1, ups=7.88, wpb=2678.7, bsz=183.6, num_updates=6000, lr=0.000816497, gnorm=0.394, train_wall=12, gb_free=22.1, wall=940
2025-03-09 18:45:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:45:04 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 0.263 | ppl 1.2 | wps 47697.2 | wpb 945.6 | bsz 63.3 | num_updates 6042 | best_loss 0.234
2025-03-09 18:45:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 6042 updates
2025-03-09 18:45:04 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint38.pt
2025-03-09 18:45:05 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint38.pt
2025-03-09 18:45:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint38.pt (epoch 38 @ 6042 updates, score 0.263) (writing took 1.742872938979417 seconds)
2025-03-09 18:45:06 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2025-03-09 18:45:06 | INFO | train | epoch 038 | loss 0.242 | ppl 1.18 | wps 17480.5 | ups 6.56 | wpb 2665.8 | bsz 178.5 | num_updates 6042 | lr 0.000813654 | gnorm 0.383 | train_wall 20 | gb_free 22.1 | wall 950
2025-03-09 18:45:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:45:06 | INFO | fairseq.trainer | begin training epoch 39
2025-03-09 18:45:06 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:45:13 | INFO | train_inner | epoch 039:     58 / 159 loss=0.232, ppl=1.17, wps=15838.5, ups=5.97, wpb=2651.1, bsz=175.3, num_updates=6100, lr=0.000809776, gnorm=0.32, train_wall=12, gb_free=22.1, wall=957
2025-03-09 18:45:26 | INFO | train_inner | epoch 039:    158 / 159 loss=0.241, ppl=1.18, wps=20898.3, ups=7.83, wpb=2670.2, bsz=175.6, num_updates=6200, lr=0.000803219, gnorm=0.378, train_wall=12, gb_free=22.1, wall=970
2025-03-09 18:45:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:45:28 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 0.254 | ppl 1.19 | wps 47571.4 | wpb 945.6 | bsz 63.3 | num_updates 6201 | best_loss 0.234
2025-03-09 18:45:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 6201 updates
2025-03-09 18:45:28 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint39.pt
2025-03-09 18:45:29 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint39.pt
2025-03-09 18:45:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint39.pt (epoch 39 @ 6201 updates, score 0.254) (writing took 1.7469381629489362 seconds)
2025-03-09 18:45:30 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2025-03-09 18:45:30 | INFO | train | epoch 039 | loss 0.226 | ppl 1.17 | wps 17465.6 | ups 6.55 | wpb 2665.8 | bsz 178.5 | num_updates 6201 | lr 0.000803155 | gnorm 0.334 | train_wall 20 | gb_free 22.1 | wall 974
2025-03-09 18:45:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:45:30 | INFO | fairseq.trainer | begin training epoch 40
2025-03-09 18:45:30 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:45:43 | INFO | train_inner | epoch 040:     99 / 159 loss=0.237, ppl=1.18, wps=15761.6, ups=5.97, wpb=2639.5, bsz=177.1, num_updates=6300, lr=0.000796819, gnorm=0.411, train_wall=12, gb_free=22.1, wall=987
2025-03-09 18:45:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:45:52 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 0.248 | ppl 1.19 | wps 47277 | wpb 945.6 | bsz 63.3 | num_updates 6360 | best_loss 0.234
2025-03-09 18:45:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 6360 updates
2025-03-09 18:45:52 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint40.pt
2025-03-09 18:45:53 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint40.pt
2025-03-09 18:45:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint40.pt (epoch 40 @ 6360 updates, score 0.248) (writing took 1.7650618087500334 seconds)
2025-03-09 18:45:54 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2025-03-09 18:45:54 | INFO | train | epoch 040 | loss 0.226 | ppl 1.17 | wps 17470.2 | ups 6.55 | wpb 2665.8 | bsz 178.5 | num_updates 6360 | lr 0.000793052 | gnorm 0.358 | train_wall 20 | gb_free 22.1 | wall 998
2025-03-09 18:45:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:45:54 | INFO | fairseq.trainer | begin training epoch 41
2025-03-09 18:45:54 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:45:59 | INFO | train_inner | epoch 041:     40 / 159 loss=0.201, ppl=1.15, wps=16151.4, ups=5.95, wpb=2713.8, bsz=180.8, num_updates=6400, lr=0.000790569, gnorm=0.256, train_wall=12, gb_free=22.1, wall=1004
2025-03-09 18:46:12 | INFO | train_inner | epoch 041:    140 / 159 loss=0.225, ppl=1.17, wps=21050.8, ups=7.85, wpb=2680.2, bsz=180.3, num_updates=6500, lr=0.000784465, gnorm=0.406, train_wall=12, gb_free=22.1, wall=1016
2025-03-09 18:46:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:46:17 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 0.271 | ppl 1.21 | wps 48364.9 | wpb 945.6 | bsz 63.3 | num_updates 6519 | best_loss 0.234
2025-03-09 18:46:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 6519 updates
2025-03-09 18:46:17 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint41.pt
2025-03-09 18:46:17 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint41.pt
2025-03-09 18:46:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint41.pt (epoch 41 @ 6519 updates, score 0.271) (writing took 1.5519782649353147 seconds)
2025-03-09 18:46:18 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2025-03-09 18:46:18 | INFO | train | epoch 041 | loss 0.222 | ppl 1.17 | wps 17599.4 | ups 6.6 | wpb 2665.8 | bsz 178.5 | num_updates 6519 | lr 0.000783321 | gnorm 0.413 | train_wall 20 | gb_free 22.1 | wall 1022
2025-03-09 18:46:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:46:18 | INFO | fairseq.trainer | begin training epoch 42
2025-03-09 18:46:18 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:46:28 | INFO | train_inner | epoch 042:     81 / 159 loss=0.244, ppl=1.18, wps=15965.2, ups=6.1, wpb=2617.6, bsz=180, num_updates=6600, lr=0.000778499, gnorm=0.489, train_wall=12, gb_free=22.3, wall=1033
2025-03-09 18:46:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:46:41 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 0.246 | ppl 1.19 | wps 47449.5 | wpb 945.6 | bsz 63.3 | num_updates 6678 | best_loss 0.234
2025-03-09 18:46:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 6678 updates
2025-03-09 18:46:41 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint42.pt
2025-03-09 18:46:41 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint42.pt
2025-03-09 18:46:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint42.pt (epoch 42 @ 6678 updates, score 0.246) (writing took 1.5891927648335695 seconds)
2025-03-09 18:46:42 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2025-03-09 18:46:42 | INFO | train | epoch 042 | loss 0.222 | ppl 1.17 | wps 17598.7 | ups 6.6 | wpb 2665.8 | bsz 178.5 | num_updates 6678 | lr 0.000773939 | gnorm 0.357 | train_wall 20 | gb_free 22.1 | wall 1047
2025-03-09 18:46:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:46:42 | INFO | fairseq.trainer | begin training epoch 43
2025-03-09 18:46:42 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:46:45 | INFO | train_inner | epoch 043:     22 / 159 loss=0.209, ppl=1.16, wps=15780.6, ups=5.97, wpb=2643.5, bsz=170.2, num_updates=6700, lr=0.000772667, gnorm=0.347, train_wall=13, gb_free=22.1, wall=1049
2025-03-09 18:46:58 | INFO | train_inner | epoch 043:    122 / 159 loss=0.2, ppl=1.15, wps=21516.9, ups=7.96, wpb=2702.6, bsz=185.3, num_updates=6800, lr=0.000766965, gnorm=0.274, train_wall=12, gb_free=22.1, wall=1062
2025-03-09 18:47:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:47:05 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 0.245 | ppl 1.19 | wps 47580.1 | wpb 945.6 | bsz 63.3 | num_updates 6837 | best_loss 0.234
2025-03-09 18:47:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 6837 updates
2025-03-09 18:47:05 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint43.pt
2025-03-09 18:47:05 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint43.pt
2025-03-09 18:47:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint43.pt (epoch 43 @ 6837 updates, score 0.245) (writing took 1.5879419250413775 seconds)
2025-03-09 18:47:06 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2025-03-09 18:47:06 | INFO | train | epoch 043 | loss 0.203 | ppl 1.15 | wps 17611.9 | ups 6.61 | wpb 2665.8 | bsz 178.5 | num_updates 6837 | lr 0.000764887 | gnorm 0.299 | train_wall 20 | gb_free 22.1 | wall 1071
2025-03-09 18:47:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:47:06 | INFO | fairseq.trainer | begin training epoch 44
2025-03-09 18:47:06 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:47:14 | INFO | train_inner | epoch 044:     63 / 159 loss=0.194, ppl=1.14, wps=16219.2, ups=6.03, wpb=2689.5, bsz=179.2, num_updates=6900, lr=0.000761387, gnorm=0.234, train_wall=12, gb_free=22.1, wall=1079
2025-03-09 18:47:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:47:29 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 0.246 | ppl 1.19 | wps 47713 | wpb 945.6 | bsz 63.3 | num_updates 6996 | best_loss 0.234
2025-03-09 18:47:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 6996 updates
2025-03-09 18:47:29 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint44.pt
2025-03-09 18:47:29 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint44.pt
2025-03-09 18:47:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint44.pt (epoch 44 @ 6996 updates, score 0.246) (writing took 1.5848591732792556 seconds)
2025-03-09 18:47:30 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2025-03-09 18:47:30 | INFO | train | epoch 044 | loss 0.194 | ppl 1.14 | wps 17607.9 | ups 6.61 | wpb 2665.8 | bsz 178.5 | num_updates 6996 | lr 0.000756145 | gnorm 0.256 | train_wall 20 | gb_free 22.6 | wall 1095
2025-03-09 18:47:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:47:30 | INFO | fairseq.trainer | begin training epoch 45
2025-03-09 18:47:30 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:47:31 | INFO | train_inner | epoch 045:      4 / 159 loss=0.198, ppl=1.15, wps=15900, ups=6, wpb=2648.2, bsz=175.6, num_updates=7000, lr=0.000755929, gnorm=0.272, train_wall=12, gb_free=22.1, wall=1095
2025-03-09 18:47:44 | INFO | train_inner | epoch 045:    104 / 159 loss=0.192, ppl=1.14, wps=21333, ups=7.94, wpb=2688, bsz=181.6, num_updates=7100, lr=0.000750587, gnorm=0.299, train_wall=12, gb_free=22.1, wall=1108
2025-03-09 18:47:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:47:53 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 0.257 | ppl 1.19 | wps 48274.7 | wpb 945.6 | bsz 63.3 | num_updates 7155 | best_loss 0.234
2025-03-09 18:47:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 7155 updates
2025-03-09 18:47:53 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint45.pt
2025-03-09 18:47:53 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint45.pt
2025-03-09 18:47:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint45.pt (epoch 45 @ 7155 updates, score 0.257) (writing took 1.5625751060433686 seconds)
2025-03-09 18:47:54 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2025-03-09 18:47:54 | INFO | train | epoch 045 | loss 0.196 | ppl 1.15 | wps 17660.9 | ups 6.63 | wpb 2665.8 | bsz 178.5 | num_updates 7155 | lr 0.000747696 | gnorm 0.325 | train_wall 20 | gb_free 22.1 | wall 1119
2025-03-09 18:47:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:47:54 | INFO | fairseq.trainer | begin training epoch 46
2025-03-09 18:47:54 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:48:00 | INFO | train_inner | epoch 046:     45 / 159 loss=0.202, ppl=1.15, wps=15605.8, ups=6.04, wpb=2582.1, bsz=177.5, num_updates=7200, lr=0.000745356, gnorm=0.542, train_wall=12, gb_free=22.1, wall=1124
2025-03-09 18:48:13 | INFO | train_inner | epoch 046:    145 / 159 loss=0.188, ppl=1.14, wps=21294.3, ups=7.8, wpb=2730.6, bsz=176.2, num_updates=7300, lr=0.000740233, gnorm=0.254, train_wall=12, gb_free=22.1, wall=1137
2025-03-09 18:48:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:48:17 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 0.249 | ppl 1.19 | wps 47577.5 | wpb 945.6 | bsz 63.3 | num_updates 7314 | best_loss 0.234
2025-03-09 18:48:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 7314 updates
2025-03-09 18:48:17 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint46.pt
2025-03-09 18:48:18 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint46.pt
2025-03-09 18:48:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint46.pt (epoch 46 @ 7314 updates, score 0.249) (writing took 1.5735835330560803 seconds)
2025-03-09 18:48:19 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2025-03-09 18:48:19 | INFO | train | epoch 046 | loss 0.193 | ppl 1.14 | wps 17580.6 | ups 6.59 | wpb 2665.8 | bsz 178.5 | num_updates 7314 | lr 0.000739524 | gnorm 0.4 | train_wall 20 | gb_free 22.1 | wall 1143
2025-03-09 18:48:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:48:19 | INFO | fairseq.trainer | begin training epoch 47
2025-03-09 18:48:19 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:48:30 | INFO | train_inner | epoch 047:     86 / 159 loss=0.191, ppl=1.14, wps=15683.4, ups=5.95, wpb=2633.9, bsz=166.9, num_updates=7400, lr=0.000735215, gnorm=0.342, train_wall=13, gb_free=22.1, wall=1154
2025-03-09 18:48:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:48:41 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 0.26 | ppl 1.2 | wps 47643.4 | wpb 945.6 | bsz 63.3 | num_updates 7473 | best_loss 0.234
2025-03-09 18:48:41 | INFO | fairseq_cli.train | early stop since valid performance hasn't improved for last 12 runs
2025-03-09 18:48:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 7473 updates
2025-03-09 18:48:41 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint47.pt
2025-03-09 18:48:42 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint47.pt
2025-03-09 18:48:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict2000_3l_embed384/checkpoint47.pt (epoch 47 @ 7473 updates, score 0.26) (writing took 1.6212362861260772 seconds)
2025-03-09 18:48:43 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2025-03-09 18:48:43 | INFO | train | epoch 047 | loss 0.184 | ppl 1.14 | wps 17584 | ups 6.6 | wpb 2665.8 | bsz 178.5 | num_updates 7473 | lr 0.000731615 | gnorm 0.297 | train_wall 20 | gb_free 22.2 | wall 1167
2025-03-09 18:48:43 | INFO | fairseq_cli.train | done training in 1163.1 seconds
2025-03-09 18:49:07 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 3000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 3000, 'batch_size_valid': 64, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.001], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 12, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=3000, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=3000, batch_size_valid='64', max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='lstm', max_epoch=0, max_update=0, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[0.001], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='/home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=12, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, share_decoder_input_output_embed=False, share_all_embeddings=True, data='/home/users/s/solfrini/git/normalisation_training/data/data_norm_bin_3000', source_lang=None, target_lang=None, load_alignments=False, left_pad_source=True, left_pad_target=False, max_source_positions=1024, max_target_positions=1024, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_print_samples=False, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, warmup_updates=4000, warmup_init_lr=-1, pad=1, eos=2, unk=3, encoder_layers=3, decoder_layers=3, encoder_embed_dim=384, decoder_embed_dim=384, decoder_out_embed_dim=384, encoder_hidden_size=768, encoder_bidirectional=True, decoder_hidden_size=768, dropout=0.3, no_seed_provided=False, encoder_embed_path=None, encoder_freeze_embed=False, encoder_dropout_in=0.3, encoder_dropout_out=0.3, decoder_embed_path=None, decoder_freeze_embed=False, decoder_attention='1', decoder_dropout_in=0.3, decoder_dropout_out=0.3, adaptive_softmax_cutoff='10000,50000,200000', _name='lstm'), 'task': {'_name': 'translation', 'data': '/home/users/s/solfrini/git/normalisation_training/data/data_norm_bin_3000', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.001]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.001]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2025-03-09 18:49:07 | INFO | fairseq.tasks.translation | [trg] dictionary: 1032 types
2025-03-09 18:49:07 | INFO | fairseq.tasks.translation | [src] dictionary: 1032 types
2025-03-09 18:49:08 | INFO | fairseq_cli.train | LSTMModel(
  (encoder): LSTMEncoder(
    (dropout_in_module): FairseqDropout()
    (dropout_out_module): FairseqDropout()
    (embed_tokens): Embedding(1032, 384, padding_idx=1)
    (lstm): LSTM(384, 768, num_layers=3, dropout=0.3, bidirectional=True)
  )
  (decoder): LSTMDecoder(
    (dropout_in_module): FairseqDropout()
    (dropout_out_module): FairseqDropout()
    (embed_tokens): Embedding(1032, 384, padding_idx=1)
    (encoder_hidden_proj): Linear(in_features=1536, out_features=768, bias=True)
    (encoder_cell_proj): Linear(in_features=1536, out_features=768, bias=True)
    (layers): ModuleList(
      (0): LSTMCell(1152, 768)
      (1-2): 2 x LSTMCell(768, 768)
    )
    (attention): AttentionLayer(
      (input_proj): Linear(in_features=768, out_features=1536, bias=False)
      (output_proj): Linear(in_features=2304, out_features=768, bias=False)
    )
    (additional_fc): Linear(in_features=768, out_features=384, bias=True)
  )
)
2025-03-09 18:49:08 | INFO | fairseq_cli.train | task: TranslationTask
2025-03-09 18:49:08 | INFO | fairseq_cli.train | model: LSTMModel
2025-03-09 18:49:08 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion
2025-03-09 18:49:08 | INFO | fairseq_cli.train | num. shared model params: 56,781,696 (num. trained: 56,781,696)
2025-03-09 18:49:08 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2025-03-09 18:49:08 | INFO | fairseq.data.data_utils | loaded 7,087 examples from: /home/users/s/solfrini/git/normalisation_training/data/data_norm_bin_3000/valid.trg-src.trg
2025-03-09 18:49:08 | INFO | fairseq.data.data_utils | loaded 7,087 examples from: /home/users/s/solfrini/git/normalisation_training/data/data_norm_bin_3000/valid.trg-src.src
2025-03-09 18:49:08 | INFO | fairseq.tasks.translation | /home/users/s/solfrini/git/normalisation_training/data/data_norm_bin_3000 valid trg-src 7087 examples
2025-03-09 18:49:09 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2025-03-09 18:49:09 | INFO | fairseq.trainer | detected shared parameter: decoder.attention.input_proj.bias <- decoder.attention.output_proj.bias
2025-03-09 18:49:09 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2025-03-09 18:49:09 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.638 GB ; name = NVIDIA TITAN RTX                        
2025-03-09 18:49:09 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2025-03-09 18:49:09 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2025-03-09 18:49:09 | INFO | fairseq_cli.train | max tokens per device = 3000 and max sentences per device = None
2025-03-09 18:49:09 | INFO | fairseq.trainer | Preparing to load checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint_last.pt
2025-03-09 18:49:09 | INFO | fairseq.trainer | No existing checkpoint found /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint_last.pt
2025-03-09 18:49:09 | INFO | fairseq.trainer | loading train data for epoch 1
2025-03-09 18:49:09 | INFO | fairseq.data.data_utils | loaded 28,377 examples from: /home/users/s/solfrini/git/normalisation_training/data/data_norm_bin_3000/train.trg-src.trg
2025-03-09 18:49:09 | INFO | fairseq.data.data_utils | loaded 28,377 examples from: /home/users/s/solfrini/git/normalisation_training/data/data_norm_bin_3000/train.trg-src.src
2025-03-09 18:49:09 | INFO | fairseq.tasks.translation | /home/users/s/solfrini/git/normalisation_training/data/data_norm_bin_3000 train trg-src 28377 examples
2025-03-09 18:49:09 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp
2025-03-09 18:49:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:49:16 | INFO | fairseq.trainer | begin training epoch 1
2025-03-09 18:49:16 | INFO | fairseq_cli.train | Start iterating over samples
/home/users/s/solfrini/.local/lib/python3.9/site-packages/fairseq/tasks/fairseq_task.py:514: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
/home/users/s/solfrini/.local/lib/python3.9/site-packages/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
2025-03-09 18:49:31 | INFO | train_inner | epoch 001:    100 / 159 loss=9.633, ppl=793.9, wps=20899.9, ups=7.83, wpb=2670.4, bsz=181.8, num_updates=100, lr=2.5e-05, gnorm=2.112, train_wall=15, gb_free=22.1, wall=22
2025-03-09 18:49:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:49:41 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 8.172 | ppl 288.38 | wps 48627.8 | wpb 945.6 | bsz 63.3 | num_updates 159
2025-03-09 18:49:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 159 updates
2025-03-09 18:49:41 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint1.pt
2025-03-09 18:49:41 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint1.pt
2025-03-09 18:49:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint1.pt (epoch 1 @ 159 updates, score 8.172) (writing took 2.8341286187060177 seconds)
2025-03-09 18:49:43 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2025-03-09 18:49:43 | INFO | train | epoch 001 | loss 9.328 | ppl 642.81 | wps 16697.8 | ups 6.27 | wpb 2665.8 | bsz 178.5 | num_updates 159 | lr 3.975e-05 | gnorm 2.211 | train_wall 22 | gb_free 22.1 | wall 35
2025-03-09 18:49:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:49:44 | INFO | fairseq.trainer | begin training epoch 2
2025-03-09 18:49:44 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:49:49 | INFO | train_inner | epoch 002:     41 / 159 loss=8.633, ppl=397, wps=15147.3, ups=5.65, wpb=2680.5, bsz=180.7, num_updates=200, lr=5e-05, gnorm=2.164, train_wall=12, gb_free=22.1, wall=40
2025-03-09 18:50:01 | INFO | train_inner | epoch 002:    141 / 159 loss=8.002, ppl=256.35, wps=20883.6, ups=7.88, wpb=2651, bsz=173.8, num_updates=300, lr=7.5e-05, gnorm=2.188, train_wall=12, gb_free=22.1, wall=53
2025-03-09 18:50:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:50:06 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.157 | ppl 142.67 | wps 47603.2 | wpb 945.6 | bsz 63.3 | num_updates 318 | best_loss 7.157
2025-03-09 18:50:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 318 updates
2025-03-09 18:50:06 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint2.pt
2025-03-09 18:50:07 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint2.pt
2025-03-09 18:50:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint2.pt (epoch 2 @ 318 updates, score 7.157) (writing took 3.0118998349644244 seconds)
2025-03-09 18:50:09 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2025-03-09 18:50:09 | INFO | train | epoch 002 | loss 8.062 | ppl 267.24 | wps 16672.6 | ups 6.25 | wpb 2665.8 | bsz 178.5 | num_updates 318 | lr 7.95e-05 | gnorm 2.134 | train_wall 20 | gb_free 22.1 | wall 60
2025-03-09 18:50:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:50:09 | INFO | fairseq.trainer | begin training epoch 3
2025-03-09 18:50:09 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:50:19 | INFO | train_inner | epoch 003:     82 / 159 loss=7.375, ppl=165.95, wps=14839, ups=5.57, wpb=2662.2, bsz=185.3, num_updates=400, lr=0.0001, gnorm=2.528, train_wall=12, gb_free=22.1, wall=71
2025-03-09 18:50:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:50:31 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 6.222 | ppl 74.65 | wps 47580.9 | wpb 945.6 | bsz 63.3 | num_updates 477 | best_loss 6.222
2025-03-09 18:50:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 477 updates
2025-03-09 18:50:31 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint3.pt
2025-03-09 18:50:32 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint3.pt
2025-03-09 18:50:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint3.pt (epoch 3 @ 477 updates, score 6.222) (writing took 2.855341076850891 seconds)
2025-03-09 18:50:34 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2025-03-09 18:50:34 | INFO | train | epoch 003 | loss 7.136 | ppl 140.69 | wps 16706 | ups 6.27 | wpb 2665.8 | bsz 178.5 | num_updates 477 | lr 0.00011925 | gnorm 2.559 | train_wall 20 | gb_free 22.1 | wall 86
2025-03-09 18:50:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:50:34 | INFO | fairseq.trainer | begin training epoch 4
2025-03-09 18:50:34 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:50:37 | INFO | train_inner | epoch 004:     23 / 159 loss=6.88, ppl=117.75, wps=14848.4, ups=5.56, wpb=2671.7, bsz=172.1, num_updates=500, lr=0.000125, gnorm=2.592, train_wall=12, gb_free=22.1, wall=88
2025-03-09 18:50:50 | INFO | train_inner | epoch 004:    123 / 159 loss=6.212, ppl=74.11, wps=20660.5, ups=7.78, wpb=2655.7, bsz=175.4, num_updates=600, lr=0.00015, gnorm=2.538, train_wall=12, gb_free=22.1, wall=101
2025-03-09 18:50:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:50:57 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 5.03 | ppl 32.67 | wps 47624.5 | wpb 945.6 | bsz 63.3 | num_updates 636 | best_loss 5.03
2025-03-09 18:50:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 636 updates
2025-03-09 18:50:57 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint4.pt
2025-03-09 18:50:57 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint4.pt
2025-03-09 18:51:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint4.pt (epoch 4 @ 636 updates, score 5.03) (writing took 2.8868594127707183 seconds)
2025-03-09 18:51:00 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2025-03-09 18:51:00 | INFO | train | epoch 004 | loss 6.187 | ppl 72.88 | wps 16650.5 | ups 6.25 | wpb 2665.8 | bsz 178.5 | num_updates 636 | lr 0.000159 | gnorm 2.707 | train_wall 20 | gb_free 22.1 | wall 111
2025-03-09 18:51:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:51:00 | INFO | fairseq.trainer | begin training epoch 5
2025-03-09 18:51:00 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:51:08 | INFO | train_inner | epoch 005:     64 / 159 loss=5.635, ppl=49.7, wps=14855.6, ups=5.61, wpb=2647.8, bsz=184.2, num_updates=700, lr=0.000175, gnorm=3.134, train_wall=12, gb_free=22.1, wall=119
2025-03-09 18:51:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:51:22 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 4.092 | ppl 17.05 | wps 47648 | wpb 945.6 | bsz 63.3 | num_updates 795 | best_loss 4.092
2025-03-09 18:51:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 795 updates
2025-03-09 18:51:22 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint5.pt
2025-03-09 18:51:23 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint5.pt
2025-03-09 18:51:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint5.pt (epoch 5 @ 795 updates, score 4.092) (writing took 2.7375044231303036 seconds)
2025-03-09 18:51:25 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2025-03-09 18:51:25 | INFO | train | epoch 005 | loss 5.234 | ppl 37.63 | wps 16739.6 | ups 6.28 | wpb 2665.8 | bsz 178.5 | num_updates 795 | lr 0.00019875 | gnorm 3.198 | train_wall 20 | gb_free 22.2 | wall 136
2025-03-09 18:51:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:51:25 | INFO | fairseq.trainer | begin training epoch 6
2025-03-09 18:51:25 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:51:26 | INFO | train_inner | epoch 006:      5 / 159 loss=5.013, ppl=32.28, wps=15066.5, ups=5.6, wpb=2689.2, bsz=174.2, num_updates=800, lr=0.0002, gnorm=3.282, train_wall=12, gb_free=22.1, wall=137
2025-03-09 18:51:39 | INFO | train_inner | epoch 006:    105 / 159 loss=4.48, ppl=22.31, wps=20658.8, ups=7.83, wpb=2638.6, bsz=173.4, num_updates=900, lr=0.000225, gnorm=3.18, train_wall=12, gb_free=22.1, wall=150
2025-03-09 18:51:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:51:48 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 2.852 | ppl 7.22 | wps 47171.8 | wpb 945.6 | bsz 63.3 | num_updates 954 | best_loss 2.852
2025-03-09 18:51:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 954 updates
2025-03-09 18:51:48 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint6.pt
2025-03-09 18:51:48 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint6.pt
2025-03-09 18:51:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint6.pt (epoch 6 @ 954 updates, score 2.852) (writing took 2.8142682779580355 seconds)
2025-03-09 18:51:51 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2025-03-09 18:51:51 | INFO | train | epoch 006 | loss 4.29 | ppl 19.57 | wps 16610.7 | ups 6.23 | wpb 2665.8 | bsz 178.5 | num_updates 954 | lr 0.0002385 | gnorm 3.188 | train_wall 20 | gb_free 22.1 | wall 162
2025-03-09 18:51:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:51:51 | INFO | fairseq.trainer | begin training epoch 7
2025-03-09 18:51:51 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:51:56 | INFO | train_inner | epoch 007:     46 / 159 loss=3.803, ppl=13.95, wps=15030, ups=5.6, wpb=2685.6, bsz=187.8, num_updates=1000, lr=0.00025, gnorm=3.254, train_wall=12, gb_free=22.1, wall=168
2025-03-09 18:52:09 | INFO | train_inner | epoch 007:    146 / 159 loss=3.551, ppl=11.72, wps=20598.8, ups=7.76, wpb=2653.3, bsz=175.3, num_updates=1100, lr=0.000275, gnorm=3.599, train_wall=12, gb_free=22.1, wall=181
2025-03-09 18:52:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:52:13 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 2.347 | ppl 5.09 | wps 48301.5 | wpb 945.6 | bsz 63.3 | num_updates 1113 | best_loss 2.347
2025-03-09 18:52:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 1113 updates
2025-03-09 18:52:13 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint7.pt
2025-03-09 18:52:14 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint7.pt
2025-03-09 18:52:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint7.pt (epoch 7 @ 1113 updates, score 2.347) (writing took 3.0744252679869533 seconds)
2025-03-09 18:52:16 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2025-03-09 18:52:16 | INFO | train | epoch 007 | loss 3.528 | ppl 11.54 | wps 16505.2 | ups 6.19 | wpb 2665.8 | bsz 178.5 | num_updates 1113 | lr 0.00027825 | gnorm 3.472 | train_wall 20 | gb_free 22.1 | wall 188
2025-03-09 18:52:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:52:16 | INFO | fairseq.trainer | begin training epoch 8
2025-03-09 18:52:16 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:52:27 | INFO | train_inner | epoch 008:     87 / 159 loss=2.852, ppl=7.22, wps=14786.4, ups=5.51, wpb=2684, bsz=178.2, num_updates=1200, lr=0.0003, gnorm=3.007, train_wall=12, gb_free=22.2, wall=199
2025-03-09 18:52:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:52:39 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 1.83 | ppl 3.56 | wps 47509 | wpb 945.6 | bsz 63.3 | num_updates 1272 | best_loss 1.83
2025-03-09 18:52:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 1272 updates
2025-03-09 18:52:39 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint8.pt
2025-03-09 18:52:40 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint8.pt
2025-03-09 18:52:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint8.pt (epoch 8 @ 1272 updates, score 1.83) (writing took 2.9528373079374433 seconds)
2025-03-09 18:52:42 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2025-03-09 18:52:42 | INFO | train | epoch 008 | loss 2.748 | ppl 6.72 | wps 16524.3 | ups 6.2 | wpb 2665.8 | bsz 178.5 | num_updates 1272 | lr 0.000318 | gnorm 2.991 | train_wall 20 | gb_free 22.1 | wall 213
2025-03-09 18:52:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:52:42 | INFO | fairseq.trainer | begin training epoch 9
2025-03-09 18:52:42 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:52:46 | INFO | train_inner | epoch 009:     28 / 159 loss=2.562, ppl=5.91, wps=14761.8, ups=5.53, wpb=2669.2, bsz=181.4, num_updates=1300, lr=0.000325, gnorm=3, train_wall=12, gb_free=22.1, wall=217
2025-03-09 18:52:58 | INFO | train_inner | epoch 009:    128 / 159 loss=1.998, ppl=4, wps=20662.9, ups=7.74, wpb=2668.7, bsz=173.6, num_updates=1400, lr=0.00035, gnorm=2.251, train_wall=12, gb_free=22.1, wall=230
2025-03-09 18:53:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:53:05 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 1.206 | ppl 2.31 | wps 47657.9 | wpb 945.6 | bsz 63.3 | num_updates 1431 | best_loss 1.206
2025-03-09 18:53:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 1431 updates
2025-03-09 18:53:05 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint9.pt
2025-03-09 18:53:05 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint9.pt
2025-03-09 18:53:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint9.pt (epoch 9 @ 1431 updates, score 1.206) (writing took 3.120200891047716 seconds)
2025-03-09 18:53:08 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2025-03-09 18:53:08 | INFO | train | epoch 009 | loss 2.011 | ppl 4.03 | wps 16423.3 | ups 6.16 | wpb 2665.8 | bsz 178.5 | num_updates 1431 | lr 0.00035775 | gnorm 2.322 | train_wall 20 | gb_free 22.2 | wall 239
2025-03-09 18:53:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:53:08 | INFO | fairseq.trainer | begin training epoch 10
2025-03-09 18:53:08 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:53:17 | INFO | train_inner | epoch 010:     69 / 159 loss=1.592, ppl=3.01, wps=14465.8, ups=5.48, wpb=2637.4, bsz=179.8, num_updates=1500, lr=0.000375, gnorm=2.094, train_wall=12, gb_free=22.1, wall=248
2025-03-09 18:53:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:53:30 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 0.849 | ppl 1.8 | wps 47701.8 | wpb 945.6 | bsz 63.3 | num_updates 1590 | best_loss 0.849
2025-03-09 18:53:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 1590 updates
2025-03-09 18:53:30 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint10.pt
2025-03-09 18:53:31 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint10.pt
2025-03-09 18:53:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint10.pt (epoch 10 @ 1590 updates, score 0.849) (writing took 2.944945485331118 seconds)
2025-03-09 18:53:33 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2025-03-09 18:53:33 | INFO | train | epoch 010 | loss 1.39 | ppl 2.62 | wps 16535.7 | ups 6.2 | wpb 2665.8 | bsz 178.5 | num_updates 1590 | lr 0.0003975 | gnorm 1.833 | train_wall 20 | gb_free 22.1 | wall 265
2025-03-09 18:53:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:53:33 | INFO | fairseq.trainer | begin training epoch 11
2025-03-09 18:53:33 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:53:35 | INFO | train_inner | epoch 011:     10 / 159 loss=1.314, ppl=2.49, wps=14835.8, ups=5.53, wpb=2683, bsz=175.5, num_updates=1600, lr=0.0004, gnorm=1.596, train_wall=12, gb_free=22.1, wall=266
2025-03-09 18:53:47 | INFO | train_inner | epoch 011:    110 / 159 loss=1.04, ppl=2.06, wps=21312.7, ups=7.94, wpb=2683.1, bsz=188.2, num_updates=1700, lr=0.000425, gnorm=1.328, train_wall=12, gb_free=22.1, wall=279
2025-03-09 18:53:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:53:56 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 0.625 | ppl 1.54 | wps 47582.9 | wpb 945.6 | bsz 63.3 | num_updates 1749 | best_loss 0.625
2025-03-09 18:53:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 1749 updates
2025-03-09 18:53:56 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint11.pt
2025-03-09 18:53:57 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint11.pt
2025-03-09 18:53:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint11.pt (epoch 11 @ 1749 updates, score 0.625) (writing took 2.9801325551234186 seconds)
2025-03-09 18:53:59 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2025-03-09 18:53:59 | INFO | train | epoch 011 | loss 1.06 | ppl 2.09 | wps 16531.4 | ups 6.2 | wpb 2665.8 | bsz 178.5 | num_updates 1749 | lr 0.00043725 | gnorm 1.383 | train_wall 20 | gb_free 22.1 | wall 290
2025-03-09 18:53:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:53:59 | INFO | fairseq.trainer | begin training epoch 12
2025-03-09 18:53:59 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:54:06 | INFO | train_inner | epoch 012:     51 / 159 loss=0.993, ppl=1.99, wps=14467.6, ups=5.45, wpb=2652.2, bsz=167.5, num_updates=1800, lr=0.00045, gnorm=1.742, train_wall=13, gb_free=22.1, wall=297
2025-03-09 18:54:18 | INFO | train_inner | epoch 012:    151 / 159 loss=0.772, ppl=1.71, wps=21144.1, ups=7.88, wpb=2683.7, bsz=185.6, num_updates=1900, lr=0.000475, gnorm=0.957, train_wall=12, gb_free=22.1, wall=310
2025-03-09 18:54:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:54:22 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 0.622 | ppl 1.54 | wps 47904.4 | wpb 945.6 | bsz 63.3 | num_updates 1908 | best_loss 0.622
2025-03-09 18:54:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 1908 updates
2025-03-09 18:54:22 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint12.pt
2025-03-09 18:54:22 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint12.pt
2025-03-09 18:54:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint12.pt (epoch 12 @ 1908 updates, score 0.622) (writing took 2.8987832008861005 seconds)
2025-03-09 18:54:25 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2025-03-09 18:54:25 | INFO | train | epoch 012 | loss 0.85 | ppl 1.8 | wps 16545.5 | ups 6.21 | wpb 2665.8 | bsz 178.5 | num_updates 1908 | lr 0.000477 | gnorm 1.401 | train_wall 20 | gb_free 22.1 | wall 316
2025-03-09 18:54:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:54:25 | INFO | fairseq.trainer | begin training epoch 13
2025-03-09 18:54:25 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:54:37 | INFO | train_inner | epoch 013:     92 / 159 loss=1.005, ppl=2.01, wps=14526.2, ups=5.5, wpb=2640.8, bsz=170.8, num_updates=2000, lr=0.0005, gnorm=2.064, train_wall=13, gb_free=22.3, wall=328
2025-03-09 18:54:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:54:47 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 0.503 | ppl 1.42 | wps 47701.7 | wpb 945.6 | bsz 63.3 | num_updates 2067 | best_loss 0.503
2025-03-09 18:54:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 2067 updates
2025-03-09 18:54:47 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint13.pt
2025-03-09 18:54:48 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint13.pt
2025-03-09 18:54:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint13.pt (epoch 13 @ 2067 updates, score 0.503) (writing took 2.9891452831216156 seconds)
2025-03-09 18:54:50 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2025-03-09 18:54:50 | INFO | train | epoch 013 | loss 0.855 | ppl 1.81 | wps 16514.9 | ups 6.2 | wpb 2665.8 | bsz 178.5 | num_updates 2067 | lr 0.00051675 | gnorm 1.519 | train_wall 20 | gb_free 22.1 | wall 342
2025-03-09 18:54:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:54:50 | INFO | fairseq.trainer | begin training epoch 14
2025-03-09 18:54:50 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:54:55 | INFO | train_inner | epoch 014:     33 / 159 loss=0.681, ppl=1.6, wps=14735.4, ups=5.54, wpb=2660.9, bsz=179.8, num_updates=2100, lr=0.000525, gnorm=0.983, train_wall=12, gb_free=22.1, wall=346
2025-03-09 18:55:07 | INFO | train_inner | epoch 014:    133 / 159 loss=0.749, ppl=1.68, wps=20801, ups=7.83, wpb=2657.8, bsz=179.1, num_updates=2200, lr=0.00055, gnorm=1.199, train_wall=12, gb_free=22.1, wall=359
2025-03-09 18:55:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:55:13 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 0.409 | ppl 1.33 | wps 47577.1 | wpb 945.6 | bsz 63.3 | num_updates 2226 | best_loss 0.409
2025-03-09 18:55:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 2226 updates
2025-03-09 18:55:13 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint14.pt
2025-03-09 18:55:14 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint14.pt
2025-03-09 18:55:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint14.pt (epoch 14 @ 2226 updates, score 0.409) (writing took 2.9843469923362136 seconds)
2025-03-09 18:55:16 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2025-03-09 18:55:16 | INFO | train | epoch 014 | loss 0.713 | ppl 1.64 | wps 16505.6 | ups 6.19 | wpb 2665.8 | bsz 178.5 | num_updates 2226 | lr 0.0005565 | gnorm 1.127 | train_wall 20 | gb_free 22.1 | wall 367
2025-03-09 18:55:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:55:16 | INFO | fairseq.trainer | begin training epoch 15
2025-03-09 18:55:16 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:55:26 | INFO | train_inner | epoch 015:     74 / 159 loss=0.562, ppl=1.48, wps=14512.5, ups=5.49, wpb=2642, bsz=177.2, num_updates=2300, lr=0.000575, gnorm=0.866, train_wall=12, gb_free=22.1, wall=377
2025-03-09 18:55:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:55:39 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 0.381 | ppl 1.3 | wps 47726.8 | wpb 945.6 | bsz 63.3 | num_updates 2385 | best_loss 0.381
2025-03-09 18:55:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 2385 updates
2025-03-09 18:55:39 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint15.pt
2025-03-09 18:55:39 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint15.pt
2025-03-09 18:55:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint15.pt (epoch 15 @ 2385 updates, score 0.381) (writing took 3.1031903931871057 seconds)
2025-03-09 18:55:42 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2025-03-09 18:55:42 | INFO | train | epoch 015 | loss 0.584 | ppl 1.5 | wps 16414.9 | ups 6.16 | wpb 2665.8 | bsz 178.5 | num_updates 2385 | lr 0.00059625 | gnorm 0.873 | train_wall 20 | gb_free 22.1 | wall 393
2025-03-09 18:55:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:55:42 | INFO | fairseq.trainer | begin training epoch 16
2025-03-09 18:55:42 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:55:44 | INFO | train_inner | epoch 016:     15 / 159 loss=0.564, ppl=1.48, wps=14995.3, ups=5.52, wpb=2716.4, bsz=183.6, num_updates=2400, lr=0.0006, gnorm=0.752, train_wall=12, gb_free=22.1, wall=395
2025-03-09 18:55:57 | INFO | train_inner | epoch 016:    115 / 159 loss=0.591, ppl=1.51, wps=20744.2, ups=7.78, wpb=2665.2, bsz=178.2, num_updates=2500, lr=0.000625, gnorm=1.158, train_wall=12, gb_free=22.1, wall=408
2025-03-09 18:56:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:56:04 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 0.373 | ppl 1.3 | wps 47643.4 | wpb 945.6 | bsz 63.3 | num_updates 2544 | best_loss 0.373
2025-03-09 18:56:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 2544 updates
2025-03-09 18:56:04 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint16.pt
2025-03-09 18:56:05 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint16.pt
2025-03-09 18:56:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint16.pt (epoch 16 @ 2544 updates, score 0.373) (writing took 2.9693808117881417 seconds)
2025-03-09 18:56:07 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2025-03-09 18:56:07 | INFO | train | epoch 016 | loss 0.581 | ppl 1.5 | wps 16510.1 | ups 6.19 | wpb 2665.8 | bsz 178.5 | num_updates 2544 | lr 0.000636 | gnorm 1.034 | train_wall 20 | gb_free 22.2 | wall 419
2025-03-09 18:56:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:56:07 | INFO | fairseq.trainer | begin training epoch 17
2025-03-09 18:56:07 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:56:15 | INFO | train_inner | epoch 017:     56 / 159 loss=0.545, ppl=1.46, wps=14722.5, ups=5.52, wpb=2665.7, bsz=181.1, num_updates=2600, lr=0.00065, gnorm=0.803, train_wall=12, gb_free=22.1, wall=426
2025-03-09 18:56:28 | INFO | train_inner | epoch 017:    156 / 159 loss=0.543, ppl=1.46, wps=20570.1, ups=7.75, wpb=2654.1, bsz=174.3, num_updates=2700, lr=0.000675, gnorm=0.959, train_wall=12, gb_free=22.1, wall=439
2025-03-09 18:56:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:56:30 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 0.418 | ppl 1.34 | wps 47654.6 | wpb 945.6 | bsz 63.3 | num_updates 2703 | best_loss 0.373
2025-03-09 18:56:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 2703 updates
2025-03-09 18:56:30 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint17.pt
2025-03-09 18:56:31 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint17.pt
2025-03-09 18:56:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint17.pt (epoch 17 @ 2703 updates, score 0.418) (writing took 1.9258901220746338 seconds)
2025-03-09 18:56:32 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2025-03-09 18:56:32 | INFO | train | epoch 017 | loss 0.526 | ppl 1.44 | wps 17178.8 | ups 6.44 | wpb 2665.8 | bsz 178.5 | num_updates 2703 | lr 0.00067575 | gnorm 0.861 | train_wall 20 | gb_free 22.1 | wall 443
2025-03-09 18:56:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:56:32 | INFO | fairseq.trainer | begin training epoch 18
2025-03-09 18:56:32 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:56:45 | INFO | train_inner | epoch 018:     97 / 159 loss=0.502, ppl=1.42, wps=15534.3, ups=5.81, wpb=2672.9, bsz=174.6, num_updates=2800, lr=0.0007, gnorm=0.94, train_wall=12, gb_free=22.1, wall=456
2025-03-09 18:56:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:56:55 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 0.342 | ppl 1.27 | wps 47478.5 | wpb 945.6 | bsz 63.3 | num_updates 2862 | best_loss 0.342
2025-03-09 18:56:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 2862 updates
2025-03-09 18:56:55 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint18.pt
2025-03-09 18:56:56 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint18.pt
2025-03-09 18:56:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint18.pt (epoch 18 @ 2862 updates, score 0.342) (writing took 2.999358414206654 seconds)
2025-03-09 18:56:58 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2025-03-09 18:56:58 | INFO | train | epoch 018 | loss 0.533 | ppl 1.45 | wps 16473.8 | ups 6.18 | wpb 2665.8 | bsz 178.5 | num_updates 2862 | lr 0.0007155 | gnorm 1.012 | train_wall 20 | gb_free 22.1 | wall 469
2025-03-09 18:56:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:56:58 | INFO | fairseq.trainer | begin training epoch 19
2025-03-09 18:56:58 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:57:03 | INFO | train_inner | epoch 019:     38 / 159 loss=0.529, ppl=1.44, wps=14907.8, ups=5.56, wpb=2682.3, bsz=184.6, num_updates=2900, lr=0.000725, gnorm=0.903, train_wall=12, gb_free=22.2, wall=474
2025-03-09 18:57:16 | INFO | train_inner | epoch 019:    138 / 159 loss=0.462, ppl=1.38, wps=20716.7, ups=7.83, wpb=2645.1, bsz=176.5, num_updates=3000, lr=0.00075, gnorm=0.729, train_wall=12, gb_free=22.1, wall=487
2025-03-09 18:57:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:57:21 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 0.31 | ppl 1.24 | wps 47483.8 | wpb 945.6 | bsz 63.3 | num_updates 3021 | best_loss 0.31
2025-03-09 18:57:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 3021 updates
2025-03-09 18:57:21 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint19.pt
2025-03-09 18:57:21 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint19.pt
2025-03-09 18:57:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint19.pt (epoch 19 @ 3021 updates, score 0.31) (writing took 2.8985078600235283 seconds)
2025-03-09 18:57:23 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2025-03-09 18:57:23 | INFO | train | epoch 019 | loss 0.448 | ppl 1.36 | wps 16557.1 | ups 6.21 | wpb 2665.8 | bsz 178.5 | num_updates 3021 | lr 0.00075525 | gnorm 0.674 | train_wall 20 | gb_free 22.1 | wall 495
2025-03-09 18:57:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:57:23 | INFO | fairseq.trainer | begin training epoch 20
2025-03-09 18:57:23 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:57:33 | INFO | train_inner | epoch 020:     79 / 159 loss=0.49, ppl=1.4, wps=14842.1, ups=5.58, wpb=2661.8, bsz=183.2, num_updates=3100, lr=0.000775, gnorm=0.874, train_wall=12, gb_free=22.1, wall=505
2025-03-09 18:57:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:57:46 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 0.348 | ppl 1.27 | wps 47470.1 | wpb 945.6 | bsz 63.3 | num_updates 3180 | best_loss 0.31
2025-03-09 18:57:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 3180 updates
2025-03-09 18:57:46 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint20.pt
2025-03-09 18:57:47 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint20.pt
2025-03-09 18:57:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint20.pt (epoch 20 @ 3180 updates, score 0.348) (writing took 1.6862134719267488 seconds)
2025-03-09 18:57:48 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2025-03-09 18:57:48 | INFO | train | epoch 020 | loss 0.487 | ppl 1.4 | wps 17441.3 | ups 6.54 | wpb 2665.8 | bsz 178.5 | num_updates 3180 | lr 0.000795 | gnorm 0.814 | train_wall 20 | gb_free 22.1 | wall 519
2025-03-09 18:57:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:57:48 | INFO | fairseq.trainer | begin training epoch 21
2025-03-09 18:57:48 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:57:50 | INFO | train_inner | epoch 021:     20 / 159 loss=0.456, ppl=1.37, wps=16007.5, ups=5.96, wpb=2683.9, bsz=177.3, num_updates=3200, lr=0.0008, gnorm=0.649, train_wall=12, gb_free=22.1, wall=521
2025-03-09 18:58:03 | INFO | train_inner | epoch 021:    120 / 159 loss=0.377, ppl=1.3, wps=20654.9, ups=7.78, wpb=2654.2, bsz=177.8, num_updates=3300, lr=0.000825, gnorm=0.616, train_wall=12, gb_free=22.1, wall=534
2025-03-09 18:58:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:58:10 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 0.358 | ppl 1.28 | wps 47614.4 | wpb 945.6 | bsz 63.3 | num_updates 3339 | best_loss 0.31
2025-03-09 18:58:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 3339 updates
2025-03-09 18:58:10 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint21.pt
2025-03-09 18:58:11 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint21.pt
2025-03-09 18:58:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint21.pt (epoch 21 @ 3339 updates, score 0.358) (writing took 1.8372504329308867 seconds)
2025-03-09 18:58:12 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2025-03-09 18:58:12 | INFO | train | epoch 021 | loss 0.458 | ppl 1.37 | wps 17417.8 | ups 6.53 | wpb 2665.8 | bsz 178.5 | num_updates 3339 | lr 0.00083475 | gnorm 0.722 | train_wall 20 | gb_free 22.1 | wall 543
2025-03-09 18:58:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:58:12 | INFO | fairseq.trainer | begin training epoch 22
2025-03-09 18:58:12 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:58:20 | INFO | train_inner | epoch 022:     61 / 159 loss=0.539, ppl=1.45, wps=15814.3, ups=5.96, wpb=2652.4, bsz=175.6, num_updates=3400, lr=0.00085, gnorm=0.834, train_wall=12, gb_free=22.1, wall=551
2025-03-09 18:58:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:58:35 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 0.31 | ppl 1.24 | wps 47530.3 | wpb 945.6 | bsz 63.3 | num_updates 3498 | best_loss 0.31
2025-03-09 18:58:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 3498 updates
2025-03-09 18:58:35 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint22.pt
2025-03-09 18:58:35 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint22.pt
2025-03-09 18:58:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint22.pt (epoch 22 @ 3498 updates, score 0.31) (writing took 2.992583126295358 seconds)
2025-03-09 18:58:38 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2025-03-09 18:58:38 | INFO | train | epoch 022 | loss 0.43 | ppl 1.35 | wps 16569.5 | ups 6.22 | wpb 2665.8 | bsz 178.5 | num_updates 3498 | lr 0.0008745 | gnorm 0.698 | train_wall 20 | gb_free 22.1 | wall 569
2025-03-09 18:58:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:58:38 | INFO | fairseq.trainer | begin training epoch 23
2025-03-09 18:58:38 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:58:38 | INFO | train_inner | epoch 023:      2 / 159 loss=0.414, ppl=1.33, wps=14809.1, ups=5.51, wpb=2685.3, bsz=180.3, num_updates=3500, lr=0.000875, gnorm=0.714, train_wall=12, gb_free=22.1, wall=569
2025-03-09 18:58:51 | INFO | train_inner | epoch 023:    102 / 159 loss=0.36, ppl=1.28, wps=21111.6, ups=7.86, wpb=2687.6, bsz=179.2, num_updates=3600, lr=0.0009, gnorm=0.433, train_wall=12, gb_free=22.1, wall=582
2025-03-09 18:58:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:59:00 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 0.304 | ppl 1.23 | wps 47627.6 | wpb 945.6 | bsz 63.3 | num_updates 3657 | best_loss 0.304
2025-03-09 18:59:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 3657 updates
2025-03-09 18:59:00 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint23.pt
2025-03-09 18:59:01 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint23.pt
2025-03-09 18:59:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint23.pt (epoch 23 @ 3657 updates, score 0.304) (writing took 2.8634399781003594 seconds)
2025-03-09 18:59:03 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2025-03-09 18:59:03 | INFO | train | epoch 023 | loss 0.372 | ppl 1.29 | wps 16602.9 | ups 6.23 | wpb 2665.8 | bsz 178.5 | num_updates 3657 | lr 0.00091425 | gnorm 0.537 | train_wall 20 | gb_free 22.1 | wall 594
2025-03-09 18:59:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:59:03 | INFO | fairseq.trainer | begin training epoch 24
2025-03-09 18:59:03 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:59:09 | INFO | train_inner | epoch 024:     43 / 159 loss=0.393, ppl=1.31, wps=15045.8, ups=5.57, wpb=2701.8, bsz=180.3, num_updates=3700, lr=0.000925, gnorm=0.632, train_wall=12, gb_free=22.1, wall=600
2025-03-09 18:59:21 | INFO | train_inner | epoch 024:    143 / 159 loss=0.346, ppl=1.27, wps=20711.5, ups=7.85, wpb=2637.2, bsz=180.2, num_updates=3800, lr=0.00095, gnorm=0.418, train_wall=12, gb_free=22.2, wall=613
2025-03-09 18:59:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:59:26 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 0.326 | ppl 1.25 | wps 47549 | wpb 945.6 | bsz 63.3 | num_updates 3816 | best_loss 0.304
2025-03-09 18:59:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 3816 updates
2025-03-09 18:59:26 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint24.pt
2025-03-09 18:59:27 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint24.pt
2025-03-09 18:59:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint24.pt (epoch 24 @ 3816 updates, score 0.326) (writing took 1.8799169808626175 seconds)
2025-03-09 18:59:28 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2025-03-09 18:59:28 | INFO | train | epoch 024 | loss 0.358 | ppl 1.28 | wps 17296.2 | ups 6.49 | wpb 2665.8 | bsz 178.5 | num_updates 3816 | lr 0.000954 | gnorm 0.481 | train_wall 20 | gb_free 22.1 | wall 619
2025-03-09 18:59:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:59:28 | INFO | fairseq.trainer | begin training epoch 25
2025-03-09 18:59:28 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:59:38 | INFO | train_inner | epoch 025:     84 / 159 loss=0.356, ppl=1.28, wps=15380.9, ups=5.87, wpb=2619.5, bsz=173.9, num_updates=3900, lr=0.000975, gnorm=0.545, train_wall=12, gb_free=22.1, wall=630
2025-03-09 18:59:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 18:59:50 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 0.31 | ppl 1.24 | wps 48278.2 | wpb 945.6 | bsz 63.3 | num_updates 3975 | best_loss 0.304
2025-03-09 18:59:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 3975 updates
2025-03-09 18:59:50 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint25.pt
2025-03-09 18:59:51 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint25.pt
2025-03-09 18:59:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint25.pt (epoch 25 @ 3975 updates, score 0.31) (writing took 1.902951329946518 seconds)
2025-03-09 18:59:52 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2025-03-09 18:59:52 | INFO | train | epoch 025 | loss 0.364 | ppl 1.29 | wps 17298.9 | ups 6.49 | wpb 2665.8 | bsz 178.5 | num_updates 3975 | lr 0.00099375 | gnorm 0.491 | train_wall 20 | gb_free 22.1 | wall 643
2025-03-09 18:59:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 18:59:52 | INFO | fairseq.trainer | begin training epoch 26
2025-03-09 18:59:52 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 18:59:56 | INFO | train_inner | epoch 026:     25 / 159 loss=0.355, ppl=1.28, wps=15687.1, ups=5.83, wpb=2690.2, bsz=175.2, num_updates=4000, lr=0.001, gnorm=0.477, train_wall=13, gb_free=22.1, wall=647
2025-03-09 19:00:08 | INFO | train_inner | epoch 026:    125 / 159 loss=0.417, ppl=1.33, wps=20575.9, ups=7.82, wpb=2630, bsz=175.2, num_updates=4100, lr=0.00098773, gnorm=0.795, train_wall=12, gb_free=22.2, wall=660
2025-03-09 19:00:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:00:15 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 0.275 | ppl 1.21 | wps 47563.7 | wpb 945.6 | bsz 63.3 | num_updates 4134 | best_loss 0.275
2025-03-09 19:00:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 4134 updates
2025-03-09 19:00:15 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint26.pt
2025-03-09 19:00:16 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint26.pt
2025-03-09 19:00:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint26.pt (epoch 26 @ 4134 updates, score 0.275) (writing took 3.1224204851314425 seconds)
2025-03-09 19:00:18 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2025-03-09 19:00:18 | INFO | train | epoch 026 | loss 0.392 | ppl 1.31 | wps 16438.7 | ups 6.17 | wpb 2665.8 | bsz 178.5 | num_updates 4134 | lr 0.000983659 | gnorm 0.668 | train_wall 20 | gb_free 22.1 | wall 669
2025-03-09 19:00:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:00:18 | INFO | fairseq.trainer | begin training epoch 27
2025-03-09 19:00:18 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:00:27 | INFO | train_inner | epoch 027:     66 / 159 loss=0.338, ppl=1.26, wps=14981.5, ups=5.5, wpb=2723.2, bsz=184.5, num_updates=4200, lr=0.0009759, gnorm=0.437, train_wall=12, gb_free=22.1, wall=678
2025-03-09 19:00:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:00:41 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 0.3 | ppl 1.23 | wps 47539.4 | wpb 945.6 | bsz 63.3 | num_updates 4293 | best_loss 0.275
2025-03-09 19:00:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 4293 updates
2025-03-09 19:00:41 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint27.pt
2025-03-09 19:00:41 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint27.pt
2025-03-09 19:00:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint27.pt (epoch 27 @ 4293 updates, score 0.3) (writing took 1.8714984548278153 seconds)
2025-03-09 19:00:42 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2025-03-09 19:00:42 | INFO | train | epoch 027 | loss 0.326 | ppl 1.25 | wps 17314.1 | ups 6.49 | wpb 2665.8 | bsz 178.5 | num_updates 4293 | lr 0.000965272 | gnorm 0.454 | train_wall 20 | gb_free 22.2 | wall 694
2025-03-09 19:00:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:00:43 | INFO | fairseq.trainer | begin training epoch 28
2025-03-09 19:00:43 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:00:43 | INFO | train_inner | epoch 028:      7 / 159 loss=0.336, ppl=1.26, wps=15376.2, ups=5.91, wpb=2603.6, bsz=176.7, num_updates=4300, lr=0.000964486, gnorm=0.547, train_wall=12, gb_free=22.2, wall=695
2025-03-09 19:00:56 | INFO | train_inner | epoch 028:    107 / 159 loss=0.348, ppl=1.27, wps=20701.3, ups=7.79, wpb=2656.6, bsz=173.3, num_updates=4400, lr=0.000953463, gnorm=0.464, train_wall=12, gb_free=22.1, wall=708
2025-03-09 19:01:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:01:05 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 0.27 | ppl 1.21 | wps 47554.7 | wpb 945.6 | bsz 63.3 | num_updates 4452 | best_loss 0.27
2025-03-09 19:01:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 4452 updates
2025-03-09 19:01:05 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint28.pt
2025-03-09 19:01:06 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint28.pt
2025-03-09 19:01:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint28.pt (epoch 28 @ 4452 updates, score 0.27) (writing took 3.059279920067638 seconds)
2025-03-09 19:01:08 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2025-03-09 19:01:08 | INFO | train | epoch 028 | loss 0.324 | ppl 1.25 | wps 16468.6 | ups 6.18 | wpb 2665.8 | bsz 178.5 | num_updates 4452 | lr 0.000947878 | gnorm 0.464 | train_wall 20 | gb_free 22.1 | wall 719
2025-03-09 19:01:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:01:08 | INFO | fairseq.trainer | begin training epoch 29
2025-03-09 19:01:08 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:01:14 | INFO | train_inner | epoch 029:     48 / 159 loss=0.278, ppl=1.21, wps=14864.7, ups=5.53, wpb=2686.6, bsz=182.8, num_updates=4500, lr=0.000942809, gnorm=0.312, train_wall=12, gb_free=22.1, wall=726
2025-03-09 19:01:27 | INFO | train_inner | epoch 029:    148 / 159 loss=0.306, ppl=1.24, wps=21010.8, ups=7.83, wpb=2682.1, bsz=182.2, num_updates=4600, lr=0.000932505, gnorm=0.488, train_wall=12, gb_free=22.1, wall=738
2025-03-09 19:01:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:01:31 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 0.265 | ppl 1.2 | wps 47563.7 | wpb 945.6 | bsz 63.3 | num_updates 4611 | best_loss 0.265
2025-03-09 19:01:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 4611 updates
2025-03-09 19:01:31 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint29.pt
2025-03-09 19:01:31 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint29.pt
2025-03-09 19:01:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint29.pt (epoch 29 @ 4611 updates, score 0.265) (writing took 2.9593140981160104 seconds)
2025-03-09 19:01:34 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2025-03-09 19:01:34 | INFO | train | epoch 029 | loss 0.297 | ppl 1.23 | wps 16540.1 | ups 6.2 | wpb 2665.8 | bsz 178.5 | num_updates 4611 | lr 0.000931392 | gnorm 0.426 | train_wall 20 | gb_free 22.1 | wall 745
2025-03-09 19:01:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:01:34 | INFO | fairseq.trainer | begin training epoch 30
2025-03-09 19:01:34 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:01:45 | INFO | train_inner | epoch 030:     89 / 159 loss=0.307, ppl=1.24, wps=14607.2, ups=5.53, wpb=2643.1, bsz=176.8, num_updates=4700, lr=0.000922531, gnorm=0.427, train_wall=12, gb_free=22.1, wall=757
2025-03-09 19:01:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:01:57 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 0.259 | ppl 1.2 | wps 47624.8 | wpb 945.6 | bsz 63.3 | num_updates 4770 | best_loss 0.259
2025-03-09 19:01:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 4770 updates
2025-03-09 19:01:57 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint30.pt
2025-03-09 19:01:57 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint30.pt
2025-03-09 19:01:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint30.pt (epoch 30 @ 4770 updates, score 0.259) (writing took 2.859037776943296 seconds)
2025-03-09 19:01:59 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2025-03-09 19:01:59 | INFO | train | epoch 030 | loss 0.292 | ppl 1.22 | wps 16593.9 | ups 6.22 | wpb 2665.8 | bsz 178.5 | num_updates 4770 | lr 0.000915737 | gnorm 0.424 | train_wall 20 | gb_free 22.2 | wall 771
2025-03-09 19:01:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:01:59 | INFO | fairseq.trainer | begin training epoch 31
2025-03-09 19:01:59 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:02:03 | INFO | train_inner | epoch 031:     30 / 159 loss=0.276, ppl=1.21, wps=14816, ups=5.56, wpb=2664.2, bsz=175.8, num_updates=4800, lr=0.000912871, gnorm=0.395, train_wall=12, gb_free=22.1, wall=774
2025-03-09 19:02:16 | INFO | train_inner | epoch 031:    130 / 159 loss=0.296, ppl=1.23, wps=20890.9, ups=7.79, wpb=2680.5, bsz=180.5, num_updates=4900, lr=0.000903508, gnorm=0.561, train_wall=12, gb_free=22.1, wall=787
2025-03-09 19:02:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:02:22 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 0.272 | ppl 1.21 | wps 47662.4 | wpb 945.6 | bsz 63.3 | num_updates 4929 | best_loss 0.259
2025-03-09 19:02:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 4929 updates
2025-03-09 19:02:22 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint31.pt
2025-03-09 19:02:23 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint31.pt
2025-03-09 19:02:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint31.pt (epoch 31 @ 4929 updates, score 0.272) (writing took 1.7237423141486943 seconds)
2025-03-09 19:02:24 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2025-03-09 19:02:24 | INFO | train | epoch 031 | loss 0.298 | ppl 1.23 | wps 17409.9 | ups 6.53 | wpb 2665.8 | bsz 178.5 | num_updates 4929 | lr 0.000900846 | gnorm 0.528 | train_wall 20 | gb_free 22.1 | wall 795
2025-03-09 19:02:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:02:24 | INFO | fairseq.trainer | begin training epoch 32
2025-03-09 19:02:24 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:02:33 | INFO | train_inner | epoch 032:     71 / 159 loss=0.317, ppl=1.25, wps=15683.8, ups=5.92, wpb=2648.4, bsz=172.7, num_updates=5000, lr=0.000894427, gnorm=0.609, train_wall=12, gb_free=22.1, wall=804
2025-03-09 19:02:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:02:46 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 0.236 | ppl 1.18 | wps 47599 | wpb 945.6 | bsz 63.3 | num_updates 5088 | best_loss 0.236
2025-03-09 19:02:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 5088 updates
2025-03-09 19:02:46 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint32.pt
2025-03-09 19:02:47 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint32.pt
2025-03-09 19:02:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint32.pt (epoch 32 @ 5088 updates, score 0.236) (writing took 3.0229634889401495 seconds)
2025-03-09 19:02:49 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2025-03-09 19:02:49 | INFO | train | epoch 032 | loss 0.284 | ppl 1.22 | wps 16557 | ups 6.21 | wpb 2665.8 | bsz 178.5 | num_updates 5088 | lr 0.000886659 | gnorm 0.45 | train_wall 20 | gb_free 22.1 | wall 821
2025-03-09 19:02:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:02:49 | INFO | fairseq.trainer | begin training epoch 33
2025-03-09 19:02:49 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:02:51 | INFO | train_inner | epoch 033:     12 / 159 loss=0.258, ppl=1.2, wps=15049.9, ups=5.59, wpb=2692, bsz=185.8, num_updates=5100, lr=0.000885615, gnorm=0.312, train_wall=12, gb_free=22.1, wall=822
2025-03-09 19:03:04 | INFO | train_inner | epoch 033:    112 / 159 loss=0.251, ppl=1.19, wps=21042.9, ups=7.88, wpb=2670.1, bsz=183.8, num_updates=5200, lr=0.000877058, gnorm=0.378, train_wall=12, gb_free=22.1, wall=835
2025-03-09 19:03:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:03:12 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 0.269 | ppl 1.21 | wps 47577 | wpb 945.6 | bsz 63.3 | num_updates 5247 | best_loss 0.236
2025-03-09 19:03:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 5247 updates
2025-03-09 19:03:12 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint33.pt
2025-03-09 19:03:13 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint33.pt
2025-03-09 19:03:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint33.pt (epoch 33 @ 5247 updates, score 0.269) (writing took 1.8504713871516287 seconds)
2025-03-09 19:03:14 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2025-03-09 19:03:14 | INFO | train | epoch 033 | loss 0.251 | ppl 1.19 | wps 17336.3 | ups 6.5 | wpb 2665.8 | bsz 178.5 | num_updates 5247 | lr 0.000873121 | gnorm 0.349 | train_wall 20 | gb_free 22.1 | wall 845
2025-03-09 19:03:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:03:14 | INFO | fairseq.trainer | begin training epoch 34
2025-03-09 19:03:14 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:03:21 | INFO | train_inner | epoch 034:     53 / 159 loss=0.246, ppl=1.19, wps=15867.7, ups=5.86, wpb=2707.5, bsz=174.3, num_updates=5300, lr=0.000868744, gnorm=0.294, train_wall=12, gb_free=22.1, wall=852
2025-03-09 19:03:33 | INFO | train_inner | epoch 034:    153 / 159 loss=0.28, ppl=1.21, wps=20612.1, ups=7.86, wpb=2623.9, bsz=177.1, num_updates=5400, lr=0.000860663, gnorm=0.546, train_wall=12, gb_free=22.1, wall=865
2025-03-09 19:03:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:03:36 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 0.257 | ppl 1.2 | wps 47516.7 | wpb 945.6 | bsz 63.3 | num_updates 5406 | best_loss 0.236
2025-03-09 19:03:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 5406 updates
2025-03-09 19:03:36 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint34.pt
2025-03-09 19:03:37 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint34.pt
2025-03-09 19:03:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint34.pt (epoch 34 @ 5406 updates, score 0.257) (writing took 1.7449268982745707 seconds)
2025-03-09 19:03:38 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2025-03-09 19:03:38 | INFO | train | epoch 034 | loss 0.268 | ppl 1.2 | wps 17402.8 | ups 6.53 | wpb 2665.8 | bsz 178.5 | num_updates 5406 | lr 0.000860185 | gnorm 0.462 | train_wall 20 | gb_free 22.1 | wall 869
2025-03-09 19:03:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:03:38 | INFO | fairseq.trainer | begin training epoch 35
2025-03-09 19:03:38 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:03:50 | INFO | train_inner | epoch 035:     94 / 159 loss=0.304, ppl=1.23, wps=15640.7, ups=5.93, wpb=2638.6, bsz=178.5, num_updates=5500, lr=0.000852803, gnorm=0.632, train_wall=12, gb_free=22.1, wall=881
2025-03-09 19:03:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:04:01 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 0.234 | ppl 1.18 | wps 47696.4 | wpb 945.6 | bsz 63.3 | num_updates 5565 | best_loss 0.234
2025-03-09 19:04:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 5565 updates
2025-03-09 19:04:01 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint35.pt
2025-03-09 19:04:01 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint35.pt
2025-03-09 19:04:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint35.pt (epoch 35 @ 5565 updates, score 0.234) (writing took 2.963112487923354 seconds)
2025-03-09 19:04:04 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2025-03-09 19:04:04 | INFO | train | epoch 035 | loss 0.282 | ppl 1.22 | wps 16576.2 | ups 6.22 | wpb 2665.8 | bsz 178.5 | num_updates 5565 | lr 0.000847808 | gnorm 0.499 | train_wall 20 | gb_free 22.1 | wall 895
2025-03-09 19:04:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:04:04 | INFO | fairseq.trainer | begin training epoch 36
2025-03-09 19:04:04 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:04:08 | INFO | train_inner | epoch 036:     35 / 159 loss=0.246, ppl=1.19, wps=14954.8, ups=5.55, wpb=2693.2, bsz=178.7, num_updates=5600, lr=0.000845154, gnorm=0.316, train_wall=12, gb_free=22.1, wall=899
2025-03-09 19:04:21 | INFO | train_inner | epoch 036:    135 / 159 loss=0.237, ppl=1.18, wps=20943.3, ups=7.85, wpb=2668.5, bsz=179.5, num_updates=5700, lr=0.000837708, gnorm=0.337, train_wall=12, gb_free=22.1, wall=912
2025-03-09 19:04:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:04:26 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 0.251 | ppl 1.19 | wps 47509 | wpb 945.6 | bsz 63.3 | num_updates 5724 | best_loss 0.234
2025-03-09 19:04:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 5724 updates
2025-03-09 19:04:26 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint36.pt
2025-03-09 19:04:27 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint36.pt
2025-03-09 19:04:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint36.pt (epoch 36 @ 5724 updates, score 0.251) (writing took 1.7296638451516628 seconds)
2025-03-09 19:04:28 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2025-03-09 19:04:28 | INFO | train | epoch 036 | loss 0.242 | ppl 1.18 | wps 17422.6 | ups 6.54 | wpb 2665.8 | bsz 178.5 | num_updates 5724 | lr 0.00083595 | gnorm 0.342 | train_wall 20 | gb_free 22.1 | wall 919
2025-03-09 19:04:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:04:28 | INFO | fairseq.trainer | begin training epoch 37
2025-03-09 19:04:28 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:04:38 | INFO | train_inner | epoch 037:     76 / 159 loss=0.252, ppl=1.19, wps=15734.1, ups=5.95, wpb=2644.7, bsz=171, num_updates=5800, lr=0.000830455, gnorm=0.319, train_wall=12, gb_free=22.1, wall=929
2025-03-09 19:04:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:04:51 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 0.249 | ppl 1.19 | wps 47407.4 | wpb 945.6 | bsz 63.3 | num_updates 5883 | best_loss 0.234
2025-03-09 19:04:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 5883 updates
2025-03-09 19:04:51 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint37.pt
2025-03-09 19:04:51 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint37.pt
2025-03-09 19:04:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint37.pt (epoch 37 @ 5883 updates, score 0.249) (writing took 1.7582590905949473 seconds)
2025-03-09 19:04:52 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2025-03-09 19:04:52 | INFO | train | epoch 037 | loss 0.238 | ppl 1.18 | wps 17369.7 | ups 6.52 | wpb 2665.8 | bsz 178.5 | num_updates 5883 | lr 0.000824576 | gnorm 0.371 | train_wall 20 | gb_free 22.1 | wall 944
2025-03-09 19:04:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:04:53 | INFO | fairseq.trainer | begin training epoch 38
2025-03-09 19:04:53 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:04:55 | INFO | train_inner | epoch 038:     17 / 159 loss=0.23, ppl=1.17, wps=15751.8, ups=5.89, wpb=2674.6, bsz=184.6, num_updates=5900, lr=0.000823387, gnorm=0.411, train_wall=12, gb_free=22.1, wall=946
2025-03-09 19:05:07 | INFO | train_inner | epoch 038:    117 / 159 loss=0.232, ppl=1.17, wps=21129.6, ups=7.89, wpb=2678.7, bsz=183.6, num_updates=6000, lr=0.000816497, gnorm=0.394, train_wall=12, gb_free=22.1, wall=959
2025-03-09 19:05:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:05:15 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 0.263 | ppl 1.2 | wps 47549.9 | wpb 945.6 | bsz 63.3 | num_updates 6042 | best_loss 0.234
2025-03-09 19:05:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 6042 updates
2025-03-09 19:05:15 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint38.pt
2025-03-09 19:05:16 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint38.pt
2025-03-09 19:05:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint38.pt (epoch 38 @ 6042 updates, score 0.263) (writing took 1.68161896802485 seconds)
2025-03-09 19:05:17 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2025-03-09 19:05:17 | INFO | train | epoch 038 | loss 0.242 | ppl 1.18 | wps 17446.7 | ups 6.54 | wpb 2665.8 | bsz 178.5 | num_updates 6042 | lr 0.000813654 | gnorm 0.383 | train_wall 20 | gb_free 22.1 | wall 968
2025-03-09 19:05:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:05:17 | INFO | fairseq.trainer | begin training epoch 39
2025-03-09 19:05:17 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:05:24 | INFO | train_inner | epoch 039:     58 / 159 loss=0.232, ppl=1.17, wps=15928.9, ups=6.01, wpb=2651.1, bsz=175.3, num_updates=6100, lr=0.000809776, gnorm=0.32, train_wall=12, gb_free=22.1, wall=975
2025-03-09 19:05:37 | INFO | train_inner | epoch 039:    158 / 159 loss=0.241, ppl=1.18, wps=20913, ups=7.83, wpb=2670.2, bsz=175.6, num_updates=6200, lr=0.000803219, gnorm=0.378, train_wall=12, gb_free=22.1, wall=988
2025-03-09 19:05:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:05:39 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 0.254 | ppl 1.19 | wps 47587.6 | wpb 945.6 | bsz 63.3 | num_updates 6201 | best_loss 0.234
2025-03-09 19:05:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 6201 updates
2025-03-09 19:05:39 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint39.pt
2025-03-09 19:05:40 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint39.pt
2025-03-09 19:05:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint39.pt (epoch 39 @ 6201 updates, score 0.254) (writing took 1.5933216866105795 seconds)
2025-03-09 19:05:41 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2025-03-09 19:05:41 | INFO | train | epoch 039 | loss 0.226 | ppl 1.17 | wps 17610.9 | ups 6.61 | wpb 2665.8 | bsz 178.5 | num_updates 6201 | lr 0.000803155 | gnorm 0.334 | train_wall 20 | gb_free 22.1 | wall 992
2025-03-09 19:05:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:05:41 | INFO | fairseq.trainer | begin training epoch 40
2025-03-09 19:05:41 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:05:53 | INFO | train_inner | epoch 040:     99 / 159 loss=0.237, ppl=1.18, wps=15900.2, ups=6.02, wpb=2639.5, bsz=177.1, num_updates=6300, lr=0.000796819, gnorm=0.411, train_wall=12, gb_free=22.1, wall=1005
2025-03-09 19:06:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:06:03 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 0.248 | ppl 1.19 | wps 47629.7 | wpb 945.6 | bsz 63.3 | num_updates 6360 | best_loss 0.234
2025-03-09 19:06:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 6360 updates
2025-03-09 19:06:03 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint40.pt
2025-03-09 19:06:04 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint40.pt
2025-03-09 19:06:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint40.pt (epoch 40 @ 6360 updates, score 0.248) (writing took 1.618535145651549 seconds)
2025-03-09 19:06:05 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2025-03-09 19:06:05 | INFO | train | epoch 040 | loss 0.226 | ppl 1.17 | wps 17561.2 | ups 6.59 | wpb 2665.8 | bsz 178.5 | num_updates 6360 | lr 0.000793052 | gnorm 0.358 | train_wall 20 | gb_free 22.1 | wall 1016
2025-03-09 19:06:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:06:05 | INFO | fairseq.trainer | begin training epoch 41
2025-03-09 19:06:05 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:06:10 | INFO | train_inner | epoch 041:     40 / 159 loss=0.201, ppl=1.15, wps=16276, ups=6, wpb=2713.8, bsz=180.8, num_updates=6400, lr=0.000790569, gnorm=0.256, train_wall=12, gb_free=22.1, wall=1021
2025-03-09 19:06:23 | INFO | train_inner | epoch 041:    140 / 159 loss=0.225, ppl=1.17, wps=21180.2, ups=7.9, wpb=2680.2, bsz=180.3, num_updates=6500, lr=0.000784465, gnorm=0.406, train_wall=12, gb_free=22.1, wall=1034
2025-03-09 19:06:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:06:27 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 0.271 | ppl 1.21 | wps 47401.7 | wpb 945.6 | bsz 63.3 | num_updates 6519 | best_loss 0.234
2025-03-09 19:06:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 6519 updates
2025-03-09 19:06:27 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint41.pt
2025-03-09 19:06:28 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint41.pt
2025-03-09 19:06:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint41.pt (epoch 41 @ 6519 updates, score 0.271) (writing took 1.8419196549803019 seconds)
2025-03-09 19:06:29 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2025-03-09 19:06:29 | INFO | train | epoch 041 | loss 0.222 | ppl 1.17 | wps 17400.4 | ups 6.53 | wpb 2665.8 | bsz 178.5 | num_updates 6519 | lr 0.000783321 | gnorm 0.413 | train_wall 20 | gb_free 22.1 | wall 1041
2025-03-09 19:06:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:06:29 | INFO | fairseq.trainer | begin training epoch 42
2025-03-09 19:06:29 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:06:40 | INFO | train_inner | epoch 042:     81 / 159 loss=0.244, ppl=1.18, wps=15579.4, ups=5.95, wpb=2617.6, bsz=180, num_updates=6600, lr=0.000778499, gnorm=0.489, train_wall=12, gb_free=22.3, wall=1051
2025-03-09 19:06:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:06:52 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 0.246 | ppl 1.19 | wps 47520.3 | wpb 945.6 | bsz 63.3 | num_updates 6678 | best_loss 0.234
2025-03-09 19:06:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 6678 updates
2025-03-09 19:06:52 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint42.pt
2025-03-09 19:06:52 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint42.pt
2025-03-09 19:06:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint42.pt (epoch 42 @ 6678 updates, score 0.246) (writing took 1.71242298418656 seconds)
2025-03-09 19:06:54 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2025-03-09 19:06:54 | INFO | train | epoch 042 | loss 0.222 | ppl 1.17 | wps 17430.2 | ups 6.54 | wpb 2665.8 | bsz 178.5 | num_updates 6678 | lr 0.000773939 | gnorm 0.357 | train_wall 20 | gb_free 22.1 | wall 1065
2025-03-09 19:06:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:06:54 | INFO | fairseq.trainer | begin training epoch 43
2025-03-09 19:06:54 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:06:57 | INFO | train_inner | epoch 043:     22 / 159 loss=0.209, ppl=1.16, wps=15605.8, ups=5.9, wpb=2643.5, bsz=170.2, num_updates=6700, lr=0.000772667, gnorm=0.347, train_wall=13, gb_free=22.1, wall=1068
2025-03-09 19:07:09 | INFO | train_inner | epoch 043:    122 / 159 loss=0.2, ppl=1.15, wps=21405.4, ups=7.92, wpb=2702.6, bsz=185.3, num_updates=6800, lr=0.000766965, gnorm=0.274, train_wall=12, gb_free=22.1, wall=1080
2025-03-09 19:07:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:07:16 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 0.245 | ppl 1.19 | wps 47592.5 | wpb 945.6 | bsz 63.3 | num_updates 6837 | best_loss 0.234
2025-03-09 19:07:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 6837 updates
2025-03-09 19:07:16 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint43.pt
2025-03-09 19:07:17 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint43.pt
2025-03-09 19:07:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint43.pt (epoch 43 @ 6837 updates, score 0.245) (writing took 1.7206130749545991 seconds)
2025-03-09 19:07:18 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2025-03-09 19:07:18 | INFO | train | epoch 043 | loss 0.203 | ppl 1.15 | wps 17434.7 | ups 6.54 | wpb 2665.8 | bsz 178.5 | num_updates 6837 | lr 0.000764887 | gnorm 0.299 | train_wall 20 | gb_free 22.1 | wall 1089
2025-03-09 19:07:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:07:18 | INFO | fairseq.trainer | begin training epoch 44
2025-03-09 19:07:18 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:07:26 | INFO | train_inner | epoch 044:     63 / 159 loss=0.194, ppl=1.14, wps=16022.4, ups=5.96, wpb=2689.5, bsz=179.2, num_updates=6900, lr=0.000761387, gnorm=0.234, train_wall=12, gb_free=22.1, wall=1097
2025-03-09 19:07:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:07:41 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 0.246 | ppl 1.19 | wps 47650.4 | wpb 945.6 | bsz 63.3 | num_updates 6996 | best_loss 0.234
2025-03-09 19:07:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 6996 updates
2025-03-09 19:07:41 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint44.pt
2025-03-09 19:07:41 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint44.pt
2025-03-09 19:07:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint44.pt (epoch 44 @ 6996 updates, score 0.246) (writing took 1.660298879723996 seconds)
2025-03-09 19:07:42 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2025-03-09 19:07:42 | INFO | train | epoch 044 | loss 0.194 | ppl 1.14 | wps 17477.2 | ups 6.56 | wpb 2665.8 | bsz 178.5 | num_updates 6996 | lr 0.000756145 | gnorm 0.256 | train_wall 20 | gb_free 22.6 | wall 1113
2025-03-09 19:07:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:07:42 | INFO | fairseq.trainer | begin training epoch 45
2025-03-09 19:07:42 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:07:43 | INFO | train_inner | epoch 045:      4 / 159 loss=0.198, ppl=1.15, wps=15754.2, ups=5.95, wpb=2648.2, bsz=175.6, num_updates=7000, lr=0.000755929, gnorm=0.272, train_wall=12, gb_free=22.1, wall=1114
2025-03-09 19:07:55 | INFO | train_inner | epoch 045:    104 / 159 loss=0.192, ppl=1.14, wps=21178.7, ups=7.88, wpb=2688, bsz=181.6, num_updates=7100, lr=0.000750587, gnorm=0.299, train_wall=12, gb_free=22.1, wall=1127
2025-03-09 19:08:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:08:05 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 0.257 | ppl 1.19 | wps 47464.2 | wpb 945.6 | bsz 63.3 | num_updates 7155 | best_loss 0.234
2025-03-09 19:08:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 7155 updates
2025-03-09 19:08:05 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint45.pt
2025-03-09 19:08:05 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint45.pt
2025-03-09 19:08:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint45.pt (epoch 45 @ 7155 updates, score 0.257) (writing took 1.7090086471289396 seconds)
2025-03-09 19:08:07 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2025-03-09 19:08:07 | INFO | train | epoch 045 | loss 0.196 | ppl 1.15 | wps 17424.4 | ups 6.54 | wpb 2665.8 | bsz 178.5 | num_updates 7155 | lr 0.000747696 | gnorm 0.325 | train_wall 20 | gb_free 22.1 | wall 1138
2025-03-09 19:08:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:08:07 | INFO | fairseq.trainer | begin training epoch 46
2025-03-09 19:08:07 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:08:12 | INFO | train_inner | epoch 046:     45 / 159 loss=0.202, ppl=1.15, wps=15370.6, ups=5.95, wpb=2582.1, bsz=177.5, num_updates=7200, lr=0.000745356, gnorm=0.542, train_wall=12, gb_free=22.1, wall=1143
2025-03-09 19:08:25 | INFO | train_inner | epoch 046:    145 / 159 loss=0.188, ppl=1.14, wps=21212.7, ups=7.77, wpb=2730.6, bsz=176.2, num_updates=7300, lr=0.000740233, gnorm=0.254, train_wall=12, gb_free=22.1, wall=1156
2025-03-09 19:08:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:08:29 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 0.249 | ppl 1.19 | wps 47616.8 | wpb 945.6 | bsz 63.3 | num_updates 7314 | best_loss 0.234
2025-03-09 19:08:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 7314 updates
2025-03-09 19:08:29 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint46.pt
2025-03-09 19:08:30 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint46.pt
2025-03-09 19:08:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint46.pt (epoch 46 @ 7314 updates, score 0.249) (writing took 1.722085292916745 seconds)
2025-03-09 19:08:31 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2025-03-09 19:08:31 | INFO | train | epoch 046 | loss 0.193 | ppl 1.14 | wps 17405.8 | ups 6.53 | wpb 2665.8 | bsz 178.5 | num_updates 7314 | lr 0.000739524 | gnorm 0.4 | train_wall 20 | gb_free 22.1 | wall 1162
2025-03-09 19:08:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:08:31 | INFO | fairseq.trainer | begin training epoch 47
2025-03-09 19:08:31 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:08:42 | INFO | train_inner | epoch 047:     86 / 159 loss=0.191, ppl=1.14, wps=15458.9, ups=5.87, wpb=2633.9, bsz=166.9, num_updates=7400, lr=0.000735215, gnorm=0.342, train_wall=13, gb_free=22.1, wall=1173
2025-03-09 19:08:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:08:53 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 0.26 | ppl 1.2 | wps 47524.4 | wpb 945.6 | bsz 63.3 | num_updates 7473 | best_loss 0.234
2025-03-09 19:08:53 | INFO | fairseq_cli.train | early stop since valid performance hasn't improved for last 12 runs
2025-03-09 19:08:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 7473 updates
2025-03-09 19:08:53 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint47.pt
2025-03-09 19:08:54 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint47.pt
2025-03-09 19:08:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict3000_3l_embed384/checkpoint47.pt (epoch 47 @ 7473 updates, score 0.26) (writing took 1.7139466651715338 seconds)
2025-03-09 19:08:55 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2025-03-09 19:08:55 | INFO | train | epoch 047 | loss 0.184 | ppl 1.14 | wps 17403.4 | ups 6.53 | wpb 2665.8 | bsz 178.5 | num_updates 7473 | lr 0.000731615 | gnorm 0.297 | train_wall 20 | gb_free 22.2 | wall 1186
2025-03-09 19:08:55 | INFO | fairseq_cli.train | done training in 1179.7 seconds
2025-03-09 19:09:22 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 3000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 3000, 'batch_size_valid': 64, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.001], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 12, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=3000, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=3000, batch_size_valid='64', max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='lstm', max_epoch=0, max_update=0, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[0.001], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='/home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=12, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, share_decoder_input_output_embed=False, share_all_embeddings=True, data='/home/users/s/solfrini/git/normalisation_training/data/data_norm_bin_4000', source_lang=None, target_lang=None, load_alignments=False, left_pad_source=True, left_pad_target=False, max_source_positions=1024, max_target_positions=1024, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_print_samples=False, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, warmup_updates=4000, warmup_init_lr=-1, pad=1, eos=2, unk=3, encoder_layers=3, decoder_layers=3, encoder_embed_dim=384, decoder_embed_dim=384, decoder_out_embed_dim=384, encoder_hidden_size=768, encoder_bidirectional=True, decoder_hidden_size=768, dropout=0.3, no_seed_provided=False, encoder_embed_path=None, encoder_freeze_embed=False, encoder_dropout_in=0.3, encoder_dropout_out=0.3, decoder_embed_path=None, decoder_freeze_embed=False, decoder_attention='1', decoder_dropout_in=0.3, decoder_dropout_out=0.3, adaptive_softmax_cutoff='10000,50000,200000', _name='lstm'), 'task': {'_name': 'translation', 'data': '/home/users/s/solfrini/git/normalisation_training/data/data_norm_bin_4000', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.001]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.001]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2025-03-09 19:09:22 | INFO | fairseq.tasks.translation | [trg] dictionary: 1032 types
2025-03-09 19:09:22 | INFO | fairseq.tasks.translation | [src] dictionary: 1032 types
2025-03-09 19:09:23 | INFO | fairseq_cli.train | LSTMModel(
  (encoder): LSTMEncoder(
    (dropout_in_module): FairseqDropout()
    (dropout_out_module): FairseqDropout()
    (embed_tokens): Embedding(1032, 384, padding_idx=1)
    (lstm): LSTM(384, 768, num_layers=3, dropout=0.3, bidirectional=True)
  )
  (decoder): LSTMDecoder(
    (dropout_in_module): FairseqDropout()
    (dropout_out_module): FairseqDropout()
    (embed_tokens): Embedding(1032, 384, padding_idx=1)
    (encoder_hidden_proj): Linear(in_features=1536, out_features=768, bias=True)
    (encoder_cell_proj): Linear(in_features=1536, out_features=768, bias=True)
    (layers): ModuleList(
      (0): LSTMCell(1152, 768)
      (1-2): 2 x LSTMCell(768, 768)
    )
    (attention): AttentionLayer(
      (input_proj): Linear(in_features=768, out_features=1536, bias=False)
      (output_proj): Linear(in_features=2304, out_features=768, bias=False)
    )
    (additional_fc): Linear(in_features=768, out_features=384, bias=True)
  )
)
2025-03-09 19:09:23 | INFO | fairseq_cli.train | task: TranslationTask
2025-03-09 19:09:23 | INFO | fairseq_cli.train | model: LSTMModel
2025-03-09 19:09:23 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion
2025-03-09 19:09:23 | INFO | fairseq_cli.train | num. shared model params: 56,781,696 (num. trained: 56,781,696)
2025-03-09 19:09:23 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2025-03-09 19:09:23 | INFO | fairseq.data.data_utils | loaded 7,087 examples from: /home/users/s/solfrini/git/normalisation_training/data/data_norm_bin_4000/valid.trg-src.trg
2025-03-09 19:09:23 | INFO | fairseq.data.data_utils | loaded 7,087 examples from: /home/users/s/solfrini/git/normalisation_training/data/data_norm_bin_4000/valid.trg-src.src
2025-03-09 19:09:23 | INFO | fairseq.tasks.translation | /home/users/s/solfrini/git/normalisation_training/data/data_norm_bin_4000 valid trg-src 7087 examples
2025-03-09 19:09:24 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2025-03-09 19:09:24 | INFO | fairseq.trainer | detected shared parameter: decoder.attention.input_proj.bias <- decoder.attention.output_proj.bias
2025-03-09 19:09:24 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2025-03-09 19:09:24 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.638 GB ; name = NVIDIA TITAN RTX                        
2025-03-09 19:09:24 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2025-03-09 19:09:24 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2025-03-09 19:09:24 | INFO | fairseq_cli.train | max tokens per device = 3000 and max sentences per device = None
2025-03-09 19:09:24 | INFO | fairseq.trainer | Preparing to load checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint_last.pt
2025-03-09 19:09:24 | INFO | fairseq.trainer | No existing checkpoint found /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint_last.pt
2025-03-09 19:09:24 | INFO | fairseq.trainer | loading train data for epoch 1
2025-03-09 19:09:24 | INFO | fairseq.data.data_utils | loaded 28,377 examples from: /home/users/s/solfrini/git/normalisation_training/data/data_norm_bin_4000/train.trg-src.trg
2025-03-09 19:09:24 | INFO | fairseq.data.data_utils | loaded 28,377 examples from: /home/users/s/solfrini/git/normalisation_training/data/data_norm_bin_4000/train.trg-src.src
2025-03-09 19:09:24 | INFO | fairseq.tasks.translation | /home/users/s/solfrini/git/normalisation_training/data/data_norm_bin_4000 train trg-src 28377 examples
2025-03-09 19:09:24 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp
2025-03-09 19:09:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:09:31 | INFO | fairseq.trainer | begin training epoch 1
2025-03-09 19:09:31 | INFO | fairseq_cli.train | Start iterating over samples
/home/users/s/solfrini/.local/lib/python3.9/site-packages/fairseq/tasks/fairseq_task.py:514: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
/home/users/s/solfrini/.local/lib/python3.9/site-packages/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
2025-03-09 19:09:47 | INFO | train_inner | epoch 001:    100 / 159 loss=9.633, ppl=793.9, wps=20504.3, ups=7.69, wpb=2670.4, bsz=181.8, num_updates=100, lr=2.5e-05, gnorm=2.112, train_wall=15, gb_free=22.1, wall=23
2025-03-09 19:09:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:09:56 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 8.172 | ppl 288.38 | wps 48615 | wpb 945.6 | bsz 63.3 | num_updates 159
2025-03-09 19:09:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 159 updates
2025-03-09 19:09:56 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint1.pt
2025-03-09 19:09:57 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint1.pt
2025-03-09 19:09:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint1.pt (epoch 1 @ 159 updates, score 8.172) (writing took 2.98818227276206 seconds)
2025-03-09 19:09:59 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2025-03-09 19:09:59 | INFO | train | epoch 001 | loss 9.328 | ppl 642.81 | wps 16388.3 | ups 6.15 | wpb 2665.8 | bsz 178.5 | num_updates 159 | lr 3.975e-05 | gnorm 2.211 | train_wall 23 | gb_free 22.1 | wall 35
2025-03-09 19:09:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:09:59 | INFO | fairseq.trainer | begin training epoch 2
2025-03-09 19:09:59 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:10:04 | INFO | train_inner | epoch 002:     41 / 159 loss=8.633, ppl=397, wps=14933, ups=5.57, wpb=2680.5, bsz=180.7, num_updates=200, lr=5e-05, gnorm=2.164, train_wall=12, gb_free=22.1, wall=40
2025-03-09 19:10:17 | INFO | train_inner | epoch 002:    141 / 159 loss=8.002, ppl=256.35, wps=20695.4, ups=7.81, wpb=2651, bsz=173.8, num_updates=300, lr=7.5e-05, gnorm=2.188, train_wall=12, gb_free=22.1, wall=53
2025-03-09 19:10:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:10:22 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.157 | ppl 142.67 | wps 48040.8 | wpb 945.6 | bsz 63.3 | num_updates 318 | best_loss 7.157
2025-03-09 19:10:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 318 updates
2025-03-09 19:10:22 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint2.pt
2025-03-09 19:10:23 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint2.pt
2025-03-09 19:10:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint2.pt (epoch 2 @ 318 updates, score 7.157) (writing took 3.122077865060419 seconds)
2025-03-09 19:10:25 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2025-03-09 19:10:25 | INFO | train | epoch 002 | loss 8.062 | ppl 267.24 | wps 16524.3 | ups 6.2 | wpb 2665.8 | bsz 178.5 | num_updates 318 | lr 7.95e-05 | gnorm 2.134 | train_wall 19 | gb_free 22.1 | wall 61
2025-03-09 19:10:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:10:25 | INFO | fairseq.trainer | begin training epoch 3
2025-03-09 19:10:25 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:10:35 | INFO | train_inner | epoch 003:     82 / 159 loss=7.375, ppl=165.95, wps=14748, ups=5.54, wpb=2662.2, bsz=185.3, num_updates=400, lr=0.0001, gnorm=2.528, train_wall=12, gb_free=22.1, wall=71
2025-03-09 19:10:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:10:48 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 6.222 | ppl 74.65 | wps 47735.2 | wpb 945.6 | bsz 63.3 | num_updates 477 | best_loss 6.222
2025-03-09 19:10:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 477 updates
2025-03-09 19:10:48 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint3.pt
2025-03-09 19:10:48 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint3.pt
2025-03-09 19:10:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint3.pt (epoch 3 @ 477 updates, score 6.222) (writing took 3.031043705996126 seconds)
2025-03-09 19:10:51 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2025-03-09 19:10:51 | INFO | train | epoch 003 | loss 7.136 | ppl 140.69 | wps 16579.3 | ups 6.22 | wpb 2665.8 | bsz 178.5 | num_updates 477 | lr 0.00011925 | gnorm 2.559 | train_wall 20 | gb_free 22.1 | wall 87
2025-03-09 19:10:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:10:51 | INFO | fairseq.trainer | begin training epoch 4
2025-03-09 19:10:51 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:10:54 | INFO | train_inner | epoch 004:     23 / 159 loss=6.88, ppl=117.75, wps=14701.8, ups=5.5, wpb=2671.7, bsz=172.1, num_updates=500, lr=0.000125, gnorm=2.592, train_wall=12, gb_free=22.1, wall=90
2025-03-09 19:11:06 | INFO | train_inner | epoch 004:    123 / 159 loss=6.212, ppl=74.11, wps=20556.5, ups=7.74, wpb=2655.7, bsz=175.4, num_updates=600, lr=0.00015, gnorm=2.538, train_wall=12, gb_free=22.1, wall=102
2025-03-09 19:11:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:11:13 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 5.03 | ppl 32.67 | wps 47675 | wpb 945.6 | bsz 63.3 | num_updates 636 | best_loss 5.03
2025-03-09 19:11:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 636 updates
2025-03-09 19:11:13 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint4.pt
2025-03-09 19:11:14 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint4.pt
2025-03-09 19:11:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint4.pt (epoch 4 @ 636 updates, score 5.03) (writing took 2.9796256870031357 seconds)
2025-03-09 19:11:16 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2025-03-09 19:11:16 | INFO | train | epoch 004 | loss 6.187 | ppl 72.88 | wps 16564.5 | ups 6.21 | wpb 2665.8 | bsz 178.5 | num_updates 636 | lr 0.000159 | gnorm 2.707 | train_wall 20 | gb_free 22.1 | wall 112
2025-03-09 19:11:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:11:16 | INFO | fairseq.trainer | begin training epoch 5
2025-03-09 19:11:16 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:11:24 | INFO | train_inner | epoch 005:     64 / 159 loss=5.635, ppl=49.7, wps=14785.1, ups=5.58, wpb=2647.8, bsz=184.2, num_updates=700, lr=0.000175, gnorm=3.134, train_wall=12, gb_free=22.1, wall=120
2025-03-09 19:11:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:11:39 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 4.092 | ppl 17.05 | wps 47617.6 | wpb 945.6 | bsz 63.3 | num_updates 795 | best_loss 4.092
2025-03-09 19:11:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 795 updates
2025-03-09 19:11:39 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint5.pt
2025-03-09 19:11:40 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint5.pt
2025-03-09 19:11:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint5.pt (epoch 5 @ 795 updates, score 4.092) (writing took 3.0186082599684596 seconds)
2025-03-09 19:11:42 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2025-03-09 19:11:42 | INFO | train | epoch 005 | loss 5.234 | ppl 37.63 | wps 16507.7 | ups 6.19 | wpb 2665.8 | bsz 178.5 | num_updates 795 | lr 0.00019875 | gnorm 3.198 | train_wall 20 | gb_free 22.2 | wall 138
2025-03-09 19:11:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:11:42 | INFO | fairseq.trainer | begin training epoch 6
2025-03-09 19:11:42 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:11:43 | INFO | train_inner | epoch 006:      5 / 159 loss=5.013, ppl=32.28, wps=14792.1, ups=5.5, wpb=2689.2, bsz=174.2, num_updates=800, lr=0.0002, gnorm=3.282, train_wall=12, gb_free=22.1, wall=139
2025-03-09 19:11:55 | INFO | train_inner | epoch 006:    105 / 159 loss=4.48, ppl=22.31, wps=20607.5, ups=7.81, wpb=2638.6, bsz=173.4, num_updates=900, lr=0.000225, gnorm=3.18, train_wall=12, gb_free=22.1, wall=151
2025-03-09 19:12:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:12:05 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 2.852 | ppl 7.22 | wps 47634.5 | wpb 945.6 | bsz 63.3 | num_updates 954 | best_loss 2.852
2025-03-09 19:12:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 954 updates
2025-03-09 19:12:05 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint6.pt
2025-03-09 19:12:05 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint6.pt
2025-03-09 19:12:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint6.pt (epoch 6 @ 954 updates, score 2.852) (writing took 2.9699652451090515 seconds)
2025-03-09 19:12:07 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2025-03-09 19:12:07 | INFO | train | epoch 006 | loss 4.29 | ppl 19.57 | wps 16508.5 | ups 6.19 | wpb 2665.8 | bsz 178.5 | num_updates 954 | lr 0.0002385 | gnorm 3.188 | train_wall 20 | gb_free 22.1 | wall 163
2025-03-09 19:12:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:12:08 | INFO | fairseq.trainer | begin training epoch 7
2025-03-09 19:12:08 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:12:13 | INFO | train_inner | epoch 007:     46 / 159 loss=3.803, ppl=13.95, wps=14925.2, ups=5.56, wpb=2685.6, bsz=187.8, num_updates=1000, lr=0.00025, gnorm=3.254, train_wall=12, gb_free=22.1, wall=169
2025-03-09 19:12:26 | INFO | train_inner | epoch 007:    146 / 159 loss=3.551, ppl=11.72, wps=20600.6, ups=7.76, wpb=2653.3, bsz=175.3, num_updates=1100, lr=0.000275, gnorm=3.599, train_wall=12, gb_free=22.1, wall=182
2025-03-09 19:12:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:12:30 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 2.347 | ppl 5.09 | wps 47508.3 | wpb 945.6 | bsz 63.3 | num_updates 1113 | best_loss 2.347
2025-03-09 19:12:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 1113 updates
2025-03-09 19:12:30 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint7.pt
2025-03-09 19:12:31 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint7.pt
2025-03-09 19:12:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint7.pt (epoch 7 @ 1113 updates, score 2.347) (writing took 3.010998811107129 seconds)
2025-03-09 19:12:33 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2025-03-09 19:12:33 | INFO | train | epoch 007 | loss 3.528 | ppl 11.54 | wps 16525 | ups 6.2 | wpb 2665.8 | bsz 178.5 | num_updates 1113 | lr 0.00027825 | gnorm 3.472 | train_wall 20 | gb_free 22.1 | wall 189
2025-03-09 19:12:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:12:33 | INFO | fairseq.trainer | begin training epoch 8
2025-03-09 19:12:33 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:12:44 | INFO | train_inner | epoch 008:     87 / 159 loss=2.852, ppl=7.22, wps=14826.7, ups=5.52, wpb=2684, bsz=178.2, num_updates=1200, lr=0.0003, gnorm=3.007, train_wall=12, gb_free=22.2, wall=200
2025-03-09 19:12:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:12:56 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 1.83 | ppl 3.56 | wps 47643.1 | wpb 945.6 | bsz 63.3 | num_updates 1272 | best_loss 1.83
2025-03-09 19:12:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 1272 updates
2025-03-09 19:12:56 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint8.pt
2025-03-09 19:12:57 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint8.pt
2025-03-09 19:12:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint8.pt (epoch 8 @ 1272 updates, score 1.83) (writing took 3.0038226558826864 seconds)
2025-03-09 19:12:59 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2025-03-09 19:12:59 | INFO | train | epoch 008 | loss 2.748 | ppl 6.72 | wps 16494.5 | ups 6.19 | wpb 2665.8 | bsz 178.5 | num_updates 1272 | lr 0.000318 | gnorm 2.991 | train_wall 20 | gb_free 22.1 | wall 215
2025-03-09 19:12:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:12:59 | INFO | fairseq.trainer | begin training epoch 9
2025-03-09 19:12:59 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:13:02 | INFO | train_inner | epoch 009:     28 / 159 loss=2.562, ppl=5.91, wps=14685.2, ups=5.5, wpb=2669.2, bsz=181.4, num_updates=1300, lr=0.000325, gnorm=3, train_wall=12, gb_free=22.1, wall=218
2025-03-09 19:13:15 | INFO | train_inner | epoch 009:    128 / 159 loss=1.998, ppl=4, wps=20660, ups=7.74, wpb=2668.7, bsz=173.6, num_updates=1400, lr=0.00035, gnorm=2.251, train_wall=12, gb_free=22.1, wall=231
2025-03-09 19:13:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:13:22 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 1.206 | ppl 2.31 | wps 47591.6 | wpb 945.6 | bsz 63.3 | num_updates 1431 | best_loss 1.206
2025-03-09 19:13:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 1431 updates
2025-03-09 19:13:22 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint9.pt
2025-03-09 19:13:22 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint9.pt
2025-03-09 19:13:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint9.pt (epoch 9 @ 1431 updates, score 1.206) (writing took 3.015521834138781 seconds)
2025-03-09 19:13:25 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2025-03-09 19:13:25 | INFO | train | epoch 009 | loss 2.011 | ppl 4.03 | wps 16467 | ups 6.18 | wpb 2665.8 | bsz 178.5 | num_updates 1431 | lr 0.00035775 | gnorm 2.322 | train_wall 20 | gb_free 22.2 | wall 241
2025-03-09 19:13:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:13:25 | INFO | fairseq.trainer | begin training epoch 10
2025-03-09 19:13:25 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:13:33 | INFO | train_inner | epoch 010:     69 / 159 loss=1.592, ppl=3.01, wps=14594.3, ups=5.53, wpb=2637.4, bsz=179.8, num_updates=1500, lr=0.000375, gnorm=2.094, train_wall=12, gb_free=22.1, wall=249
2025-03-09 19:13:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:13:47 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 0.849 | ppl 1.8 | wps 47617.2 | wpb 945.6 | bsz 63.3 | num_updates 1590 | best_loss 0.849
2025-03-09 19:13:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 1590 updates
2025-03-09 19:13:47 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint10.pt
2025-03-09 19:13:48 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint10.pt
2025-03-09 19:13:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint10.pt (epoch 10 @ 1590 updates, score 0.849) (writing took 2.9369119480252266 seconds)
2025-03-09 19:13:50 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2025-03-09 19:13:50 | INFO | train | epoch 010 | loss 1.39 | ppl 2.62 | wps 16658.7 | ups 6.25 | wpb 2665.8 | bsz 178.5 | num_updates 1590 | lr 0.0003975 | gnorm 1.833 | train_wall 20 | gb_free 22.1 | wall 266
2025-03-09 19:13:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:13:50 | INFO | fairseq.trainer | begin training epoch 11
2025-03-09 19:13:50 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:13:51 | INFO | train_inner | epoch 011:     10 / 159 loss=1.314, ppl=2.49, wps=14942.6, ups=5.57, wpb=2683, bsz=175.5, num_updates=1600, lr=0.0004, gnorm=1.596, train_wall=12, gb_free=22.1, wall=267
2025-03-09 19:14:04 | INFO | train_inner | epoch 011:    110 / 159 loss=1.04, ppl=2.06, wps=21369.7, ups=7.96, wpb=2683.1, bsz=188.2, num_updates=1700, lr=0.000425, gnorm=1.328, train_wall=12, gb_free=22.1, wall=280
2025-03-09 19:14:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:14:13 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 0.625 | ppl 1.54 | wps 48260.3 | wpb 945.6 | bsz 63.3 | num_updates 1749 | best_loss 0.625
2025-03-09 19:14:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 1749 updates
2025-03-09 19:14:13 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint11.pt
2025-03-09 19:14:13 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint11.pt
2025-03-09 19:14:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint11.pt (epoch 11 @ 1749 updates, score 0.625) (writing took 2.9528483832255006 seconds)
2025-03-09 19:14:16 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2025-03-09 19:14:16 | INFO | train | epoch 011 | loss 1.06 | ppl 2.09 | wps 16603.3 | ups 6.23 | wpb 2665.8 | bsz 178.5 | num_updates 1749 | lr 0.00043725 | gnorm 1.383 | train_wall 20 | gb_free 22.1 | wall 292
2025-03-09 19:14:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:14:16 | INFO | fairseq.trainer | begin training epoch 12
2025-03-09 19:14:16 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:14:22 | INFO | train_inner | epoch 012:     51 / 159 loss=0.993, ppl=1.99, wps=14547.5, ups=5.49, wpb=2652.2, bsz=167.5, num_updates=1800, lr=0.00045, gnorm=1.742, train_wall=13, gb_free=22.1, wall=298
2025-03-09 19:14:35 | INFO | train_inner | epoch 012:    151 / 159 loss=0.772, ppl=1.71, wps=21178, ups=7.89, wpb=2683.7, bsz=185.6, num_updates=1900, lr=0.000475, gnorm=0.957, train_wall=12, gb_free=22.1, wall=311
2025-03-09 19:14:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:14:38 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 0.622 | ppl 1.54 | wps 47537.4 | wpb 945.6 | bsz 63.3 | num_updates 1908 | best_loss 0.622
2025-03-09 19:14:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 1908 updates
2025-03-09 19:14:38 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint12.pt
2025-03-09 19:14:39 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint12.pt
2025-03-09 19:14:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint12.pt (epoch 12 @ 1908 updates, score 0.622) (writing took 2.8154071550816298 seconds)
2025-03-09 19:14:41 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2025-03-09 19:14:41 | INFO | train | epoch 012 | loss 0.85 | ppl 1.8 | wps 16620.8 | ups 6.23 | wpb 2665.8 | bsz 178.5 | num_updates 1908 | lr 0.000477 | gnorm 1.401 | train_wall 20 | gb_free 22.1 | wall 317
2025-03-09 19:14:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:14:41 | INFO | fairseq.trainer | begin training epoch 13
2025-03-09 19:14:41 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:14:53 | INFO | train_inner | epoch 013:     92 / 159 loss=1.005, ppl=2.01, wps=14579.7, ups=5.52, wpb=2640.8, bsz=170.8, num_updates=2000, lr=0.0005, gnorm=2.064, train_wall=13, gb_free=22.3, wall=329
2025-03-09 19:15:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:15:04 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 0.503 | ppl 1.42 | wps 47550 | wpb 945.6 | bsz 63.3 | num_updates 2067 | best_loss 0.503
2025-03-09 19:15:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 2067 updates
2025-03-09 19:15:04 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint13.pt
2025-03-09 19:15:04 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint13.pt
2025-03-09 19:15:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint13.pt (epoch 13 @ 2067 updates, score 0.503) (writing took 2.877682242076844 seconds)
2025-03-09 19:15:07 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2025-03-09 19:15:07 | INFO | train | epoch 013 | loss 0.855 | ppl 1.81 | wps 16580.3 | ups 6.22 | wpb 2665.8 | bsz 178.5 | num_updates 2067 | lr 0.00051675 | gnorm 1.519 | train_wall 20 | gb_free 22.1 | wall 343
2025-03-09 19:15:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:15:07 | INFO | fairseq.trainer | begin training epoch 14
2025-03-09 19:15:07 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:15:11 | INFO | train_inner | epoch 014:     33 / 159 loss=0.681, ppl=1.6, wps=14823.3, ups=5.57, wpb=2660.9, bsz=179.8, num_updates=2100, lr=0.000525, gnorm=0.983, train_wall=12, gb_free=22.1, wall=347
2025-03-09 19:15:24 | INFO | train_inner | epoch 014:    133 / 159 loss=0.749, ppl=1.68, wps=20796.5, ups=7.82, wpb=2657.8, bsz=179.1, num_updates=2200, lr=0.00055, gnorm=1.199, train_wall=12, gb_free=22.1, wall=360
2025-03-09 19:15:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:15:29 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 0.409 | ppl 1.33 | wps 47575 | wpb 945.6 | bsz 63.3 | num_updates 2226 | best_loss 0.409
2025-03-09 19:15:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 2226 updates
2025-03-09 19:15:29 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint14.pt
2025-03-09 19:15:30 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint14.pt
2025-03-09 19:15:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint14.pt (epoch 14 @ 2226 updates, score 0.409) (writing took 2.801570480223745 seconds)
2025-03-09 19:15:32 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2025-03-09 19:15:32 | INFO | train | epoch 014 | loss 0.713 | ppl 1.64 | wps 16630 | ups 6.24 | wpb 2665.8 | bsz 178.5 | num_updates 2226 | lr 0.0005565 | gnorm 1.127 | train_wall 20 | gb_free 22.1 | wall 368
2025-03-09 19:15:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:15:32 | INFO | fairseq.trainer | begin training epoch 15
2025-03-09 19:15:32 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:15:42 | INFO | train_inner | epoch 015:     74 / 159 loss=0.562, ppl=1.48, wps=14671.1, ups=5.55, wpb=2642, bsz=177.2, num_updates=2300, lr=0.000575, gnorm=0.866, train_wall=12, gb_free=22.1, wall=378
2025-03-09 19:15:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:15:55 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 0.381 | ppl 1.3 | wps 47667.9 | wpb 945.6 | bsz 63.3 | num_updates 2385 | best_loss 0.381
2025-03-09 19:15:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 2385 updates
2025-03-09 19:15:55 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint15.pt
2025-03-09 19:15:55 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint15.pt
2025-03-09 19:15:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint15.pt (epoch 15 @ 2385 updates, score 0.381) (writing took 2.867448105942458 seconds)
2025-03-09 19:15:58 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2025-03-09 19:15:58 | INFO | train | epoch 015 | loss 0.584 | ppl 1.5 | wps 16583.8 | ups 6.22 | wpb 2665.8 | bsz 178.5 | num_updates 2385 | lr 0.00059625 | gnorm 0.873 | train_wall 20 | gb_free 22.1 | wall 394
2025-03-09 19:15:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:15:58 | INFO | fairseq.trainer | begin training epoch 16
2025-03-09 19:15:58 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:16:00 | INFO | train_inner | epoch 016:     15 / 159 loss=0.564, ppl=1.48, wps=15207.8, ups=5.6, wpb=2716.4, bsz=183.6, num_updates=2400, lr=0.0006, gnorm=0.752, train_wall=12, gb_free=22.1, wall=396
2025-03-09 19:16:12 | INFO | train_inner | epoch 016:    115 / 159 loss=0.591, ppl=1.51, wps=20762.6, ups=7.79, wpb=2665.2, bsz=178.2, num_updates=2500, lr=0.000625, gnorm=1.158, train_wall=12, gb_free=22.1, wall=408
2025-03-09 19:16:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:16:20 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 0.373 | ppl 1.3 | wps 47547.5 | wpb 945.6 | bsz 63.3 | num_updates 2544 | best_loss 0.373
2025-03-09 19:16:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 2544 updates
2025-03-09 19:16:20 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint16.pt
2025-03-09 19:16:21 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint16.pt
2025-03-09 19:16:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint16.pt (epoch 16 @ 2544 updates, score 0.373) (writing took 2.813642526976764 seconds)
2025-03-09 19:16:23 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2025-03-09 19:16:23 | INFO | train | epoch 016 | loss 0.581 | ppl 1.5 | wps 16611.8 | ups 6.23 | wpb 2665.8 | bsz 178.5 | num_updates 2544 | lr 0.000636 | gnorm 1.034 | train_wall 20 | gb_free 22.2 | wall 419
2025-03-09 19:16:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:16:23 | INFO | fairseq.trainer | begin training epoch 17
2025-03-09 19:16:23 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:16:30 | INFO | train_inner | epoch 017:     56 / 159 loss=0.545, ppl=1.46, wps=14887.2, ups=5.58, wpb=2665.7, bsz=181.1, num_updates=2600, lr=0.00065, gnorm=0.803, train_wall=12, gb_free=22.1, wall=426
2025-03-09 19:16:43 | INFO | train_inner | epoch 017:    156 / 159 loss=0.543, ppl=1.46, wps=20780.3, ups=7.83, wpb=2654.1, bsz=174.3, num_updates=2700, lr=0.000675, gnorm=0.959, train_wall=12, gb_free=22.1, wall=439
2025-03-09 19:16:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:16:46 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 0.418 | ppl 1.34 | wps 47610.9 | wpb 945.6 | bsz 63.3 | num_updates 2703 | best_loss 0.373
2025-03-09 19:16:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 2703 updates
2025-03-09 19:16:46 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint17.pt
2025-03-09 19:16:46 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint17.pt
2025-03-09 19:16:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint17.pt (epoch 17 @ 2703 updates, score 0.418) (writing took 1.6734307971782982 seconds)
2025-03-09 19:16:47 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2025-03-09 19:16:47 | INFO | train | epoch 017 | loss 0.526 | ppl 1.44 | wps 17486 | ups 6.56 | wpb 2665.8 | bsz 178.5 | num_updates 2703 | lr 0.00067575 | gnorm 0.861 | train_wall 20 | gb_free 22.1 | wall 443
2025-03-09 19:16:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:16:47 | INFO | fairseq.trainer | begin training epoch 18
2025-03-09 19:16:47 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:17:00 | INFO | train_inner | epoch 018:     97 / 159 loss=0.502, ppl=1.42, wps=15803.4, ups=5.91, wpb=2672.9, bsz=174.6, num_updates=2800, lr=0.0007, gnorm=0.94, train_wall=12, gb_free=22.1, wall=456
2025-03-09 19:17:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:17:10 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 0.342 | ppl 1.27 | wps 47483.9 | wpb 945.6 | bsz 63.3 | num_updates 2862 | best_loss 0.342
2025-03-09 19:17:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 2862 updates
2025-03-09 19:17:10 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint18.pt
2025-03-09 19:17:11 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint18.pt
2025-03-09 19:17:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint18.pt (epoch 18 @ 2862 updates, score 0.342) (writing took 2.772340815048665 seconds)
2025-03-09 19:17:13 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2025-03-09 19:17:13 | INFO | train | epoch 018 | loss 0.533 | ppl 1.45 | wps 16694.7 | ups 6.26 | wpb 2665.8 | bsz 178.5 | num_updates 2862 | lr 0.0007155 | gnorm 1.012 | train_wall 20 | gb_free 22.1 | wall 469
2025-03-09 19:17:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:17:13 | INFO | fairseq.trainer | begin training epoch 19
2025-03-09 19:17:13 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:17:18 | INFO | train_inner | epoch 019:     38 / 159 loss=0.529, ppl=1.44, wps=15216.1, ups=5.67, wpb=2682.3, bsz=184.6, num_updates=2900, lr=0.000725, gnorm=0.903, train_wall=12, gb_free=22.2, wall=474
2025-03-09 19:17:30 | INFO | train_inner | epoch 019:    138 / 159 loss=0.462, ppl=1.38, wps=20735.9, ups=7.84, wpb=2645.1, bsz=176.5, num_updates=3000, lr=0.00075, gnorm=0.729, train_wall=12, gb_free=22.1, wall=486
2025-03-09 19:17:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:17:35 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 0.31 | ppl 1.24 | wps 47632.2 | wpb 945.6 | bsz 63.3 | num_updates 3021 | best_loss 0.31
2025-03-09 19:17:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 3021 updates
2025-03-09 19:17:35 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint19.pt
2025-03-09 19:17:36 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint19.pt
2025-03-09 19:17:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint19.pt (epoch 19 @ 3021 updates, score 0.31) (writing took 2.9150726101361215 seconds)
2025-03-09 19:17:38 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2025-03-09 19:17:38 | INFO | train | epoch 019 | loss 0.448 | ppl 1.36 | wps 16599 | ups 6.23 | wpb 2665.8 | bsz 178.5 | num_updates 3021 | lr 0.00075525 | gnorm 0.674 | train_wall 20 | gb_free 22.1 | wall 494
2025-03-09 19:17:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:17:38 | INFO | fairseq.trainer | begin training epoch 20
2025-03-09 19:17:38 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:17:48 | INFO | train_inner | epoch 020:     79 / 159 loss=0.49, ppl=1.4, wps=14849.9, ups=5.58, wpb=2661.8, bsz=183.2, num_updates=3100, lr=0.000775, gnorm=0.874, train_wall=12, gb_free=22.1, wall=504
2025-03-09 19:17:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:18:01 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 0.348 | ppl 1.27 | wps 47623.2 | wpb 945.6 | bsz 63.3 | num_updates 3180 | best_loss 0.31
2025-03-09 19:18:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 3180 updates
2025-03-09 19:18:01 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint20.pt
2025-03-09 19:18:02 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint20.pt
2025-03-09 19:18:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint20.pt (epoch 20 @ 3180 updates, score 0.348) (writing took 1.684471283107996 seconds)
2025-03-09 19:18:03 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2025-03-09 19:18:03 | INFO | train | epoch 020 | loss 0.487 | ppl 1.4 | wps 17410.9 | ups 6.53 | wpb 2665.8 | bsz 178.5 | num_updates 3180 | lr 0.000795 | gnorm 0.814 | train_wall 20 | gb_free 22.1 | wall 519
2025-03-09 19:18:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:18:03 | INFO | fairseq.trainer | begin training epoch 21
2025-03-09 19:18:03 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:18:05 | INFO | train_inner | epoch 021:     20 / 159 loss=0.456, ppl=1.37, wps=15888.1, ups=5.92, wpb=2683.9, bsz=177.3, num_updates=3200, lr=0.0008, gnorm=0.649, train_wall=12, gb_free=22.1, wall=521
2025-03-09 19:18:18 | INFO | train_inner | epoch 021:    120 / 159 loss=0.377, ppl=1.3, wps=20485, ups=7.72, wpb=2654.2, bsz=177.8, num_updates=3300, lr=0.000825, gnorm=0.616, train_wall=12, gb_free=22.1, wall=534
2025-03-09 19:18:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:18:25 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 0.358 | ppl 1.28 | wps 47558.9 | wpb 945.6 | bsz 63.3 | num_updates 3339 | best_loss 0.31
2025-03-09 19:18:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 3339 updates
2025-03-09 19:18:25 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint21.pt
2025-03-09 19:18:26 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint21.pt
2025-03-09 19:18:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint21.pt (epoch 21 @ 3339 updates, score 0.358) (writing took 1.7040512450039387 seconds)
2025-03-09 19:18:27 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2025-03-09 19:18:27 | INFO | train | epoch 021 | loss 0.458 | ppl 1.37 | wps 17354.6 | ups 6.51 | wpb 2665.8 | bsz 178.5 | num_updates 3339 | lr 0.00083475 | gnorm 0.722 | train_wall 20 | gb_free 22.1 | wall 543
2025-03-09 19:18:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:18:27 | INFO | fairseq.trainer | begin training epoch 22
2025-03-09 19:18:27 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:18:35 | INFO | train_inner | epoch 022:     61 / 159 loss=0.539, ppl=1.45, wps=15901.3, ups=6, wpb=2652.4, bsz=175.6, num_updates=3400, lr=0.00085, gnorm=0.834, train_wall=12, gb_free=22.1, wall=551
2025-03-09 19:18:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:18:50 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 0.31 | ppl 1.24 | wps 47607.8 | wpb 945.6 | bsz 63.3 | num_updates 3498 | best_loss 0.31
2025-03-09 19:18:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 3498 updates
2025-03-09 19:18:50 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint22.pt
2025-03-09 19:18:50 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint22.pt
2025-03-09 19:18:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint22.pt (epoch 22 @ 3498 updates, score 0.31) (writing took 2.877468935213983 seconds)
2025-03-09 19:18:53 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2025-03-09 19:18:53 | INFO | train | epoch 022 | loss 0.43 | ppl 1.35 | wps 16592.7 | ups 6.22 | wpb 2665.8 | bsz 178.5 | num_updates 3498 | lr 0.0008745 | gnorm 0.698 | train_wall 20 | gb_free 22.1 | wall 569
2025-03-09 19:18:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:18:53 | INFO | fairseq.trainer | begin training epoch 23
2025-03-09 19:18:53 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:18:53 | INFO | train_inner | epoch 023:      2 / 159 loss=0.414, ppl=1.33, wps=14836.6, ups=5.53, wpb=2685.3, bsz=180.3, num_updates=3500, lr=0.000875, gnorm=0.714, train_wall=12, gb_free=22.1, wall=569
2025-03-09 19:19:06 | INFO | train_inner | epoch 023:    102 / 159 loss=0.36, ppl=1.28, wps=21120.3, ups=7.86, wpb=2687.6, bsz=179.2, num_updates=3600, lr=0.0009, gnorm=0.433, train_wall=12, gb_free=22.1, wall=582
2025-03-09 19:19:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:19:15 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 0.304 | ppl 1.23 | wps 47602.3 | wpb 945.6 | bsz 63.3 | num_updates 3657 | best_loss 0.304
2025-03-09 19:19:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 3657 updates
2025-03-09 19:19:15 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint23.pt
2025-03-09 19:19:16 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint23.pt
2025-03-09 19:19:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint23.pt (epoch 23 @ 3657 updates, score 0.304) (writing took 2.9823934170417488 seconds)
2025-03-09 19:19:18 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2025-03-09 19:19:18 | INFO | train | epoch 023 | loss 0.372 | ppl 1.29 | wps 16513 | ups 6.19 | wpb 2665.8 | bsz 178.5 | num_updates 3657 | lr 0.00091425 | gnorm 0.537 | train_wall 20 | gb_free 22.1 | wall 594
2025-03-09 19:19:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:19:18 | INFO | fairseq.trainer | begin training epoch 24
2025-03-09 19:19:18 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:19:24 | INFO | train_inner | epoch 024:     43 / 159 loss=0.393, ppl=1.31, wps=14912, ups=5.52, wpb=2701.8, bsz=180.3, num_updates=3700, lr=0.000925, gnorm=0.632, train_wall=12, gb_free=22.1, wall=600
2025-03-09 19:19:37 | INFO | train_inner | epoch 024:    143 / 159 loss=0.346, ppl=1.27, wps=20650.4, ups=7.83, wpb=2637.2, bsz=180.2, num_updates=3800, lr=0.00095, gnorm=0.418, train_wall=12, gb_free=22.2, wall=613
2025-03-09 19:19:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:19:41 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 0.326 | ppl 1.25 | wps 47638.5 | wpb 945.6 | bsz 63.3 | num_updates 3816 | best_loss 0.304
2025-03-09 19:19:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 3816 updates
2025-03-09 19:19:41 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint24.pt
2025-03-09 19:19:42 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint24.pt
2025-03-09 19:19:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint24.pt (epoch 24 @ 3816 updates, score 0.326) (writing took 1.7254012217745185 seconds)
2025-03-09 19:19:43 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2025-03-09 19:19:43 | INFO | train | epoch 024 | loss 0.358 | ppl 1.28 | wps 17366.1 | ups 6.51 | wpb 2665.8 | bsz 178.5 | num_updates 3816 | lr 0.000954 | gnorm 0.481 | train_wall 20 | gb_free 22.1 | wall 619
2025-03-09 19:19:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:19:43 | INFO | fairseq.trainer | begin training epoch 25
2025-03-09 19:19:43 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:19:53 | INFO | train_inner | epoch 025:     84 / 159 loss=0.356, ppl=1.28, wps=15504.6, ups=5.92, wpb=2619.5, bsz=173.9, num_updates=3900, lr=0.000975, gnorm=0.545, train_wall=12, gb_free=22.1, wall=629
2025-03-09 19:20:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:20:05 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 0.31 | ppl 1.24 | wps 47668.3 | wpb 945.6 | bsz 63.3 | num_updates 3975 | best_loss 0.304
2025-03-09 19:20:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 3975 updates
2025-03-09 19:20:05 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint25.pt
2025-03-09 19:20:06 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint25.pt
2025-03-09 19:20:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint25.pt (epoch 25 @ 3975 updates, score 0.31) (writing took 1.7028392762877047 seconds)
2025-03-09 19:20:07 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2025-03-09 19:20:07 | INFO | train | epoch 025 | loss 0.364 | ppl 1.29 | wps 17445.6 | ups 6.54 | wpb 2665.8 | bsz 178.5 | num_updates 3975 | lr 0.00099375 | gnorm 0.491 | train_wall 20 | gb_free 22.1 | wall 643
2025-03-09 19:20:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:20:07 | INFO | fairseq.trainer | begin training epoch 26
2025-03-09 19:20:07 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:20:10 | INFO | train_inner | epoch 026:     25 / 159 loss=0.355, ppl=1.28, wps=15930.9, ups=5.92, wpb=2690.2, bsz=175.2, num_updates=4000, lr=0.001, gnorm=0.477, train_wall=13, gb_free=22.1, wall=646
2025-03-09 19:20:23 | INFO | train_inner | epoch 026:    125 / 159 loss=0.417, ppl=1.33, wps=20535.4, ups=7.81, wpb=2630, bsz=175.2, num_updates=4100, lr=0.00098773, gnorm=0.795, train_wall=12, gb_free=22.2, wall=659
2025-03-09 19:20:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:20:30 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 0.275 | ppl 1.21 | wps 47598.5 | wpb 945.6 | bsz 63.3 | num_updates 4134 | best_loss 0.275
2025-03-09 19:20:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 4134 updates
2025-03-09 19:20:30 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint26.pt
2025-03-09 19:20:30 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint26.pt
2025-03-09 19:20:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint26.pt (epoch 26 @ 4134 updates, score 0.275) (writing took 2.9056806629523635 seconds)
2025-03-09 19:20:33 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2025-03-09 19:20:33 | INFO | train | epoch 026 | loss 0.392 | ppl 1.31 | wps 16587.1 | ups 6.22 | wpb 2665.8 | bsz 178.5 | num_updates 4134 | lr 0.000983659 | gnorm 0.668 | train_wall 20 | gb_free 22.1 | wall 669
2025-03-09 19:20:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:20:33 | INFO | fairseq.trainer | begin training epoch 27
2025-03-09 19:20:33 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:20:41 | INFO | train_inner | epoch 027:     66 / 159 loss=0.338, ppl=1.26, wps=15113.4, ups=5.55, wpb=2723.2, bsz=184.5, num_updates=4200, lr=0.0009759, gnorm=0.437, train_wall=12, gb_free=22.1, wall=677
2025-03-09 19:20:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:20:55 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 0.3 | ppl 1.23 | wps 47654.9 | wpb 945.6 | bsz 63.3 | num_updates 4293 | best_loss 0.275
2025-03-09 19:20:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 4293 updates
2025-03-09 19:20:55 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint27.pt
2025-03-09 19:20:56 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint27.pt
2025-03-09 19:20:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint27.pt (epoch 27 @ 4293 updates, score 0.3) (writing took 1.763276670128107 seconds)
2025-03-09 19:20:57 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2025-03-09 19:20:57 | INFO | train | epoch 027 | loss 0.326 | ppl 1.25 | wps 17333.2 | ups 6.5 | wpb 2665.8 | bsz 178.5 | num_updates 4293 | lr 0.000965272 | gnorm 0.454 | train_wall 20 | gb_free 22.2 | wall 693
2025-03-09 19:20:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:20:57 | INFO | fairseq.trainer | begin training epoch 28
2025-03-09 19:20:57 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:20:58 | INFO | train_inner | epoch 028:      7 / 159 loss=0.336, ppl=1.26, wps=15468.7, ups=5.94, wpb=2603.6, bsz=176.7, num_updates=4300, lr=0.000964486, gnorm=0.547, train_wall=12, gb_free=22.2, wall=694
2025-03-09 19:21:11 | INFO | train_inner | epoch 028:    107 / 159 loss=0.348, ppl=1.27, wps=20659, ups=7.78, wpb=2656.6, bsz=173.3, num_updates=4400, lr=0.000953463, gnorm=0.464, train_wall=12, gb_free=22.1, wall=707
2025-03-09 19:21:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:21:20 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 0.27 | ppl 1.21 | wps 47658.9 | wpb 945.6 | bsz 63.3 | num_updates 4452 | best_loss 0.27
2025-03-09 19:21:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 4452 updates
2025-03-09 19:21:20 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint28.pt
2025-03-09 19:21:20 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint28.pt
2025-03-09 19:21:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint28.pt (epoch 28 @ 4452 updates, score 0.27) (writing took 2.814325566869229 seconds)
2025-03-09 19:21:23 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2025-03-09 19:21:23 | INFO | train | epoch 028 | loss 0.324 | ppl 1.25 | wps 16627 | ups 6.24 | wpb 2665.8 | bsz 178.5 | num_updates 4452 | lr 0.000947878 | gnorm 0.464 | train_wall 20 | gb_free 22.1 | wall 719
2025-03-09 19:21:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:21:23 | INFO | fairseq.trainer | begin training epoch 29
2025-03-09 19:21:23 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:21:29 | INFO | train_inner | epoch 029:     48 / 159 loss=0.278, ppl=1.21, wps=15113.2, ups=5.63, wpb=2686.6, bsz=182.8, num_updates=4500, lr=0.000942809, gnorm=0.312, train_wall=12, gb_free=22.1, wall=725
2025-03-09 19:21:41 | INFO | train_inner | epoch 029:    148 / 159 loss=0.306, ppl=1.24, wps=20957.9, ups=7.81, wpb=2682.1, bsz=182.2, num_updates=4600, lr=0.000932505, gnorm=0.488, train_wall=12, gb_free=22.1, wall=737
2025-03-09 19:21:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:21:45 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 0.265 | ppl 1.2 | wps 47515.9 | wpb 945.6 | bsz 63.3 | num_updates 4611 | best_loss 0.265
2025-03-09 19:21:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 4611 updates
2025-03-09 19:21:45 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint29.pt
2025-03-09 19:21:46 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint29.pt
2025-03-09 19:21:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint29.pt (epoch 29 @ 4611 updates, score 0.265) (writing took 2.893662012182176 seconds)
2025-03-09 19:21:48 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2025-03-09 19:21:48 | INFO | train | epoch 029 | loss 0.297 | ppl 1.23 | wps 16595.6 | ups 6.23 | wpb 2665.8 | bsz 178.5 | num_updates 4611 | lr 0.000931392 | gnorm 0.426 | train_wall 20 | gb_free 22.1 | wall 744
2025-03-09 19:21:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:21:48 | INFO | fairseq.trainer | begin training epoch 30
2025-03-09 19:21:48 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:21:59 | INFO | train_inner | epoch 030:     89 / 159 loss=0.307, ppl=1.24, wps=14655.6, ups=5.54, wpb=2643.1, bsz=176.8, num_updates=4700, lr=0.000922531, gnorm=0.427, train_wall=12, gb_free=22.1, wall=755
2025-03-09 19:22:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:22:11 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 0.259 | ppl 1.2 | wps 47627.3 | wpb 945.6 | bsz 63.3 | num_updates 4770 | best_loss 0.259
2025-03-09 19:22:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 4770 updates
2025-03-09 19:22:11 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint30.pt
2025-03-09 19:22:11 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint30.pt
2025-03-09 19:22:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint30.pt (epoch 30 @ 4770 updates, score 0.259) (writing took 2.7860157676041126 seconds)
2025-03-09 19:22:13 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2025-03-09 19:22:13 | INFO | train | epoch 030 | loss 0.292 | ppl 1.22 | wps 16683.8 | ups 6.26 | wpb 2665.8 | bsz 178.5 | num_updates 4770 | lr 0.000915737 | gnorm 0.424 | train_wall 20 | gb_free 22.2 | wall 769
2025-03-09 19:22:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:22:13 | INFO | fairseq.trainer | begin training epoch 31
2025-03-09 19:22:13 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:22:17 | INFO | train_inner | epoch 031:     30 / 159 loss=0.276, ppl=1.21, wps=14954.8, ups=5.61, wpb=2664.2, bsz=175.8, num_updates=4800, lr=0.000912871, gnorm=0.395, train_wall=12, gb_free=22.1, wall=773
2025-03-09 19:22:30 | INFO | train_inner | epoch 031:    130 / 159 loss=0.296, ppl=1.23, wps=20991.8, ups=7.83, wpb=2680.5, bsz=180.5, num_updates=4900, lr=0.000903508, gnorm=0.561, train_wall=12, gb_free=22.1, wall=786
2025-03-09 19:22:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:22:36 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 0.272 | ppl 1.21 | wps 47526.4 | wpb 945.6 | bsz 63.3 | num_updates 4929 | best_loss 0.259
2025-03-09 19:22:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 4929 updates
2025-03-09 19:22:36 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint31.pt
2025-03-09 19:22:37 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint31.pt
2025-03-09 19:22:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint31.pt (epoch 31 @ 4929 updates, score 0.272) (writing took 1.7949286880902946 seconds)
2025-03-09 19:22:38 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2025-03-09 19:22:38 | INFO | train | epoch 031 | loss 0.298 | ppl 1.23 | wps 17399 | ups 6.53 | wpb 2665.8 | bsz 178.5 | num_updates 4929 | lr 0.000900846 | gnorm 0.528 | train_wall 20 | gb_free 22.1 | wall 794
2025-03-09 19:22:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:22:38 | INFO | fairseq.trainer | begin training epoch 32
2025-03-09 19:22:38 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:22:47 | INFO | train_inner | epoch 032:     71 / 159 loss=0.317, ppl=1.25, wps=15528.8, ups=5.86, wpb=2648.4, bsz=172.7, num_updates=5000, lr=0.000894427, gnorm=0.609, train_wall=12, gb_free=22.1, wall=803
2025-03-09 19:22:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:23:01 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 0.236 | ppl 1.18 | wps 47589.5 | wpb 945.6 | bsz 63.3 | num_updates 5088 | best_loss 0.236
2025-03-09 19:23:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 5088 updates
2025-03-09 19:23:01 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint32.pt
2025-03-09 19:23:01 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint32.pt
2025-03-09 19:23:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint32.pt (epoch 32 @ 5088 updates, score 0.236) (writing took 2.905859800055623 seconds)
2025-03-09 19:23:03 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2025-03-09 19:23:03 | INFO | train | epoch 032 | loss 0.284 | ppl 1.22 | wps 16549.2 | ups 6.21 | wpb 2665.8 | bsz 178.5 | num_updates 5088 | lr 0.000886659 | gnorm 0.45 | train_wall 20 | gb_free 22.1 | wall 819
2025-03-09 19:23:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:23:03 | INFO | fairseq.trainer | begin training epoch 33
2025-03-09 19:23:03 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:23:05 | INFO | train_inner | epoch 033:     12 / 159 loss=0.258, ppl=1.2, wps=15079.1, ups=5.6, wpb=2692, bsz=185.8, num_updates=5100, lr=0.000885615, gnorm=0.312, train_wall=12, gb_free=22.1, wall=821
2025-03-09 19:23:18 | INFO | train_inner | epoch 033:    112 / 159 loss=0.251, ppl=1.19, wps=20931.1, ups=7.84, wpb=2670.1, bsz=183.8, num_updates=5200, lr=0.000877058, gnorm=0.378, train_wall=12, gb_free=22.1, wall=834
2025-03-09 19:23:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:23:26 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 0.269 | ppl 1.21 | wps 47662.6 | wpb 945.6 | bsz 63.3 | num_updates 5247 | best_loss 0.236
2025-03-09 19:23:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 5247 updates
2025-03-09 19:23:26 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint33.pt
2025-03-09 19:23:27 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint33.pt
2025-03-09 19:23:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint33.pt (epoch 33 @ 5247 updates, score 0.269) (writing took 1.8765090880915523 seconds)
2025-03-09 19:23:28 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2025-03-09 19:23:28 | INFO | train | epoch 033 | loss 0.251 | ppl 1.19 | wps 17240.4 | ups 6.47 | wpb 2665.8 | bsz 178.5 | num_updates 5247 | lr 0.000873121 | gnorm 0.349 | train_wall 20 | gb_free 22.1 | wall 844
2025-03-09 19:23:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:23:28 | INFO | fairseq.trainer | begin training epoch 34
2025-03-09 19:23:28 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:23:35 | INFO | train_inner | epoch 034:     53 / 159 loss=0.246, ppl=1.19, wps=15793, ups=5.83, wpb=2707.5, bsz=174.3, num_updates=5300, lr=0.000868744, gnorm=0.294, train_wall=12, gb_free=22.1, wall=851
2025-03-09 19:23:48 | INFO | train_inner | epoch 034:    153 / 159 loss=0.28, ppl=1.21, wps=20586, ups=7.85, wpb=2623.9, bsz=177.1, num_updates=5400, lr=0.000860663, gnorm=0.546, train_wall=12, gb_free=22.1, wall=864
2025-03-09 19:23:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:23:51 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 0.257 | ppl 1.2 | wps 47618.4 | wpb 945.6 | bsz 63.3 | num_updates 5406 | best_loss 0.236
2025-03-09 19:23:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 5406 updates
2025-03-09 19:23:51 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint34.pt
2025-03-09 19:23:51 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint34.pt
2025-03-09 19:23:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint34.pt (epoch 34 @ 5406 updates, score 0.257) (writing took 1.6847895798273385 seconds)
2025-03-09 19:23:52 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2025-03-09 19:23:52 | INFO | train | epoch 034 | loss 0.268 | ppl 1.2 | wps 17415.2 | ups 6.53 | wpb 2665.8 | bsz 178.5 | num_updates 5406 | lr 0.000860185 | gnorm 0.462 | train_wall 20 | gb_free 22.1 | wall 868
2025-03-09 19:23:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:23:52 | INFO | fairseq.trainer | begin training epoch 35
2025-03-09 19:23:52 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:24:04 | INFO | train_inner | epoch 035:     94 / 159 loss=0.304, ppl=1.23, wps=15771.4, ups=5.98, wpb=2638.6, bsz=178.5, num_updates=5500, lr=0.000852803, gnorm=0.632, train_wall=12, gb_free=22.1, wall=880
2025-03-09 19:24:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:24:15 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 0.234 | ppl 1.18 | wps 47630.9 | wpb 945.6 | bsz 63.3 | num_updates 5565 | best_loss 0.234
2025-03-09 19:24:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 5565 updates
2025-03-09 19:24:15 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint35.pt
2025-03-09 19:24:16 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint35.pt
2025-03-09 19:24:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint35.pt (epoch 35 @ 5565 updates, score 0.234) (writing took 2.9764510630629957 seconds)
2025-03-09 19:24:18 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2025-03-09 19:24:18 | INFO | train | epoch 035 | loss 0.282 | ppl 1.22 | wps 16615.8 | ups 6.23 | wpb 2665.8 | bsz 178.5 | num_updates 5565 | lr 0.000847808 | gnorm 0.499 | train_wall 20 | gb_free 22.1 | wall 894
2025-03-09 19:24:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:24:18 | INFO | fairseq.trainer | begin training epoch 36
2025-03-09 19:24:18 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:24:22 | INFO | train_inner | epoch 036:     35 / 159 loss=0.246, ppl=1.19, wps=14947.1, ups=5.55, wpb=2693.2, bsz=178.7, num_updates=5600, lr=0.000845154, gnorm=0.316, train_wall=12, gb_free=22.1, wall=898
2025-03-09 19:24:35 | INFO | train_inner | epoch 036:    135 / 159 loss=0.237, ppl=1.18, wps=20840, ups=7.81, wpb=2668.5, bsz=179.5, num_updates=5700, lr=0.000837708, gnorm=0.337, train_wall=12, gb_free=22.1, wall=911
2025-03-09 19:24:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:24:41 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 0.251 | ppl 1.19 | wps 47625.4 | wpb 945.6 | bsz 63.3 | num_updates 5724 | best_loss 0.234
2025-03-09 19:24:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 5724 updates
2025-03-09 19:24:41 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint36.pt
2025-03-09 19:24:41 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint36.pt
2025-03-09 19:24:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint36.pt (epoch 36 @ 5724 updates, score 0.251) (writing took 1.8471901598386467 seconds)
2025-03-09 19:24:42 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2025-03-09 19:24:42 | INFO | train | epoch 036 | loss 0.242 | ppl 1.18 | wps 17282.2 | ups 6.48 | wpb 2665.8 | bsz 178.5 | num_updates 5724 | lr 0.00083595 | gnorm 0.342 | train_wall 20 | gb_free 22.1 | wall 918
2025-03-09 19:24:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:24:42 | INFO | fairseq.trainer | begin training epoch 37
2025-03-09 19:24:42 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:24:52 | INFO | train_inner | epoch 037:     76 / 159 loss=0.252, ppl=1.19, wps=15596.9, ups=5.9, wpb=2644.7, bsz=171, num_updates=5800, lr=0.000830455, gnorm=0.319, train_wall=12, gb_free=22.1, wall=928
2025-03-09 19:25:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:25:05 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 0.249 | ppl 1.19 | wps 47638.1 | wpb 945.6 | bsz 63.3 | num_updates 5883 | best_loss 0.234
2025-03-09 19:25:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 5883 updates
2025-03-09 19:25:05 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint37.pt
2025-03-09 19:25:06 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint37.pt
2025-03-09 19:25:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint37.pt (epoch 37 @ 5883 updates, score 0.249) (writing took 1.8364691459573805 seconds)
2025-03-09 19:25:07 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2025-03-09 19:25:07 | INFO | train | epoch 037 | loss 0.238 | ppl 1.18 | wps 17292.7 | ups 6.49 | wpb 2665.8 | bsz 178.5 | num_updates 5883 | lr 0.000824576 | gnorm 0.371 | train_wall 20 | gb_free 22.1 | wall 943
2025-03-09 19:25:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:25:07 | INFO | fairseq.trainer | begin training epoch 38
2025-03-09 19:25:07 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:25:09 | INFO | train_inner | epoch 038:     17 / 159 loss=0.23, ppl=1.17, wps=15745.1, ups=5.89, wpb=2674.6, bsz=184.6, num_updates=5900, lr=0.000823387, gnorm=0.411, train_wall=12, gb_free=22.1, wall=945
2025-03-09 19:25:22 | INFO | train_inner | epoch 038:    117 / 159 loss=0.232, ppl=1.17, wps=20934.4, ups=7.82, wpb=2678.7, bsz=183.6, num_updates=6000, lr=0.000816497, gnorm=0.394, train_wall=12, gb_free=22.1, wall=958
2025-03-09 19:25:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:25:30 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 0.263 | ppl 1.2 | wps 47635 | wpb 945.6 | bsz 63.3 | num_updates 6042 | best_loss 0.234
2025-03-09 19:25:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 6042 updates
2025-03-09 19:25:30 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint38.pt
2025-03-09 19:25:30 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint38.pt
2025-03-09 19:25:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint38.pt (epoch 38 @ 6042 updates, score 0.263) (writing took 1.936567007098347 seconds)
2025-03-09 19:25:32 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2025-03-09 19:25:32 | INFO | train | epoch 038 | loss 0.242 | ppl 1.18 | wps 17210.8 | ups 6.46 | wpb 2665.8 | bsz 178.5 | num_updates 6042 | lr 0.000813654 | gnorm 0.383 | train_wall 20 | gb_free 22.1 | wall 968
2025-03-09 19:25:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:25:32 | INFO | fairseq.trainer | begin training epoch 39
2025-03-09 19:25:32 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:25:39 | INFO | train_inner | epoch 039:     58 / 159 loss=0.232, ppl=1.17, wps=15543.7, ups=5.86, wpb=2651.1, bsz=175.3, num_updates=6100, lr=0.000809776, gnorm=0.32, train_wall=12, gb_free=22.1, wall=975
2025-03-09 19:25:52 | INFO | train_inner | epoch 039:    158 / 159 loss=0.241, ppl=1.18, wps=20690.2, ups=7.75, wpb=2670.2, bsz=175.6, num_updates=6200, lr=0.000803219, gnorm=0.378, train_wall=12, gb_free=22.1, wall=988
2025-03-09 19:25:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:25:54 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 0.254 | ppl 1.19 | wps 47539.8 | wpb 945.6 | bsz 63.3 | num_updates 6201 | best_loss 0.234
2025-03-09 19:25:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 6201 updates
2025-03-09 19:25:54 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint39.pt
2025-03-09 19:25:55 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint39.pt
2025-03-09 19:25:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint39.pt (epoch 39 @ 6201 updates, score 0.254) (writing took 1.8349506561644375 seconds)
2025-03-09 19:25:56 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2025-03-09 19:25:56 | INFO | train | epoch 039 | loss 0.226 | ppl 1.17 | wps 17256.4 | ups 6.47 | wpb 2665.8 | bsz 178.5 | num_updates 6201 | lr 0.000803155 | gnorm 0.334 | train_wall 20 | gb_free 22.1 | wall 992
2025-03-09 19:25:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:25:56 | INFO | fairseq.trainer | begin training epoch 40
2025-03-09 19:25:56 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:26:09 | INFO | train_inner | epoch 040:     99 / 159 loss=0.237, ppl=1.18, wps=15516.7, ups=5.88, wpb=2639.5, bsz=177.1, num_updates=6300, lr=0.000796819, gnorm=0.411, train_wall=12, gb_free=22.1, wall=1005
2025-03-09 19:26:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:26:19 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 0.248 | ppl 1.19 | wps 45619.4 | wpb 945.6 | bsz 63.3 | num_updates 6360 | best_loss 0.234
2025-03-09 19:26:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 6360 updates
2025-03-09 19:26:19 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint40.pt
2025-03-09 19:26:20 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint40.pt
2025-03-09 19:26:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint40.pt (epoch 40 @ 6360 updates, score 0.248) (writing took 1.7158635156229138 seconds)
2025-03-09 19:26:21 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2025-03-09 19:26:21 | INFO | train | epoch 040 | loss 0.226 | ppl 1.17 | wps 17265.5 | ups 6.48 | wpb 2665.8 | bsz 178.5 | num_updates 6360 | lr 0.000793052 | gnorm 0.358 | train_wall 20 | gb_free 22.1 | wall 1017
2025-03-09 19:26:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:26:21 | INFO | fairseq.trainer | begin training epoch 41
2025-03-09 19:26:21 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:26:26 | INFO | train_inner | epoch 041:     40 / 159 loss=0.201, ppl=1.15, wps=15998.2, ups=5.9, wpb=2713.8, bsz=180.8, num_updates=6400, lr=0.000790569, gnorm=0.256, train_wall=12, gb_free=22.1, wall=1022
2025-03-09 19:26:39 | INFO | train_inner | epoch 041:    140 / 159 loss=0.225, ppl=1.17, wps=21059.3, ups=7.86, wpb=2680.2, bsz=180.3, num_updates=6500, lr=0.000784465, gnorm=0.406, train_wall=12, gb_free=22.1, wall=1035
2025-03-09 19:26:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:26:43 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 0.271 | ppl 1.21 | wps 47596.3 | wpb 945.6 | bsz 63.3 | num_updates 6519 | best_loss 0.234
2025-03-09 19:26:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 6519 updates
2025-03-09 19:26:43 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint41.pt
2025-03-09 19:26:44 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint41.pt
2025-03-09 19:26:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint41.pt (epoch 41 @ 6519 updates, score 0.271) (writing took 1.7246374762617052 seconds)
2025-03-09 19:26:45 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2025-03-09 19:26:45 | INFO | train | epoch 041 | loss 0.222 | ppl 1.17 | wps 17408.2 | ups 6.53 | wpb 2665.8 | bsz 178.5 | num_updates 6519 | lr 0.000783321 | gnorm 0.413 | train_wall 20 | gb_free 22.1 | wall 1041
2025-03-09 19:26:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:26:45 | INFO | fairseq.trainer | begin training epoch 42
2025-03-09 19:26:45 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:26:55 | INFO | train_inner | epoch 042:     81 / 159 loss=0.244, ppl=1.18, wps=15698.8, ups=6, wpb=2617.6, bsz=180, num_updates=6600, lr=0.000778499, gnorm=0.489, train_wall=12, gb_free=22.3, wall=1051
2025-03-09 19:27:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:27:08 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 0.246 | ppl 1.19 | wps 47570.8 | wpb 945.6 | bsz 63.3 | num_updates 6678 | best_loss 0.234
2025-03-09 19:27:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 6678 updates
2025-03-09 19:27:08 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint42.pt
2025-03-09 19:27:08 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint42.pt
2025-03-09 19:27:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint42.pt (epoch 42 @ 6678 updates, score 0.246) (writing took 1.6443574479781091 seconds)
2025-03-09 19:27:09 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2025-03-09 19:27:09 | INFO | train | epoch 042 | loss 0.222 | ppl 1.17 | wps 17492.3 | ups 6.56 | wpb 2665.8 | bsz 178.5 | num_updates 6678 | lr 0.000773939 | gnorm 0.357 | train_wall 20 | gb_free 22.1 | wall 1065
2025-03-09 19:27:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:27:09 | INFO | fairseq.trainer | begin training epoch 43
2025-03-09 19:27:09 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:27:12 | INFO | train_inner | epoch 043:     22 / 159 loss=0.209, ppl=1.16, wps=15672.3, ups=5.93, wpb=2643.5, bsz=170.2, num_updates=6700, lr=0.000772667, gnorm=0.347, train_wall=13, gb_free=22.1, wall=1068
2025-03-09 19:27:25 | INFO | train_inner | epoch 043:    122 / 159 loss=0.2, ppl=1.15, wps=21381.8, ups=7.91, wpb=2702.6, bsz=185.3, num_updates=6800, lr=0.000766965, gnorm=0.274, train_wall=12, gb_free=22.1, wall=1081
2025-03-09 19:27:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:27:32 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 0.245 | ppl 1.19 | wps 47605.3 | wpb 945.6 | bsz 63.3 | num_updates 6837 | best_loss 0.234
2025-03-09 19:27:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 6837 updates
2025-03-09 19:27:32 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint43.pt
2025-03-09 19:27:32 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint43.pt
2025-03-09 19:27:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint43.pt (epoch 43 @ 6837 updates, score 0.245) (writing took 1.6268347301520407 seconds)
2025-03-09 19:27:33 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2025-03-09 19:27:33 | INFO | train | epoch 043 | loss 0.203 | ppl 1.15 | wps 17495.2 | ups 6.56 | wpb 2665.8 | bsz 178.5 | num_updates 6837 | lr 0.000764887 | gnorm 0.299 | train_wall 20 | gb_free 22.1 | wall 1089
2025-03-09 19:27:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:27:33 | INFO | fairseq.trainer | begin training epoch 44
2025-03-09 19:27:33 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:27:41 | INFO | train_inner | epoch 044:     63 / 159 loss=0.194, ppl=1.14, wps=16153.6, ups=6.01, wpb=2689.5, bsz=179.2, num_updates=6900, lr=0.000761387, gnorm=0.234, train_wall=12, gb_free=22.1, wall=1097
2025-03-09 19:27:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:27:56 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 0.246 | ppl 1.19 | wps 47617.5 | wpb 945.6 | bsz 63.3 | num_updates 6996 | best_loss 0.234
2025-03-09 19:27:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 6996 updates
2025-03-09 19:27:56 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint44.pt
2025-03-09 19:27:57 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint44.pt
2025-03-09 19:27:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint44.pt (epoch 44 @ 6996 updates, score 0.246) (writing took 1.647774341981858 seconds)
2025-03-09 19:27:58 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2025-03-09 19:27:58 | INFO | train | epoch 044 | loss 0.194 | ppl 1.14 | wps 17516.8 | ups 6.57 | wpb 2665.8 | bsz 178.5 | num_updates 6996 | lr 0.000756145 | gnorm 0.256 | train_wall 20 | gb_free 22.6 | wall 1114
2025-03-09 19:27:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:27:58 | INFO | fairseq.trainer | begin training epoch 45
2025-03-09 19:27:58 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:27:58 | INFO | train_inner | epoch 045:      4 / 159 loss=0.198, ppl=1.15, wps=15787.6, ups=5.96, wpb=2648.2, bsz=175.6, num_updates=7000, lr=0.000755929, gnorm=0.272, train_wall=12, gb_free=22.1, wall=1114
2025-03-09 19:28:11 | INFO | train_inner | epoch 045:    104 / 159 loss=0.192, ppl=1.14, wps=21229.5, ups=7.9, wpb=2688, bsz=181.6, num_updates=7100, lr=0.000750587, gnorm=0.299, train_wall=12, gb_free=22.1, wall=1127
2025-03-09 19:28:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:28:20 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 0.257 | ppl 1.19 | wps 47515.2 | wpb 945.6 | bsz 63.3 | num_updates 7155 | best_loss 0.234
2025-03-09 19:28:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 7155 updates
2025-03-09 19:28:20 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint45.pt
2025-03-09 19:28:21 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint45.pt
2025-03-09 19:28:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint45.pt (epoch 45 @ 7155 updates, score 0.257) (writing took 1.755655625835061 seconds)
2025-03-09 19:28:22 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2025-03-09 19:28:22 | INFO | train | epoch 045 | loss 0.196 | ppl 1.15 | wps 17417.5 | ups 6.53 | wpb 2665.8 | bsz 178.5 | num_updates 7155 | lr 0.000747696 | gnorm 0.325 | train_wall 20 | gb_free 22.1 | wall 1138
2025-03-09 19:28:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:28:22 | INFO | fairseq.trainer | begin training epoch 46
2025-03-09 19:28:22 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:28:28 | INFO | train_inner | epoch 046:     45 / 159 loss=0.202, ppl=1.15, wps=15309.5, ups=5.93, wpb=2582.1, bsz=177.5, num_updates=7200, lr=0.000745356, gnorm=0.542, train_wall=12, gb_free=22.1, wall=1144
2025-03-09 19:28:41 | INFO | train_inner | epoch 046:    145 / 159 loss=0.188, ppl=1.14, wps=21264.9, ups=7.79, wpb=2730.6, bsz=176.2, num_updates=7300, lr=0.000740233, gnorm=0.254, train_wall=12, gb_free=22.1, wall=1157
2025-03-09 19:28:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:28:45 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 0.249 | ppl 1.19 | wps 47536.2 | wpb 945.6 | bsz 63.3 | num_updates 7314 | best_loss 0.234
2025-03-09 19:28:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 7314 updates
2025-03-09 19:28:45 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint46.pt
2025-03-09 19:28:45 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint46.pt
2025-03-09 19:28:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint46.pt (epoch 46 @ 7314 updates, score 0.249) (writing took 1.7215114110149443 seconds)
2025-03-09 19:28:46 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2025-03-09 19:28:46 | INFO | train | epoch 046 | loss 0.193 | ppl 1.14 | wps 17420.4 | ups 6.53 | wpb 2665.8 | bsz 178.5 | num_updates 7314 | lr 0.000739524 | gnorm 0.4 | train_wall 20 | gb_free 22.1 | wall 1162
2025-03-09 19:28:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 159
2025-03-09 19:28:46 | INFO | fairseq.trainer | begin training epoch 47
2025-03-09 19:28:46 | INFO | fairseq_cli.train | Start iterating over samples
2025-03-09 19:28:58 | INFO | train_inner | epoch 047:     86 / 159 loss=0.191, ppl=1.14, wps=15492.2, ups=5.88, wpb=2633.9, bsz=166.9, num_updates=7400, lr=0.000735215, gnorm=0.342, train_wall=13, gb_free=22.1, wall=1174
2025-03-09 19:29:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2025-03-09 19:29:09 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 0.26 | ppl 1.2 | wps 47599.6 | wpb 945.6 | bsz 63.3 | num_updates 7473 | best_loss 0.234
2025-03-09 19:29:09 | INFO | fairseq_cli.train | early stop since valid performance hasn't improved for last 12 runs
2025-03-09 19:29:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 7473 updates
2025-03-09 19:29:09 | INFO | fairseq.trainer | Saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint47.pt
2025-03-09 19:29:09 | INFO | fairseq.trainer | Finished saving checkpoint to /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint47.pt
2025-03-09 19:29:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/users/s/solfrini/git/normalisation_training/models/lstm_dict4000_3l_embed384/checkpoint47.pt (epoch 47 @ 7473 updates, score 0.26) (writing took 1.7025181069038808 seconds)
2025-03-09 19:29:11 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2025-03-09 19:29:11 | INFO | train | epoch 047 | loss 0.184 | ppl 1.14 | wps 17453.7 | ups 6.55 | wpb 2665.8 | bsz 178.5 | num_updates 7473 | lr 0.000731615 | gnorm 0.297 | train_wall 20 | gb_free 22.2 | wall 1187
2025-03-09 19:29:11 | INFO | fairseq_cli.train | done training in 1180.0 seconds
