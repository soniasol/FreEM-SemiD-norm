import os
import subprocess
import align
import sentencepiece

import numpy as np

from sacrebleu.metrics import BLEU, CHRF, TER

#import torch
#import torch.serialization

# Allow Fairseq's global variables inside the checkpoint
#torch.serialization.add_safe_globals(["argparse.Namespace"])

# define a few useful functions

def decode_sp(list_sents):
    """
    Decode a list of sentencepiece tokenized sentences

    Parameters:
    ----------
    list_sents: list of list of str
        List of sentencepiece tokenized sentences

    Returns:
    -------
    list of str
        List of decoded sentences
    """
    return [''.join(sent).replace(' ', '').replace('▁', ' ').strip() for sent in list_sents]

def extract_hypothesis(filename):
    """
    Extract the hypothesis from a file

    Parameters:
    ----------
    filename: str
        Name of the file to read

    Returns:
    -------
    list of str
        List of hypothesis
    """
    outputs = []
    with open(filename) as fp:
        for line in fp:
            # if the line contains 'H-', it is a hypothesis
            if 'H-' in line:
                # get the third column only
                outputs.append(line.strip().split('\t')[2])
    return outputs

def write_file(list_sents, filename):
    """
    Write a list of sentences to a file

    Parameters
    ----------
    list_sents : list of str
        List of sentences to write to the file
    filename : str
        Name of the file to write to

    Returns
    -------
    None
    """
    with open(filename, 'w') as fp:
        for sent in list_sents:
            fp.write(sent + '\n')

def read_file(filename):
  """
  Read a file and return a list of sentences

  Parameters
  ----------
  filename : str
      Name of the file to read

  Returns
  -------
  list of str
      List of sentences in the file
  """
  list_sents = []
  with open(filename) as fp:
    for line in fp:
      list_sents.append(line.strip())
  return list_sents

def run_fairseq_interactive(model_n_words):
    """
    Runs fairseq-interactive on a given dataset using a trained model.

    Parameters:
    ----------
    model_n_words: int
        Number of words in the dictionary.

    Returns:
    -------
    None
    """
    # Define paths correctly
    input_file = f"{DIRECTORY}/data/tmp_norm.{model_n_words}.sp.src.tmp"
    output_file = f"{DIRECTORY}/data/tmp_norm.{model_n_words}.sp.src.output"
    data_bin = f"{DIRECTORY}/data/data_norm_bin_{model_n_words}"
    model_path = f"{DIRECTORY}/models/lstm_dict{model_n_words}_3l_embed384/checkpoint_best.pt"

    cmd = [
        "fairseq-interactive",
        data_bin,
        "--source-lang", "src",
        "--target-lang", "trg",
        "--path", model_path
    ]

    try:
        with open(input_file, "r") as infile, open(output_file, "w") as outfile:
            subprocess.run(cmd, stdin=infile, stdout=outfile, stderr=subprocess.PIPE, text=True, check=True)
        print(f"Processing complete. Output saved to {output_file}")

    except FileNotFoundError as e:
        print(f"File not found: {e.filename}")
    except subprocess.CalledProcessError as e:
        print(f"Fairseq error: {e.stderr}")
    except Exception as e:
        print(f"Unexpected error: {e}")


def normalise(sents, norm_filename, spm_model, model_n_words):
    """
    Normalise a list of sentences

    Parameters:
    ----------
    sents: list of str
        List of sentences to normalise
    norm_filename: str
        Name of the output file generated by fairseq
    spm_model: SentencePieceProcessor
        SentencePiece model
    model_n_words: int
        Number of words in the dictionary

    Returns:
    -------
    list of str
        List of normalised sentences
    """
    # generate temporary file
    filetmp = os.path.join(DIRECTORY,'data/tmp_norm.%s.sp.src.tmp'%model_n_words)
    # preprocessing
    input_sp = spm_model.encode(sents, out_type=str)
    # encode src sentences
    input_sp_sents = [' '.join(sent) for sent in input_sp]
    write_file(input_sp_sents, filetmp)
    
    # normalisation
    run_fairseq_interactive(model_n_words)

    # postprocessing
    outputs = extract_hypothesis(os.path.join(DIRECTORY,'data/%s'%norm_filename))
    outputs_postproc = decode_sp(outputs)
    return outputs_postproc

def compute_metrics(norm_filename, spm_model, model_n_words, compute_acc = False):
    """
    Compute the BLEU, TER, CHRF and alignment accuracy of the normalisation

    Parameters:
    ----------
    norm_filename: str
        Name of the file generated by fairseq
    spm_model: SentencePieceProcessor
        SentencePiece model
    model_n_words: int
        Number of words in the dictionary

    Returns:
    -------
    BLEU, TER, CHRF, ACC
        BLEU, TER, CHRF scores and alignment accuracy
    """

    #getting the test files
    test_trg = read_file(os.path.join(DIRECTORY,'data/test.trg'))
    test_src = read_file(os.path.join(DIRECTORY,'data/test.src'))
    
    # normalising the test.src
    test_norm = normalise(
        sents = test_src,
        norm_filename = norm_filename, 
        spm_model = spm_model, 
        model_n_words = model_n_words)

    # save the test.src
    write_file(test_norm, os.path.join(DIRECTORY,'data/test.%s.norm.trg'%model_n_words))

    # compute the BLEU score
    bleu = BLEU()
    bleu_score = bleu.corpus_score(test_norm,[test_trg])
    print(bleu_score)

    # compute the Translation Error Rate (TER)
    ter = TER()
    ter_score = ter.corpus_score(test_norm,[test_trg])

    # compute the Character n-gram F-score (CHRF)
    chrf = CHRF()
    chrf_score = chrf.corpus_score(test_norm,[test_trg])

    if compute_acc:
        # compute the accuracy of the alignment
        # FIXME: align doesn't have an align function 
        align_dev_norm_10 = align.align(test_trg, test_norm)
        num_diff = 0
        total = 0

        for sentence in align_dev_norm_10:
            for word in sentence:
                if '>' in word:
                    num_diff += 1
                total += 1
        
        accuracy = (total - num_diff)/total

        return bleu_score, ter_score, chrf_score, accuracy
    else:
        return bleu_score, ter_score, chrf_score, 0

def save_results(bleu_score, ter_score, chrf_score, accuracy, model_n_words):
    """
    Save the results to a file

    Parameters:
    ----------
    bleu_score: BLEU
        BLEU score
    ter_score: TER
        TER score
    chrf_score: CHRF
        CHRF score
    accuracy: float
        Alignment accuracy
    model_n_words: int
        Number of words in the dictionary

    Returns:
    -------
    None
    """

    with open(os.path.join(DIRECTORY,'outputs/results_%s.txt'%model_n_words), 'w') as f:
        f.write("%s words\n"%model_n_words)
        f.write("BLEU: %s\n"%bleu_score)
        f.write("TER: %s\n"%ter_score)
        f.write("CHRF: %s\n"%chrf_score)
        f.write("Accuracy: %g\n"%accuracy)

# ------------------ #

# absolute path to the github repo directory where all the data are stored
DIRECTORY = "/home/users/s/solfrini/git/normalisation_training"

if __name__ == "__main__":

    print("test 1000")
    # Loading the bpe model - 1000 words
    spm_model = sentencepiece.SentencePieceProcessor(model_file=os.path.join(DIRECTORY,'data/bpe_joint_1000.model'))

    bleu_1000, ter_1000, chrf_1000, accuracy_1000 = compute_metrics(
        norm_filename = "tmp_norm.1000.sp.src.output",
        spm_model = spm_model,
        model_n_words = 1000,
        )
    
    print(bleu_1000, ter_1000, chrf_1000, accuracy_1000)
    
    # save the results
    save_results(bleu_1000, ter_1000, chrf_1000, accuracy_1000, 1000)
    

    # Loading the bpe model - 2000 words
    spm_model = sentencepiece.SentencePieceProcessor(model_file=os.path.join(DIRECTORY,'data/bpe_joint_2000.model'))

    bleu_2000, ter_2000, chrf_2000, accuracy_2000 = compute_metrics(
        norm_filename = "tmp_norm.2000.sp.src.output",
        spm_model = spm_model,
        model_n_words = 2000,
        )
    
    # save the results
    save_results(bleu_2000, ter_2000, chrf_2000, accuracy_2000, 2000)

    
    # Loading the bpe model - 3000 words
    spm_model = sentencepiece.SentencePieceProcessor(model_file=os.path.join(DIRECTORY,'data/bpe_joint_3000.model'))

    bleu_3000, ter_3000, chrf_3000, accuracy_3000 = compute_metrics(
        norm_filename = "tmp_norm.3000.sp.src.output",
        spm_model = spm_model,
        model_n_words = 3000,
        )
    
    # save the results
    save_results(bleu_3000, ter_3000, chrf_3000, accuracy_3000, 3000)

    
    # Loading the bpe model - 4000 words
    spm_model = sentencepiece.SentencePieceProcessor(model_file=os.path.join(DIRECTORY,'data/bpe_joint_4000.model'))

    bleu_4000, ter_4000, chrf_4000, accuracy_4000 = compute_metrics(
        norm_filename = "tmp_norm.4000.sp.src.output",
        spm_model = spm_model,
        model_n_words = 4000,
        )
    
    # save the results
    save_results(bleu_4000, ter_4000, chrf_4000, accuracy_4000, 4000)